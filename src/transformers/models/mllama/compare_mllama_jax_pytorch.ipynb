{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:01<00:00,  4.75it/s]\n",
      "2024-11-25 21:56:09.938396: W external/xla/xla/tsl/framework/bfc_allocator.cc:497] Allocator (GPU_0_bfc) ran out of memory trying to allocate 180.18MiB (rounded to 188928000)requested by op \n",
      "2024-11-25 21:56:09.938504: W external/xla/xla/tsl/framework/bfc_allocator.cc:508] ****************xxxxx**************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "E1125 21:56:09.938525 1386435 pjrt_stream_executor_client.cc:3084] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 188928000 bytes.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 188928000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m     21\u001b[0m model_torch \u001b[38;5;241m=\u001b[39m MllamaVisionModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mFlaxMllamaVisionModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/amd/model/hub/models--meta-llama--Llama-3.2-11B-Vision/pytorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/amd/transformers/src/transformers/modeling_flax_utils.py:903\u001b[0m, in \u001b[0;36mFlaxPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, dtype, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m     safetensors_from_pt \u001b[38;5;241m=\u001b[39m safetensors_metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# init random models\u001b[39;00m\n\u001b[0;32m--> 903\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_do_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt \u001b[38;5;129;01mor\u001b[39;00m safetensors_from_pt:\n\u001b[1;32m    906\u001b[0m     state \u001b[38;5;241m=\u001b[39m load_pytorch_checkpoint_in_flax_state_dict(model, resolved_archive_file, is_sharded)\n",
      "File \u001b[0;32m/home/amd/transformers/src/transformers/models/mllama/modeling_flax_mllama.py:689\u001b[0m, in \u001b[0;36mFlaxMllamaVisionPreTrainedModel.__init__\u001b[0;34m(self, config, input_shape, seed, dtype, _do_init, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m   input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mmax_num_tiles, config\u001b[38;5;241m.\u001b[39mnum_channels, config\u001b[38;5;241m.\u001b[39mimage_size, config\u001b[38;5;241m.\u001b[39mimage_size,)\n\u001b[1;32m    688\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_class(config\u001b[38;5;241m=\u001b[39mconfig, dtype\u001b[38;5;241m=\u001b[39mdtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 689\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_do_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/amd/transformers/src/transformers/modeling_flax_utils.py:220\u001b[0m, in \u001b[0;36mFlaxPreTrainedModel.__init__\u001b[0;34m(self, config, module, input_shape, seed, dtype, _do_init)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_initialized \u001b[38;5;241m=\u001b[39m _do_init\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _do_init:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# randomly initialized parameters\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m     random_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     params_shape_tree \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39meval_shape(\u001b[38;5;28;01mlambda\u001b[39;00m params: params, random_params)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/amd/transformers/src/transformers/models/mllama/modeling_flax_mllama.py:700\u001b[0m, in \u001b[0;36mFlaxMllamaVisionPreTrainedModel.init_weights\u001b[0;34m(self, rng, input_shape, params)\u001b[0m\n\u001b[1;32m    697\u001b[0m params_rng, dropout_rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[1;32m    698\u001b[0m rngs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: params_rng, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: dropout_rng}\n\u001b[0;32m--> 700\u001b[0m random_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect_ratio_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect_ratio_mask\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m   random_params \u001b[38;5;241m=\u001b[39m flatten_dict(unfreeze(random_params))\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m/home/amd/transformers/src/transformers/models/mllama/modeling_flax_mllama.py:759\u001b[0m, in \u001b[0;36mFlaxMllamaVisionModule.__call__\u001b[0;34m(self, pixel_values, aspect_ratio_ids, aspect_ratio_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    752\u001b[0m     pixel_values: jnp\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    757\u001b[0m     return_dict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    758\u001b[0m ):\n\u001b[0;32m--> 759\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m      \u001b[49m\u001b[43maspect_ratio_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect_ratio_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# a list of aspect ratio ids for the images in the data\u001b[39;49;00m\n\u001b[1;32m    762\u001b[0m \u001b[43m      \u001b[49m\u001b[43maspect_ratio_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect_ratio_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/home/amd/transformers/src/transformers/models/mllama/modeling_flax_mllama.py:592\u001b[0m, in \u001b[0;36mFlaxMllamaVisionTransformer.__call__\u001b[0;34m(self, pixel_values, aspect_ratio_ids, aspect_ratio_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# Position embeddings\u001b[39;00m\n\u001b[1;32m    591\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m hidden_state\u001b[38;5;241m.\u001b[39mreshape((batch_size \u001b[38;5;241m*\u001b[39m num_concurrent_media, num_tiles, num_patches, dim))\n\u001b[0;32m--> 592\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgated_positional_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect_ratio_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_pre(hidden_state)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;66;03m# Compute the number of tokens to pad\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/home/amd/transformers/src/transformers/models/mllama/modeling_flax_mllama.py:393\u001b[0m, in \u001b[0;36mFlaxMllamaPrecomputedPositionEmbedding.__call__\u001b[0;34m(self, hidden_state, aspect_ratio_ids)\u001b[0m\n\u001b[1;32m    390\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m hidden_state \u001b[38;5;241m+\u001b[39m gated_position_embedding\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, num_patches, hidden_size)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Precomputed tile position embeddings\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m tile_position_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtile_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43maspect_ratio_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m hidden_state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    395\u001b[0m tile_position_embedding \u001b[38;5;241m=\u001b[39m tile_position_embedding\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    396\u001b[0m     batch_size, max_num_tiles, num_patches, hidden_size\n\u001b[1;32m    397\u001b[0m )\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flax/linen/linear.py:1101\u001b[0m, in \u001b[0;36mEmbed.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1101\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/nn/initializers.py:341\u001b[0m, in \u001b[0;36mvariance_scaling.<locals>.init\u001b[0;34m(key, shape, dtype)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _complex_truncated_normal(key, \u001b[38;5;241m2\u001b[39m, shape, dtype) \u001b[38;5;241m*\u001b[39m stddev\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m distribution \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 341\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m distribution \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    343\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39missubdtype(dtype, jnp\u001b[38;5;241m.\u001b[39mfloating):\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py:177\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1287\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1285\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1287\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1290\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 188928000 bytes."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import MllamaVisionModel, FlaxMllamaVisionModel\n",
    "from transformers import AutoProcessor, MllamaTextModel\n",
    "import requests\n",
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "torch.set_printoptions(precision=8)\n",
    "jax.numpy.set_printoptions(precision=10)\n",
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.5'\n",
    "\n",
    "hf_token = \"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\"\n",
    "login(hf_token)\n",
    "checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "model_torch = MllamaVisionModel.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "model = FlaxMllamaVisionModel.from_pretrained(\"/home/amd/model/hub/models--meta-llama--Llama-3.2-11B-Vision/pytorch\", from_pt=True, dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.25195312, -0.06542969, -0.02661133,  ...,  0.25781250,\n",
      "          -0.01263428,  0.31054688],\n",
      "         [-0.33007812, -0.01623535,  0.09814453,  ...,  0.22167969,\n",
      "          -0.09863281,  0.48046875],\n",
      "         [-0.34179688, -0.02038574,  0.11181641,  ...,  0.22070312,\n",
      "          -0.10449219,  0.47656250],\n",
      "         ...,\n",
      "         [-0.53906250,  0.02600098,  0.16503906,  ...,  0.17773438,\n",
      "          -0.05615234, -0.05346680],\n",
      "         [-0.53906250,  0.02600098,  0.16503906,  ...,  0.17773438,\n",
      "          -0.05615234, -0.05346680],\n",
      "         [-0.53906250,  0.02600098,  0.16503906,  ...,  0.17773438,\n",
      "          -0.05615234, -0.05346680]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.05151367, -0.01531982, -0.06152344,  ...,  0.02380371,\n",
      "           0.09423828, -0.00494385],\n",
      "         [-0.18066406, -0.01855469, -0.04980469,  ...,  0.06396484,\n",
      "           0.29882812, -0.05273438],\n",
      "         [-0.19335938, -0.02331543, -0.02978516,  ...,  0.05981445,\n",
      "           0.34179688, -0.04321289],\n",
      "         ...,\n",
      "         [ 0.05786133, -0.00043869, -0.19531250,  ..., -0.07421875,\n",
      "           0.11474609, -0.11816406],\n",
      "         [ 0.05786133, -0.00043869, -0.19531250,  ..., -0.07421875,\n",
      "           0.11474609, -0.11816406],\n",
      "         [ 0.05786133, -0.00043869, -0.19531250,  ..., -0.07421875,\n",
      "           0.11474609, -0.11816406]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.02917480,  0.10498047, -0.06225586,  ...,  0.03808594,\n",
      "          -0.10937500, -0.17675781],\n",
      "         [-0.03149414, -0.04809570, -0.10986328,  ..., -0.14453125,\n",
      "          -0.00558472, -0.02368164],\n",
      "         [-0.02355957,  0.02221680, -0.01098633,  ..., -0.06982422,\n",
      "           0.04736328, -0.03466797],\n",
      "         ...,\n",
      "         [ 0.34570312, -0.17382812, -0.08300781,  ...,  0.16601562,\n",
      "          -0.03881836, -0.15136719],\n",
      "         [ 0.34570312, -0.17382812, -0.08300781,  ...,  0.16601562,\n",
      "          -0.03881836, -0.15136719],\n",
      "         [ 0.34570312, -0.17382812, -0.08300781,  ...,  0.16601562,\n",
      "          -0.03881836, -0.15136719]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.26171875, -0.21679688,  0.10595703,  ...,  0.22656250,\n",
      "           0.08789062,  0.07861328],\n",
      "         [ 0.25000000, -0.35742188, -0.02563477,  ...,  0.11962891,\n",
      "          -0.01843262,  0.18652344],\n",
      "         [ 0.44335938, -0.55468750,  0.33398438,  ...,  0.33593750,\n",
      "           0.09423828,  0.29296875],\n",
      "         ...,\n",
      "         [ 0.58203125, -0.32421875,  0.06884766,  ...,  0.01263428,\n",
      "           0.29296875,  0.06982422],\n",
      "         [ 0.58203125, -0.32421875,  0.06884766,  ...,  0.01263428,\n",
      "           0.29296875,  0.06982422],\n",
      "         [ 0.58203125, -0.32421875,  0.06884766,  ...,  0.01263428,\n",
      "           0.29296875,  0.06982422]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.01672363, -0.18457031, -0.19824219,  ...,  0.09082031,\n",
      "           0.18261719, -0.04443359],\n",
      "         [ 0.13671875, -0.18261719, -0.42382812,  ..., -0.08496094,\n",
      "          -0.28320312, -0.03857422],\n",
      "         [ 0.15429688, -0.13085938, -0.40234375,  ...,  0.13281250,\n",
      "          -0.11621094, -0.03442383],\n",
      "         ...,\n",
      "         [ 0.38867188, -0.21679688, -0.04467773,  ...,  0.05395508,\n",
      "          -0.03784180,  0.01477051],\n",
      "         [ 0.38867188, -0.21679688, -0.04467773,  ...,  0.05395508,\n",
      "          -0.03784180,  0.01477051],\n",
      "         [ 0.38867188, -0.21679688, -0.04467773,  ...,  0.05395508,\n",
      "          -0.03784180,  0.01477051]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.10986328, -0.26953125, -0.06835938,  ...,  0.05664062,\n",
      "          -0.00149536, -0.11523438],\n",
      "         [ 0.12890625,  0.07226562, -0.19140625,  ...,  0.12060547,\n",
      "           0.06005859, -0.25585938],\n",
      "         [ 0.07275391,  0.09765625, -0.24316406,  ...,  0.07617188,\n",
      "           0.03588867, -0.29687500],\n",
      "         ...,\n",
      "         [ 0.46093750, -0.16015625, -0.01513672,  ..., -0.02319336,\n",
      "           0.10107422, -0.17675781],\n",
      "         [ 0.46093750, -0.16015625, -0.01513672,  ..., -0.02319336,\n",
      "           0.10107422, -0.17675781],\n",
      "         [ 0.46093750, -0.16015625, -0.01513672,  ..., -0.02319336,\n",
      "           0.10107422, -0.17675781]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.60546875,  0.01989746,  0.03784180,  ..., -0.11279297,\n",
      "           0.27343750,  0.08593750],\n",
      "         [-0.00732422, -0.52734375,  0.07031250,  ..., -0.39843750,\n",
      "          -0.06005859,  0.00137329],\n",
      "         [ 0.36914062, -0.06933594,  0.24804688,  ..., -0.30078125,\n",
      "          -0.10888672, -0.41796875],\n",
      "         ...,\n",
      "         [ 0.71093750,  0.02319336,  0.01977539,  ..., -0.26953125,\n",
      "          -0.05981445, -0.19531250],\n",
      "         [ 0.71093750,  0.02319336,  0.01977539,  ..., -0.26953125,\n",
      "          -0.05981445, -0.19531250],\n",
      "         [ 0.71093750,  0.02319336,  0.01977539,  ..., -0.26953125,\n",
      "          -0.05981445, -0.19531250]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.20507812,  0.39257812, -0.12158203,  ..., -0.18261719,\n",
      "           0.18750000, -0.12597656],\n",
      "         [-0.08886719,  0.29687500, -0.31445312,  ...,  0.22363281,\n",
      "          -0.11035156, -0.04907227],\n",
      "         [-0.05786133,  0.31640625, -0.16015625,  ...,  0.01635742,\n",
      "           0.01470947, -0.07421875],\n",
      "         ...,\n",
      "         [ 0.24121094,  0.32812500, -0.10009766,  ..., -0.28515625,\n",
      "           0.12500000, -0.24609375],\n",
      "         [ 0.24121094,  0.32812500, -0.10009766,  ..., -0.28515625,\n",
      "           0.12500000, -0.24609375],\n",
      "         [ 0.24121094,  0.32812500, -0.10009766,  ..., -0.28515625,\n",
      "           0.12500000, -0.24609375]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.25195312, -0.36718750, -0.20800781,  ..., -0.11035156,\n",
      "          -0.33007812, -0.06591797],\n",
      "         [ 0.41015625,  0.27929688,  0.35546875,  ...,  0.18554688,\n",
      "          -0.40039062, -0.25195312],\n",
      "         [-0.04565430,  0.18164062,  0.15820312,  ...,  0.03613281,\n",
      "          -0.48437500, -0.48437500],\n",
      "         ...,\n",
      "         [-0.04418945, -0.33007812,  0.10498047,  ..., -0.05468750,\n",
      "          -0.31250000, -0.01507568],\n",
      "         [-0.04418945, -0.33007812,  0.10498047,  ..., -0.05468750,\n",
      "          -0.31250000, -0.01507568],\n",
      "         [-0.04418945, -0.33007812,  0.10498047,  ..., -0.05468750,\n",
      "          -0.31250000, -0.01507568]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.17480469,  0.36718750,  0.10839844,  ..., -0.24316406,\n",
      "           0.38085938,  0.12500000],\n",
      "         [-0.79296875,  0.51953125,  0.64062500,  ...,  0.28906250,\n",
      "          -0.03100586,  0.20410156],\n",
      "         [-0.79687500,  0.70703125,  0.59375000,  ...,  0.40234375,\n",
      "          -0.37890625,  0.32226562],\n",
      "         ...,\n",
      "         [-0.04980469,  0.28320312, -0.02832031,  ..., -0.07177734,\n",
      "           0.25976562, -0.08007812],\n",
      "         [-0.04980469,  0.28320312, -0.02832031,  ..., -0.07177734,\n",
      "           0.25976562, -0.08007812],\n",
      "         [-0.04980469,  0.28320312, -0.02832031,  ..., -0.07177734,\n",
      "           0.25976562, -0.08007812]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.18066406, -0.10205078,  0.06127930,  ...,  0.06884766,\n",
      "           0.23046875, -0.14746094],\n",
      "         [-1.10937500, -0.86328125, -0.26757812,  ...,  0.12792969,\n",
      "          -0.15625000, -0.44726562],\n",
      "         [-1.15625000, -0.99218750, -0.46875000,  ..., -0.06884766,\n",
      "           0.02233887, -0.22753906],\n",
      "         ...,\n",
      "         [-0.08398438, -0.45507812,  0.29492188,  ..., -0.01672363,\n",
      "           0.31250000,  0.05761719],\n",
      "         [-0.08398438, -0.45507812,  0.29492188,  ..., -0.01672363,\n",
      "           0.31250000,  0.05761719],\n",
      "         [-0.08398438, -0.45507812,  0.29492188,  ..., -0.01672363,\n",
      "           0.31250000,  0.05761719]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.35742188, -0.24316406,  0.35156250,  ..., -0.19726562,\n",
      "          -0.16894531,  0.43750000],\n",
      "         [-0.51953125, -0.57031250, -0.26367188,  ...,  0.01965332,\n",
      "          -0.62890625,  0.24902344],\n",
      "         [ 0.11376953, -0.29296875, -0.39062500,  ...,  0.90625000,\n",
      "          -0.49804688,  0.46093750],\n",
      "         ...,\n",
      "         [-0.37109375, -0.20800781,  0.29687500,  ...,  0.04492188,\n",
      "           0.07080078,  0.36132812],\n",
      "         [-0.37109375, -0.20800781,  0.29687500,  ...,  0.04492188,\n",
      "           0.07080078,  0.36132812],\n",
      "         [-0.37109375, -0.20800781,  0.29687500,  ...,  0.04492188,\n",
      "           0.07080078,  0.36132812]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.11328125, -0.13574219, -0.27734375,  ..., -0.12109375,\n",
      "           0.01977539, -0.28906250],\n",
      "         [ 0.29296875, -0.03784180, -0.14843750,  ...,  0.07861328,\n",
      "           0.12500000, -0.52343750],\n",
      "         [ 0.02758789, -0.45312500, -0.19140625,  ...,  0.26757812,\n",
      "           0.28125000, -0.27734375],\n",
      "         ...,\n",
      "         [ 0.22167969, -0.11572266, -0.13085938,  ..., -0.11621094,\n",
      "           0.29882812, -0.21191406],\n",
      "         [ 0.22167969, -0.11572266, -0.13085938,  ..., -0.11621094,\n",
      "           0.29882812, -0.21191406],\n",
      "         [ 0.22167969, -0.11572266, -0.13085938,  ..., -0.11621094,\n",
      "           0.29882812, -0.21191406]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.17871094, -0.13281250, -0.30664062,  ..., -0.07177734,\n",
      "          -0.09033203, -0.33984375],\n",
      "         [ 0.83203125,  0.22265625, -0.30468750,  ...,  0.03930664,\n",
      "          -0.04467773, -0.68750000],\n",
      "         [ 1.09375000,  0.19726562, -0.10644531,  ...,  0.35937500,\n",
      "           0.06640625, -0.71484375],\n",
      "         ...,\n",
      "         [ 0.62500000,  0.08300781,  0.09570312,  ...,  0.08056641,\n",
      "           0.33984375, -0.01458740],\n",
      "         [ 0.62500000,  0.08300781,  0.09570312,  ...,  0.08056641,\n",
      "           0.33984375, -0.01458740],\n",
      "         [ 0.62500000,  0.08300781,  0.09570312,  ...,  0.08056641,\n",
      "           0.33984375, -0.01458740]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.20019531,  0.09277344, -0.04101562,  ...,  0.48437500,\n",
      "           0.50390625, -0.38671875],\n",
      "         [-0.76171875,  0.43750000, -0.53125000,  ..., -1.40625000,\n",
      "           0.21386719,  1.06250000],\n",
      "         [-0.75781250,  0.28320312, -0.58203125,  ..., -0.92968750,\n",
      "           0.30273438,  0.84375000],\n",
      "         ...,\n",
      "         [ 0.16015625,  0.57812500, -0.45117188,  ...,  0.01007080,\n",
      "           0.56640625, -0.03564453],\n",
      "         [ 0.16015625,  0.57812500, -0.45117188,  ...,  0.01007080,\n",
      "           0.56640625, -0.03564453],\n",
      "         [ 0.16015625,  0.57812500, -0.45117188,  ...,  0.01007080,\n",
      "           0.56640625, -0.03564453]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.34960938, -1.15625000, -0.33593750,  ..., -0.53515625,\n",
      "           0.05249023,  0.13281250],\n",
      "         [ 0.26562500, -0.66796875, -0.04199219,  ..., -0.93750000,\n",
      "           0.10742188,  0.21484375],\n",
      "         [-0.01989746, -1.07812500, -0.44726562,  ..., -1.25781250,\n",
      "           0.09570312,  0.25781250],\n",
      "         ...,\n",
      "         [-0.06835938, -0.42382812, -0.34179688,  ...,  0.22851562,\n",
      "          -0.13281250,  0.19921875],\n",
      "         [-0.06835938, -0.42382812, -0.34179688,  ...,  0.22851562,\n",
      "          -0.13281250,  0.19921875],\n",
      "         [-0.06835938, -0.42382812, -0.34179688,  ...,  0.22851562,\n",
      "          -0.13281250,  0.19921875]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.53515625,  0.07324219, -1.28906250,  ...,  0.48046875,\n",
      "          -0.48242188, -0.40625000],\n",
      "         [-0.26757812,  0.21093750, -0.57031250,  ..., -0.50000000,\n",
      "          -0.10302734, -0.03295898],\n",
      "         [-0.57421875,  0.39843750, -0.41210938,  ..., -0.59375000,\n",
      "           0.05517578,  0.22949219],\n",
      "         ...,\n",
      "         [ 0.48046875,  0.06079102, -0.12158203,  ...,  0.30468750,\n",
      "          -0.00665283, -0.08642578],\n",
      "         [ 0.48046875,  0.06079102, -0.12158203,  ...,  0.30468750,\n",
      "          -0.00665283, -0.08642578],\n",
      "         [ 0.48046875,  0.06079102, -0.12158203,  ...,  0.30468750,\n",
      "          -0.00665283, -0.08642578]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-1.70312500,  0.21093750,  1.08593750,  ..., -0.36914062,\n",
      "           0.03540039,  0.16601562],\n",
      "         [-0.79296875,  0.16796875,  0.32226562,  ..., -0.18554688,\n",
      "           0.16992188, -0.58984375],\n",
      "         [-0.21093750, -0.60156250,  0.36132812,  ..., -0.23925781,\n",
      "          -0.21484375, -0.94921875],\n",
      "         ...,\n",
      "         [-0.15332031, -0.62890625,  0.26757812,  ..., -0.25390625,\n",
      "           0.17285156,  0.15820312],\n",
      "         [-0.15332031, -0.62890625,  0.26757812,  ..., -0.25390625,\n",
      "           0.17285156,  0.15820312],\n",
      "         [-0.15332031, -0.62890625,  0.26757812,  ..., -0.25390625,\n",
      "           0.17285156,  0.15820312]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.60156250, -0.20019531,  0.47656250,  ..., -0.12500000,\n",
      "           0.14355469, -0.39648438],\n",
      "         [-0.49023438,  0.69921875,  0.01953125,  ..., -0.01757812,\n",
      "          -0.12011719, -0.61718750],\n",
      "         [-0.43359375,  0.59375000, -0.04492188,  ..., -0.45117188,\n",
      "          -0.12890625, -0.03857422],\n",
      "         ...,\n",
      "         [ 0.36523438, -0.34179688, -0.19042969,  ...,  0.03466797,\n",
      "          -0.22265625, -0.11669922],\n",
      "         [ 0.36523438, -0.34179688, -0.19042969,  ...,  0.03466797,\n",
      "          -0.22265625, -0.11669922],\n",
      "         [ 0.36523438, -0.34179688, -0.19042969,  ...,  0.03466797,\n",
      "          -0.22265625, -0.11669922]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.50781250,  0.05590820,  0.01708984,  ...,  0.14062500,\n",
      "          -0.16796875, -0.00952148],\n",
      "         [-0.07910156, -0.02453613, -0.24609375,  ...,  0.30664062,\n",
      "          -0.61328125,  0.11767578],\n",
      "         [-0.06298828, -0.69140625,  0.31835938,  ...,  0.48046875,\n",
      "          -0.67578125, -0.12353516],\n",
      "         ...,\n",
      "         [ 0.59375000, -0.14843750, -0.38671875,  ...,  0.13769531,\n",
      "           0.19140625, -0.14941406],\n",
      "         [ 0.59375000, -0.14843750, -0.38671875,  ...,  0.13769531,\n",
      "           0.19140625, -0.14941406],\n",
      "         [ 0.59375000, -0.14843750, -0.38671875,  ...,  0.13769531,\n",
      "           0.19140625, -0.14941406]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.02832031,  0.28320312, -0.43164062,  ..., -0.05200195,\n",
      "          -0.31250000, -0.05761719],\n",
      "         [ 0.33789062, -0.15820312,  0.06738281,  ...,  0.01263428,\n",
      "          -0.23535156, -0.40820312],\n",
      "         [ 0.87500000, -0.15917969,  0.15722656,  ..., -0.21093750,\n",
      "          -0.15820312, -0.46875000],\n",
      "         ...,\n",
      "         [ 0.01159668,  0.25781250, -0.17578125,  ..., -0.10791016,\n",
      "          -0.08154297, -0.39648438],\n",
      "         [ 0.01159668,  0.25781250, -0.17578125,  ..., -0.10791016,\n",
      "          -0.08154297, -0.39648438],\n",
      "         [ 0.01159668,  0.25781250, -0.17578125,  ..., -0.10791016,\n",
      "          -0.08154297, -0.39648438]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.24511719,  0.32617188, -0.21289062,  ...,  0.28515625,\n",
      "           0.14453125, -0.07128906],\n",
      "         [ 0.39257812,  0.30273438, -0.67187500,  ...,  0.53125000,\n",
      "          -0.06347656,  0.16113281],\n",
      "         [ 0.21093750,  0.22070312, -0.51562500,  ...,  0.38671875,\n",
      "          -0.01031494,  0.15332031],\n",
      "         ...,\n",
      "         [ 0.04516602,  0.06225586, -0.40234375,  ...,  0.43359375,\n",
      "           0.04565430, -0.05541992],\n",
      "         [ 0.04516602,  0.06225586, -0.40234375,  ...,  0.43359375,\n",
      "           0.04565430, -0.05541992],\n",
      "         [ 0.04516602,  0.06225586, -0.40234375,  ...,  0.43359375,\n",
      "           0.04565430, -0.05541992]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.27343750,  0.09765625, -0.01562500,  ...,  0.35742188,\n",
      "           0.54687500, -0.30273438],\n",
      "         [ 0.07763672, -0.01538086, -0.02233887,  ...,  0.31640625,\n",
      "           0.13964844, -0.03369141],\n",
      "         [ 0.28710938,  0.01562500, -0.12988281,  ...,  0.05395508,\n",
      "           0.12890625, -0.05908203],\n",
      "         ...,\n",
      "         [-0.05517578,  0.04272461,  0.19726562,  ..., -0.04516602,\n",
      "          -0.04296875,  0.25585938],\n",
      "         [-0.05517578,  0.04272461,  0.19726562,  ..., -0.04516602,\n",
      "          -0.04296875,  0.25585938],\n",
      "         [-0.05517578,  0.04272461,  0.19726562,  ..., -0.04516602,\n",
      "          -0.04296875,  0.25585938]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.42773438, -0.08837891, -0.05419922,  ...,  0.44335938,\n",
      "          -0.36718750,  0.49023438],\n",
      "         [ 0.69140625, -0.00738525,  0.40234375,  ..., -0.30468750,\n",
      "           0.17578125,  0.12353516],\n",
      "         [ 0.64062500, -0.16992188,  0.21582031,  ..., -0.40625000,\n",
      "           0.30078125,  0.19238281],\n",
      "         ...,\n",
      "         [ 0.06835938, -0.22656250, -0.16406250,  ...,  0.11328125,\n",
      "          -0.08203125, -0.10986328],\n",
      "         [ 0.06835938, -0.22656250, -0.16406250,  ...,  0.11328125,\n",
      "          -0.08203125, -0.10986328],\n",
      "         [ 0.06835938, -0.22656250, -0.16406250,  ...,  0.11328125,\n",
      "          -0.08203125, -0.10986328]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.04101562,  0.03466797,  0.03088379,  ...,  0.04199219,\n",
      "           0.22949219,  0.21484375],\n",
      "         [ 0.49804688, -0.07519531,  0.16308594,  ..., -0.27539062,\n",
      "          -0.36328125,  0.35351562],\n",
      "         [ 0.34960938, -0.25390625,  0.03149414,  ..., -0.16894531,\n",
      "          -0.34765625,  0.47851562],\n",
      "         ...,\n",
      "         [-0.05883789,  0.38476562,  0.24511719,  ..., -0.19628906,\n",
      "          -0.06225586,  0.36328125],\n",
      "         [-0.05883789,  0.38476562,  0.24511719,  ..., -0.19628906,\n",
      "          -0.06225586,  0.36328125],\n",
      "         [-0.05883789,  0.38476562,  0.24511719,  ..., -0.19628906,\n",
      "          -0.06225586,  0.36328125]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.07080078, -0.01220703,  0.19140625,  ..., -0.06738281,\n",
      "          -0.03002930,  0.18652344],\n",
      "         [-0.14062500, -0.04418945, -0.24414062,  ..., -0.61718750,\n",
      "          -0.12695312,  0.28320312],\n",
      "         [-0.02221680, -0.18164062, -0.20312500,  ..., -0.60156250,\n",
      "           0.14941406,  0.48828125],\n",
      "         ...,\n",
      "         [-0.12792969,  0.34570312, -0.14843750,  ...,  0.06689453,\n",
      "          -0.03833008,  0.17089844],\n",
      "         [-0.12792969,  0.34570312, -0.14843750,  ...,  0.06689453,\n",
      "          -0.03833008,  0.17089844],\n",
      "         [-0.12792969,  0.34570312, -0.14843750,  ...,  0.06689453,\n",
      "          -0.03833008,  0.17089844]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.00271606,  0.00381470, -0.05566406,  ...,  0.12451172,\n",
      "          -0.20703125, -0.23535156],\n",
      "         [-0.14257812,  0.33398438,  0.09277344,  ..., -0.11328125,\n",
      "           0.22949219, -0.51171875],\n",
      "         [-0.14746094,  0.20605469, -0.08740234,  ..., -0.20214844,\n",
      "           0.19921875, -0.02502441],\n",
      "         ...,\n",
      "         [-0.18066406,  0.31250000,  0.27539062,  ...,  0.13378906,\n",
      "           0.02722168, -0.10009766],\n",
      "         [-0.18066406,  0.31250000,  0.27539062,  ...,  0.13378906,\n",
      "           0.02722168, -0.10009766],\n",
      "         [-0.18066406,  0.31250000,  0.27539062,  ...,  0.13378906,\n",
      "           0.02722168, -0.10009766]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.06982422, -0.06176758,  0.09228516,  ..., -0.06494141,\n",
      "           0.09765625, -0.11328125],\n",
      "         [ 0.17285156,  0.20019531, -0.28515625,  ...,  0.08935547,\n",
      "          -0.09570312,  0.18261719],\n",
      "         [ 0.04833984, -0.03015137, -0.02062988,  ..., -0.08056641,\n",
      "          -0.08984375,  0.03588867],\n",
      "         ...,\n",
      "         [ 0.16699219, -0.06250000,  0.49804688,  ...,  0.06494141,\n",
      "          -0.17089844,  0.24023438],\n",
      "         [ 0.16699219, -0.06250000,  0.49804688,  ...,  0.06494141,\n",
      "          -0.17089844,  0.24023438],\n",
      "         [ 0.16699219, -0.06250000,  0.49804688,  ...,  0.06494141,\n",
      "          -0.17089844,  0.24023438]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.15039062, -0.00360107,  0.42187500,  ...,  0.02514648,\n",
      "          -0.03540039,  0.17187500],\n",
      "         [-0.40820312, -0.01104736,  0.01519775,  ..., -0.74218750,\n",
      "          -0.29296875,  0.06396484],\n",
      "         [-0.21777344, -0.19042969,  0.14843750,  ..., -0.56640625,\n",
      "          -0.19140625, -0.12792969],\n",
      "         ...,\n",
      "         [-0.46484375, -0.42187500,  0.06127930,  ..., -0.38476562,\n",
      "          -0.40234375,  0.64062500],\n",
      "         [-0.46484375, -0.42187500,  0.06127930,  ..., -0.38476562,\n",
      "          -0.40234375,  0.64062500],\n",
      "         [-0.46484375, -0.42187500,  0.06127930,  ..., -0.38476562,\n",
      "          -0.40234375,  0.64062500]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-1.01562500, -0.02978516,  0.04492188,  ..., -0.02087402,\n",
      "          -0.17675781,  0.07470703],\n",
      "         [-0.32812500,  0.45312500, -0.17675781,  ...,  0.12060547,\n",
      "           0.14746094,  0.20117188],\n",
      "         [-0.25976562,  0.18554688,  0.12597656,  ...,  0.38867188,\n",
      "           0.17382812,  0.50390625],\n",
      "         ...,\n",
      "         [-1.29687500, -0.25781250, -0.54687500,  ..., -0.50781250,\n",
      "           0.73046875,  0.29687500],\n",
      "         [-1.29687500, -0.25781250, -0.54687500,  ..., -0.50781250,\n",
      "           0.73046875,  0.29687500],\n",
      "         [-1.29687500, -0.25781250, -0.54687500,  ..., -0.50781250,\n",
      "           0.73046875,  0.29687500]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.74609375,  0.91015625, -0.09423828,  ..., -1.07812500,\n",
      "           0.36132812, -0.49414062],\n",
      "         [ 0.18847656,  0.06738281,  0.00152588,  ..., -0.91015625,\n",
      "          -0.40820312, -0.69140625],\n",
      "         [ 0.33007812, -0.30078125, -0.59765625,  ..., -0.65234375,\n",
      "          -0.82421875, -0.14257812],\n",
      "         ...,\n",
      "         [-0.26953125,  1.15625000, -0.97265625,  ..., -0.43554688,\n",
      "          -0.82812500, -1.21093750],\n",
      "         [-0.26953125,  1.15625000, -0.97265625,  ..., -0.43554688,\n",
      "          -0.82812500, -1.21093750],\n",
      "         [-0.26953125,  1.15625000, -0.97265625,  ..., -0.43554688,\n",
      "          -0.82812500, -1.21093750]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.36328125,  1.42187500,  0.32812500,  ..., -0.31640625,\n",
      "           0.37304688, -0.17285156],\n",
      "         [-1.16406250, -1.00781250, -0.62500000,  ...,  0.12060547,\n",
      "           1.48437500,  2.20312500],\n",
      "         [-1.54687500, -0.76953125, -0.87890625,  ..., -0.23535156,\n",
      "           1.28125000,  2.25000000],\n",
      "         ...,\n",
      "         [-0.66406250,  0.85937500, -0.92578125,  ..., -0.53125000,\n",
      "           0.40039062,  0.50000000],\n",
      "         [-0.66406250,  0.85937500, -0.92578125,  ..., -0.53125000,\n",
      "           0.40039062,  0.50000000],\n",
      "         [-0.66406250,  0.85937500, -0.92578125,  ..., -0.53125000,\n",
      "           0.40039062,  0.50000000]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.48242188, -0.09130859,  0.46093750,  ...,  0.40820312,\n",
      "           0.18750000,  0.03369141],\n",
      "         [-0.48632812, -0.25585938,  0.50390625,  ...,  0.25585938,\n",
      "          -0.06201172, -0.03881836],\n",
      "         [-0.41210938, -0.40429688,  0.33398438,  ..., -0.02514648,\n",
      "           0.22265625, -0.12451172],\n",
      "         ...,\n",
      "         [-0.35351562, -0.04687500,  0.47851562,  ...,  0.20996094,\n",
      "           0.55078125, -0.11767578],\n",
      "         [-0.35351562, -0.04687500,  0.47851562,  ...,  0.20996094,\n",
      "           0.55078125, -0.11767578],\n",
      "         [-0.35351562, -0.04687500,  0.47851562,  ...,  0.20996094,\n",
      "           0.55078125, -0.11767578]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.98437500,  0.21972656, -0.22753906,  ..., -0.38085938,\n",
      "          -0.35546875, -0.84765625],\n",
      "         [-0.73437500, -0.21582031, -0.06445312,  ...,  0.23437500,\n",
      "           1.06250000,  0.30664062],\n",
      "         [-0.08398438,  0.57031250, -0.31054688,  ...,  0.13964844,\n",
      "           1.17968750,  0.27539062],\n",
      "         ...,\n",
      "         [-1.82812500,  0.09765625, -1.66406250,  ...,  0.74218750,\n",
      "          -0.18457031, -0.83984375],\n",
      "         [-1.82812500,  0.09765625, -1.66406250,  ...,  0.74218750,\n",
      "          -0.18457031, -0.83984375],\n",
      "         [-1.82812500,  0.09765625, -1.66406250,  ...,  0.74218750,\n",
      "          -0.18457031, -0.83984375]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-1.07421875e-01, -1.03125000e+00,  4.82421875e-01,  ...,\n",
      "           6.67968750e-01,  9.71679688e-02, -3.96484375e-01],\n",
      "         [ 1.10156250e+00, -3.17187500e+00,  4.31640625e-01,  ...,\n",
      "           1.61718750e+00,  1.87500000e-01,  1.36718750e-01],\n",
      "         [ 7.61718750e-01, -2.56250000e+00,  2.08007812e-01,  ...,\n",
      "           1.14843750e+00,  5.50781250e-01, -2.89916992e-03],\n",
      "         ...,\n",
      "         [ 1.15234375e-01, -4.27734375e-01,  1.46484375e-01,  ...,\n",
      "          -7.85827637e-04,  5.70312500e-01, -3.18359375e-01],\n",
      "         [ 1.15234375e-01, -4.27734375e-01,  1.46484375e-01,  ...,\n",
      "          -7.85827637e-04,  5.70312500e-01, -3.18359375e-01],\n",
      "         [ 1.15234375e-01, -4.27734375e-01,  1.46484375e-01,  ...,\n",
      "          -7.85827637e-04,  5.70312500e-01, -3.18359375e-01]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.06835938, -0.08496094,  0.13574219,  ..., -0.17089844,\n",
      "           0.56250000,  0.15722656],\n",
      "         [ 0.40429688,  0.10791016, -0.29492188,  ...,  0.20019531,\n",
      "          -0.16113281,  0.41406250],\n",
      "         [ 0.04980469, -0.14941406, -0.53125000,  ..., -0.09912109,\n",
      "          -0.18261719,  0.19921875],\n",
      "         ...,\n",
      "         [ 0.43164062, -0.59375000, -0.68750000,  ..., -0.19921875,\n",
      "          -0.55468750,  0.51953125],\n",
      "         [ 0.43164062, -0.59375000, -0.68750000,  ..., -0.19921875,\n",
      "          -0.55468750,  0.51953125],\n",
      "         [ 0.43164062, -0.59375000, -0.68750000,  ..., -0.19921875,\n",
      "          -0.55468750,  0.51953125]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.02539062, -0.07763672,  0.21289062,  ..., -0.05249023,\n",
      "           0.26562500, -0.14941406],\n",
      "         [ 0.49804688, -0.39257812,  0.00952148,  ...,  0.64062500,\n",
      "          -0.53515625, -0.23632812],\n",
      "         [ 0.52734375,  0.44921875, -0.12695312,  ...,  0.48437500,\n",
      "          -1.07031250, -0.54296875],\n",
      "         ...,\n",
      "         [-0.35937500,  1.87500000, -0.53515625,  ..., -1.00781250,\n",
      "          -0.27929688, -0.83203125],\n",
      "         [-0.35937500,  1.87500000, -0.53515625,  ..., -1.00781250,\n",
      "          -0.27929688, -0.83203125],\n",
      "         [-0.35937500,  1.87500000, -0.53515625,  ..., -1.00781250,\n",
      "          -0.27929688, -0.83203125]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.54687500,  0.21679688,  0.05273438,  ...,  0.29296875,\n",
      "          -0.31250000, -0.18750000],\n",
      "         [-0.13574219, -0.22460938,  0.41601562,  ...,  0.31640625,\n",
      "          -0.33789062,  0.21875000],\n",
      "         [ 0.15917969,  0.24023438,  0.25976562,  ...,  0.20019531,\n",
      "          -0.46484375,  0.07910156],\n",
      "         ...,\n",
      "         [-0.51562500, -0.89843750, -0.34960938,  ..., -0.14257812,\n",
      "           0.13378906,  0.87500000],\n",
      "         [-0.51562500, -0.89843750, -0.34960938,  ..., -0.14257812,\n",
      "           0.13378906,  0.87500000],\n",
      "         [-0.51562500, -0.89843750, -0.34960938,  ..., -0.14257812,\n",
      "           0.13378906,  0.87500000]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.94531250,  0.49609375,  0.21191406,  ...,  0.03222656,\n",
      "          -0.48242188,  0.06396484],\n",
      "         [-0.68750000,  0.75000000, -0.03808594,  ..., -0.71484375,\n",
      "          -0.66015625,  0.73046875],\n",
      "         [-0.46484375,  0.32812500,  0.40625000,  ..., -0.75390625,\n",
      "          -0.40039062,  0.85156250],\n",
      "         ...,\n",
      "         [ 0.15332031, -0.22753906, -0.33984375,  ..., -0.34375000,\n",
      "          -0.58984375, -0.15234375],\n",
      "         [ 0.15332031, -0.22753906, -0.33984375,  ..., -0.34375000,\n",
      "          -0.58984375, -0.15234375],\n",
      "         [ 0.15332031, -0.22753906, -0.33984375,  ..., -0.34375000,\n",
      "          -0.58984375, -0.15234375]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.63281250,  0.87890625,  0.66796875,  ...,  0.61328125,\n",
      "           0.17480469, -2.09375000],\n",
      "         [-0.58203125, -1.17968750,  1.18750000,  ..., -0.19433594,\n",
      "           0.83203125,  0.83203125],\n",
      "         [-1.21875000, -1.93750000,  0.50000000,  ...,  0.02380371,\n",
      "           0.55468750,  0.29687500],\n",
      "         ...,\n",
      "         [-0.59765625, -2.34375000,  0.19824219,  ..., -0.12695312,\n",
      "           0.96875000, -0.37890625],\n",
      "         [-0.59765625, -2.34375000,  0.19824219,  ..., -0.12695312,\n",
      "           0.96875000, -0.37890625],\n",
      "         [-0.59765625, -2.34375000,  0.19824219,  ..., -0.12695312,\n",
      "           0.96875000, -0.37890625]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs_jax = processor(images=image, return_tensors=\"jax\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_torch = model_torch(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax (1, 4128, 1280) [[[-0.253906 -0.0654297 -0.0266113 ... 0.255859 -0.0126343 0.310547]\n",
      "  [-0.330078 -0.0166016 0.0986328 ... 0.22168 -0.0991211 0.480469]\n",
      "  [-0.341797 -0.0205078 0.111816 ... 0.220703 -0.104492 0.476562]\n",
      "  ...\n",
      "  [-0.539062 0.0266113 0.165039 ... 0.178711 -0.0561523 -0.0537109]\n",
      "  [-0.539062 0.0266113 0.165039 ... 0.178711 -0.0561523 -0.0537109]\n",
      "  [-0.539062 0.0266113 0.165039 ... 0.178711 -0.0561523 -0.0537109]]]\n",
      "jax (1, 4128, 1280) [[[-0.0515137 -0.0154419 -0.060791 ... 0.0239258 0.09375 -0.00466919]\n",
      "  [-0.181641 -0.0186768 -0.0498047 ... 0.0639648 0.298828 -0.0529785]\n",
      "  [-0.193359 -0.0236816 -0.0301514 ... 0.0600586 0.341797 -0.0429688]\n",
      "  ...\n",
      "  [0.057373 -0.000492096 -0.195312 ... -0.074707 0.114746 -0.118652]\n",
      "  [0.057373 -0.000492096 -0.195312 ... -0.074707 0.114746 -0.118652]\n",
      "  [0.057373 -0.000492096 -0.195312 ... -0.074707 0.114746 -0.118652]]]\n",
      "jax (1, 4128, 1280) [[[-0.0291748 0.10498 -0.0625 ... 0.0383301 -0.109863 -0.176758]\n",
      "  [-0.0311279 -0.0493164 -0.109375 ... -0.145508 -0.00619507 -0.0231934]\n",
      "  [-0.0236816 0.0197754 -0.0117798 ... -0.0717773 0.0473633 -0.0334473]\n",
      "  ...\n",
      "  [0.345703 -0.173828 -0.0820312 ... 0.165039 -0.0388184 -0.151367]\n",
      "  [0.345703 -0.173828 -0.0820312 ... 0.165039 -0.0388184 -0.151367]\n",
      "  [0.345703 -0.173828 -0.0820312 ... 0.165039 -0.0388184 -0.151367]]]\n",
      "jax (1, 4128, 1280) [[[0.261719 -0.216797 0.105957 ... 0.224609 0.0874023 0.0776367]\n",
      "  [0.25 -0.355469 -0.0270996 ... 0.120117 -0.0187988 0.185547]\n",
      "  [0.445312 -0.554688 0.332031 ... 0.332031 0.090332 0.291016]\n",
      "  ...\n",
      "  [0.582031 -0.326172 0.0698242 ... 0.0134888 0.294922 0.0693359]\n",
      "  [0.582031 -0.326172 0.0698242 ... 0.0134888 0.294922 0.0693359]\n",
      "  [0.582031 -0.326172 0.0698242 ... 0.0134888 0.294922 0.0693359]]]\n",
      "jax (1, 4128, 1280) [[[0.0167236 -0.18457 -0.199219 ... 0.0913086 0.181641 -0.0441895]\n",
      "  [0.132812 -0.18457 -0.423828 ... -0.0859375 -0.285156 -0.0402832]\n",
      "  [0.154297 -0.130859 -0.402344 ... 0.133789 -0.115723 -0.0349121]\n",
      "  ...\n",
      "  [0.388672 -0.216797 -0.0439453 ... 0.0541992 -0.034668 0.0140381]\n",
      "  [0.388672 -0.216797 -0.0439453 ... 0.0541992 -0.034668 0.0140381]\n",
      "  [0.388672 -0.216797 -0.0439453 ... 0.0541992 -0.034668 0.0140381]]]\n",
      "jax (1, 4128, 1280) [[[0.110352 -0.267578 -0.0683594 ... 0.0556641 -0.00105286 -0.115234]\n",
      "  [0.126953 0.0722656 -0.19043 ... 0.119629 0.0625 -0.255859]\n",
      "  [0.0683594 0.0991211 -0.242188 ... 0.074707 0.0349121 -0.296875]\n",
      "  ...\n",
      "  [0.460938 -0.161133 -0.0143433 ... -0.0233154 0.102051 -0.177734]\n",
      "  [0.460938 -0.161133 -0.0143433 ... -0.0233154 0.102051 -0.177734]\n",
      "  [0.460938 -0.161133 -0.0143433 ... -0.0233154 0.102051 -0.177734]]]\n",
      "jax (1, 4128, 1280) [[[0.605469 0.0227051 0.036377 ... -0.11084 0.273438 0.0874023]\n",
      "  [-0.00506592 -0.523438 0.0712891 ... -0.400391 -0.0549316 0.00582886]\n",
      "  [0.375 -0.0654297 0.251953 ... -0.302734 -0.110352 -0.417969]\n",
      "  ...\n",
      "  [0.710938 0.0228271 0.020874 ... -0.267578 -0.0583496 -0.195312]\n",
      "  [0.710938 0.0228271 0.020874 ... -0.267578 -0.0583496 -0.195312]\n",
      "  [0.710938 0.0228271 0.020874 ... -0.267578 -0.0583496 -0.195312]]]\n",
      "jax (1, 4128, 1280) [[[0.205078 0.392578 -0.121094 ... -0.182617 0.1875 -0.125]\n",
      "  [-0.0864258 0.296875 -0.320312 ... 0.227539 -0.11377 -0.0510254]\n",
      "  [-0.0571289 0.320312 -0.162109 ... 0.0168457 0.0151367 -0.0717773]\n",
      "  ...\n",
      "  [0.242188 0.330078 -0.0996094 ... -0.285156 0.125 -0.245117]\n",
      "  [0.242188 0.330078 -0.0996094 ... -0.285156 0.125 -0.245117]\n",
      "  [0.242188 0.330078 -0.0996094 ... -0.285156 0.125 -0.245117]]]\n",
      "jax (1, 4128, 1280) [[[-0.253906 -0.365234 -0.208008 ... -0.111816 -0.332031 -0.0693359]\n",
      "  [0.425781 0.273438 0.365234 ... 0.189453 -0.404297 -0.24707]\n",
      "  [-0.0458984 0.179688 0.157227 ... 0.0290527 -0.488281 -0.488281]\n",
      "  ...\n",
      "  [-0.0444336 -0.330078 0.106934 ... -0.0563965 -0.3125 -0.0169678]\n",
      "  [-0.0444336 -0.330078 0.106934 ... -0.0563965 -0.3125 -0.0169678]\n",
      "  [-0.0444336 -0.330078 0.106934 ... -0.0563965 -0.3125 -0.0169678]]]\n",
      "jax (1, 4128, 1280) [[[-0.175781 0.367188 0.10791 ... -0.241211 0.380859 0.128906]\n",
      "  [-0.796875 0.515625 0.640625 ... 0.291016 -0.0267334 0.208008]\n",
      "  [-0.796875 0.710938 0.597656 ... 0.40625 -0.380859 0.322266]\n",
      "  ...\n",
      "  [-0.0510254 0.283203 -0.0301514 ... -0.0708008 0.259766 -0.0795898]\n",
      "  [-0.0510254 0.283203 -0.0301514 ... -0.0708008 0.259766 -0.0795898]\n",
      "  [-0.0510254 0.283203 -0.0301514 ... -0.0708008 0.259766 -0.0795898]]]\n",
      "jax (1, 4128, 1280) [[[-0.181641 -0.104492 0.0629883 ... 0.0688477 0.232422 -0.147461]\n",
      "  [-1.10938 -0.859375 -0.265625 ... 0.136719 -0.147461 -0.453125]\n",
      "  [-1.15625 -1 -0.472656 ... -0.0639648 0.0244141 -0.233398]\n",
      "  ...\n",
      "  [-0.0830078 -0.455078 0.292969 ... -0.0164795 0.3125 0.0593262]\n",
      "  [-0.0830078 -0.455078 0.292969 ... -0.0164795 0.3125 0.0593262]\n",
      "  [-0.0830078 -0.455078 0.292969 ... -0.0164795 0.3125 0.0593262]]]\n",
      "jax (1, 4128, 1280) [[[-0.357422 -0.241211 0.353516 ... -0.195312 -0.169922 0.439453]\n",
      "  [-0.523438 -0.566406 -0.267578 ... 0.0246582 -0.632812 0.251953]\n",
      "  [0.11377 -0.296875 -0.373047 ... 0.90625 -0.515625 0.447266]\n",
      "  ...\n",
      "  [-0.371094 -0.208008 0.296875 ... 0.045166 0.0688477 0.361328]\n",
      "  [-0.371094 -0.208008 0.296875 ... 0.045166 0.0688477 0.361328]\n",
      "  [-0.371094 -0.208008 0.296875 ... 0.045166 0.0688477 0.361328]]]\n",
      "jax (1, 4128, 1280) [[[-0.109863 -0.136719 -0.273438 ... -0.12207 0.0230713 -0.287109]\n",
      "  [0.294922 -0.0500488 -0.140625 ... 0.0756836 0.125977 -0.523438]\n",
      "  [0.0288086 -0.458984 -0.189453 ... 0.263672 0.292969 -0.277344]\n",
      "  ...\n",
      "  [0.223633 -0.114746 -0.131836 ... -0.118164 0.298828 -0.209961]\n",
      "  [0.223633 -0.114746 -0.131836 ... -0.118164 0.298828 -0.209961]\n",
      "  [0.223633 -0.114746 -0.131836 ... -0.118164 0.298828 -0.209961]]]\n",
      "jax (1, 4128, 1280) [[[-0.172852 -0.129883 -0.302734 ... -0.0698242 -0.0913086 -0.339844]\n",
      "  [0.835938 0.223633 -0.300781 ... 0.0444336 -0.0397949 -0.691406]\n",
      "  [1.10156 0.206055 -0.104492 ... 0.365234 0.0649414 -0.71875]\n",
      "  ...\n",
      "  [0.621094 0.081543 0.0957031 ... 0.0786133 0.339844 -0.0164795]\n",
      "  [0.621094 0.081543 0.0957031 ... 0.0786133 0.339844 -0.0164795]\n",
      "  [0.621094 0.081543 0.0957031 ... 0.0786133 0.339844 -0.0164795]]]\n",
      "jax (1, 4128, 1280) [[[0.200195 0.0952148 -0.0393066 ... 0.486328 0.503906 -0.382812]\n",
      "  [-0.761719 0.433594 -0.523438 ... -1.42188 0.21875 1.07031]\n",
      "  [-0.769531 0.271484 -0.585938 ... -0.9375 0.310547 0.851562]\n",
      "  ...\n",
      "  [0.163086 0.574219 -0.455078 ... 0.00958252 0.5625 -0.0402832]\n",
      "  [0.163086 0.574219 -0.455078 ... 0.00958252 0.5625 -0.0402832]\n",
      "  [0.163086 0.574219 -0.455078 ... 0.00958252 0.5625 -0.0402832]]]\n",
      "jax (1, 4128, 1280) [[[-0.349609 -1.15625 -0.335938 ... -0.535156 0.0488281 0.125977]\n",
      "  [0.255859 -0.664062 -0.0563965 ... -0.925781 0.109863 0.214844]\n",
      "  [-0.0181885 -1.08594 -0.455078 ... -1.25781 0.105469 0.263672]\n",
      "  ...\n",
      "  [-0.0844727 -0.408203 -0.345703 ... 0.21582 -0.132812 0.196289]\n",
      "  [-0.0844727 -0.408203 -0.345703 ... 0.21582 -0.132812 0.196289]\n",
      "  [-0.0844727 -0.408203 -0.345703 ... 0.21582 -0.132812 0.196289]]]\n",
      "jax (1, 4128, 1280) [[[0.535156 0.0786133 -1.29688 ... 0.480469 -0.482422 -0.40625]\n",
      "  [-0.267578 0.21875 -0.578125 ... -0.496094 -0.101074 -0.0385742]\n",
      "  [-0.582031 0.398438 -0.421875 ... -0.585938 0.059082 0.234375]\n",
      "  ...\n",
      "  [0.472656 0.0688477 -0.117188 ... 0.302734 -0.0144043 -0.0849609]\n",
      "  [0.472656 0.0688477 -0.117188 ... 0.302734 -0.0144043 -0.0849609]\n",
      "  [0.472656 0.0688477 -0.117188 ... 0.302734 -0.0144043 -0.0849609]]]\n",
      "jax (1, 4128, 1280) [[[-1.6875 0.204102 1.07031 ... -0.376953 0.0522461 0.161133]\n",
      "  [-0.789062 0.163086 0.316406 ... -0.202148 0.163086 -0.578125]\n",
      "  [-0.207031 -0.628906 0.367188 ... -0.240234 -0.213867 -0.96875]\n",
      "  ...\n",
      "  [-0.154297 -0.632812 0.269531 ... -0.25 0.168945 0.154297]\n",
      "  [-0.154297 -0.632812 0.269531 ... -0.25 0.168945 0.154297]\n",
      "  [-0.154297 -0.632812 0.269531 ... -0.25 0.168945 0.154297]]]\n",
      "jax (1, 4128, 1280) [[[0.601562 -0.207031 0.486328 ... -0.126953 0.143555 -0.390625]\n",
      "  [-0.480469 0.699219 0.00872803 ... -0.00280762 -0.140625 -0.628906]\n",
      "  [-0.441406 0.589844 -0.0446777 ... -0.449219 -0.123535 -0.0444336]\n",
      "  ...\n",
      "  [0.365234 -0.34375 -0.188477 ... 0.0349121 -0.220703 -0.118164]\n",
      "  [0.365234 -0.34375 -0.188477 ... 0.0349121 -0.220703 -0.118164]\n",
      "  [0.365234 -0.34375 -0.188477 ... 0.0349121 -0.220703 -0.118164]]]\n",
      "jax (1, 4128, 1280) [[[0.507812 0.0498047 0.0109253 ... 0.145508 -0.169922 -0.00680542]\n",
      "  [-0.0668945 -0.0280762 -0.263672 ... 0.294922 -0.613281 0.117188]\n",
      "  [-0.0664062 -0.679688 0.314453 ... 0.478516 -0.683594 -0.12207]\n",
      "  ...\n",
      "  [0.59375 -0.149414 -0.384766 ... 0.134766 0.189453 -0.147461]\n",
      "  [0.59375 -0.149414 -0.384766 ... 0.134766 0.189453 -0.147461]\n",
      "  [0.59375 -0.149414 -0.384766 ... 0.134766 0.189453 -0.147461]]]\n",
      "jax (1, 4128, 1280) [[[0.0224609 0.273438 -0.429688 ... -0.0505371 -0.320312 -0.059082]\n",
      "  [0.332031 -0.15625 0.0554199 ... 0.0213623 -0.249023 -0.402344]\n",
      "  [0.871094 -0.155273 0.164062 ... -0.203125 -0.15918 -0.476562]\n",
      "  ...\n",
      "  [0.0120239 0.257812 -0.176758 ... -0.105469 -0.0810547 -0.398438]\n",
      "  [0.0120239 0.257812 -0.176758 ... -0.105469 -0.0810547 -0.398438]\n",
      "  [0.0120239 0.257812 -0.176758 ... -0.105469 -0.0810547 -0.398438]]]\n",
      "jax (1, 4128, 1280) [[[0.239258 0.324219 -0.210938 ... 0.28125 0.140625 -0.0839844]\n",
      "  [0.394531 0.298828 -0.667969 ... 0.53125 -0.0727539 0.165039]\n",
      "  [0.209961 0.226562 -0.511719 ... 0.394531 -0.00994873 0.151367]\n",
      "  ...\n",
      "  [0.0446777 0.0639648 -0.402344 ... 0.435547 0.0427246 -0.0534668]\n",
      "  [0.0446777 0.0639648 -0.402344 ... 0.435547 0.0427246 -0.0534668]\n",
      "  [0.0446777 0.0639648 -0.402344 ... 0.435547 0.0427246 -0.0534668]]]\n",
      "jax (1, 4128, 1280) [[[0.275391 0.0922852 -0.015625 ... 0.357422 0.546875 -0.294922]\n",
      "  [0.0727539 -0.0180664 -0.0217285 ... 0.308594 0.140625 -0.0349121]\n",
      "  [0.289062 0.0128174 -0.122559 ... 0.0644531 0.125977 -0.0563965]\n",
      "  ...\n",
      "  [-0.0568848 0.0476074 0.198242 ... -0.0429688 -0.0446777 0.255859]\n",
      "  [-0.0568848 0.0476074 0.198242 ... -0.0429688 -0.0446777 0.255859]\n",
      "  [-0.0568848 0.0476074 0.198242 ... -0.0429688 -0.0446777 0.255859]]]\n",
      "jax (1, 4128, 1280) [[[0.417969 -0.0791016 -0.0622559 ... 0.447266 -0.373047 0.472656]\n",
      "  [0.695312 -0.00686646 0.388672 ... -0.3125 0.158203 0.132812]\n",
      "  [0.644531 -0.163086 0.21582 ... -0.392578 0.285156 0.192383]\n",
      "  ...\n",
      "  [0.0678711 -0.224609 -0.163086 ... 0.112793 -0.081543 -0.111328]\n",
      "  [0.0678711 -0.224609 -0.163086 ... 0.112793 -0.081543 -0.111328]\n",
      "  [0.0678711 -0.224609 -0.163086 ... 0.112793 -0.081543 -0.111328]]]\n",
      "jax (1, 4128, 1280) [[[0.0397949 0.0356445 0.036377 ... 0.0395508 0.228516 0.213867]\n",
      "  [0.507812 -0.0913086 0.147461 ... -0.253906 -0.335938 0.347656]\n",
      "  [0.373047 -0.253906 0.0388184 ... -0.19043 -0.367188 0.484375]\n",
      "  ...\n",
      "  [-0.0588379 0.388672 0.245117 ... -0.196289 -0.0649414 0.363281]\n",
      "  [-0.0588379 0.388672 0.245117 ... -0.196289 -0.0649414 0.363281]\n",
      "  [-0.0588379 0.388672 0.245117 ... -0.196289 -0.0649414 0.363281]]]\n",
      "jax (1, 4128, 1280) [[[0.0737305 -0.0115967 0.188477 ... -0.0688477 -0.0292969 0.183594]\n",
      "  [-0.144531 -0.0344238 -0.246094 ... -0.601562 -0.131836 0.294922]\n",
      "  [-0.0196533 -0.183594 -0.203125 ... -0.609375 0.147461 0.474609]\n",
      "  ...\n",
      "  [-0.128906 0.341797 -0.146484 ... 0.0693359 -0.0393066 0.171875]\n",
      "  [-0.128906 0.341797 -0.146484 ... 0.0693359 -0.0393066 0.171875]\n",
      "  [-0.128906 0.341797 -0.146484 ... 0.0693359 -0.0393066 0.171875]]]\n",
      "jax (1, 4128, 1280) [[[-0.00125885 -0.00198364 -0.0544434 ... 0.121094 -0.205078 -0.233398]\n",
      "  [-0.133789 0.322266 0.0820312 ... -0.104004 0.225586 -0.484375]\n",
      "  [-0.157227 0.213867 -0.0693359 ... -0.198242 0.208984 -0.0446777]\n",
      "  ...\n",
      "  [-0.177734 0.3125 0.273438 ... 0.130859 0.0227051 -0.100586]\n",
      "  [-0.177734 0.3125 0.273438 ... 0.130859 0.0227051 -0.100586]\n",
      "  [-0.177734 0.3125 0.273438 ... 0.130859 0.0227051 -0.100586]]]\n",
      "jax (1, 4128, 1280) [[[-0.0639648 -0.0603027 0.0961914 ... -0.0617676 0.100586 -0.113281]\n",
      "  [0.15332 0.175781 -0.277344 ... 0.0654297 -0.112305 0.174805]\n",
      "  [0.050293 -0.0247803 -0.03125 ... -0.0756836 -0.0932617 0.0439453]\n",
      "  ...\n",
      "  [0.168945 -0.0617676 0.490234 ... 0.065918 -0.173828 0.246094]\n",
      "  [0.168945 -0.0617676 0.490234 ... 0.065918 -0.173828 0.246094]\n",
      "  [0.168945 -0.0617676 0.490234 ... 0.065918 -0.173828 0.246094]]]\n",
      "jax (1, 4128, 1280) [[[0.143555 -0.0100098 0.419922 ... 0.0214844 -0.0424805 0.172852]\n",
      "  [-0.419922 -0.00897217 0.0220947 ... -0.71875 -0.271484 0.0534668]\n",
      "  [-0.226562 -0.19043 0.141602 ... -0.570312 -0.204102 -0.109375]\n",
      "  ...\n",
      "  [-0.46875 -0.421875 0.057373 ... -0.380859 -0.410156 0.636719]\n",
      "  [-0.46875 -0.421875 0.057373 ... -0.380859 -0.410156 0.636719]\n",
      "  [-0.46875 -0.421875 0.057373 ... -0.380859 -0.410156 0.636719]]]\n",
      "jax (1, 4128, 1280) [[[-1.01562 -0.0344238 0.0583496 ... -0.0172119 -0.179688 0.0703125]\n",
      "  [-0.296875 0.453125 -0.15625 ... 0.131836 0.150391 0.196289]\n",
      "  [-0.259766 0.192383 0.132812 ... 0.386719 0.178711 0.5]\n",
      "  ...\n",
      "  [-1.29688 -0.265625 -0.546875 ... -0.511719 0.71875 0.296875]\n",
      "  [-1.29688 -0.265625 -0.546875 ... -0.511719 0.71875 0.296875]\n",
      "  [-1.29688 -0.265625 -0.546875 ... -0.511719 0.71875 0.296875]]]\n",
      "jax (1, 4128, 1280) [[[-0.730469 0.917969 -0.0878906 ... -1.07031 0.345703 -0.492188]\n",
      "  [0.164062 0.106934 -0.0201416 ... -0.914062 -0.40625 -0.730469]\n",
      "  [0.326172 -0.285156 -0.527344 ... -0.644531 -0.769531 -0.167969]\n",
      "  ...\n",
      "  [-0.255859 1.16406 -0.953125 ... -0.433594 -0.839844 -1.21875]\n",
      "  [-0.255859 1.16406 -0.953125 ... -0.433594 -0.839844 -1.21875]\n",
      "  [-0.255859 1.16406 -0.953125 ... -0.433594 -0.839844 -1.21875]]]\n",
      "jax (1, 4128, 1280) [[[0.351562 1.41406 0.332031 ... -0.316406 0.384766 -0.1875]\n",
      "  [-1.20312 -0.96875 -0.679688 ... 0.0756836 1.40625 2.0625]\n",
      "  [-1.60156 -0.839844 -0.898438 ... -0.234375 1.35938 2.3125]\n",
      "  ...\n",
      "  [-0.671875 0.863281 -0.917969 ... -0.542969 0.398438 0.496094]\n",
      "  [-0.671875 0.863281 -0.917969 ... -0.542969 0.398438 0.496094]\n",
      "  [-0.671875 0.863281 -0.917969 ... -0.542969 0.398438 0.496094]]]\n",
      "jax (1, 4128, 1280) [[[-0.480469 -0.0888672 0.470703 ... 0.416016 0.1875 0.0327148]\n",
      "  [-0.474609 -0.283203 0.464844 ... 0.232422 0.00271606 -0.0834961]\n",
      "  [-0.416016 -0.390625 0.371094 ... -0.0125732 0.206055 -0.0961914]\n",
      "  ...\n",
      "  [-0.357422 -0.0400391 0.486328 ... 0.204102 0.546875 -0.116211]\n",
      "  [-0.357422 -0.0400391 0.486328 ... 0.204102 0.546875 -0.116211]\n",
      "  [-0.357422 -0.0400391 0.486328 ... 0.204102 0.546875 -0.116211]]]\n",
      "jax (1, 4128, 1280) [[[-0.224609 -1.24219 -0.679688 ... 1.82812 0.443359 -0.273438]\n",
      "  [1.20312 -0.679688 -0.114746 ... 1.32812 0.367188 0.416016]\n",
      "  [1.67969 -0.628906 -0.423828 ... 1.57031 0.878906 0.566406]\n",
      "  ...\n",
      "  [-0.617188 -1 -1.5625 ... 2.17188 0.230469 -0.363281]\n",
      "  [-0.617188 -1 -1.5625 ... 2.17188 0.230469 -0.363281]\n",
      "  [-0.617188 -1 -1.5625 ... 2.17188 0.230469 -0.363281]]]\n",
      "jax (1, 4128, 1280) [[[0.578125 -1.46094 0.128906 ... 0.447266 -0.828125 0.102051]\n",
      "  [0.816406 -1.35156 0.204102 ... 0.757812 -0.0952148 0.462891]\n",
      "  [0.660156 -0.75 0.244141 ... 0.353516 -0.15625 0.392578]\n",
      "  ...\n",
      "  [0.347656 -1.21875 0.0371094 ... -0.118164 -0.597656 0.0869141]\n",
      "  [0.347656 -1.21875 0.0371094 ... -0.118164 -0.597656 0.0869141]\n",
      "  [0.347656 -1.21875 0.0371094 ... -0.118164 -0.597656 0.0869141]]]\n",
      "jax (1, 4128, 1280) [[[0.511719 -1.26562 2.8125 ... 1.02344 -0.285156 2.42188]\n",
      "  [-0.119629 -1.48438 2.79688 ... 0.871094 -0.419922 1.88281]\n",
      "  [-0.244141 -1.67188 2.76562 ... 0.835938 -0.40625 1.97656]\n",
      "  ...\n",
      "  [-0.318359 -2.01562 2.875 ... 0.707031 0.0603027 2.375]\n",
      "  [-0.318359 -2.01562 2.875 ... 0.707031 0.0603027 2.375]\n",
      "  [-0.318359 -2.01562 2.875 ... 0.707031 0.0603027 2.375]]]\n",
      "jax (1, 4128, 1280) [[[1.80469 8.1875 2.8125 ... 3.70312 -2.14062 0.535156]\n",
      "  [1.69531 7.46875 2.85938 ... 3.51562 -1.58594 0.589844]\n",
      "  [1.53125 6.875 2.92188 ... 3.57812 -1.30469 0.824219]\n",
      "  ...\n",
      "  [1.75781 8.875 2.48438 ... 3.23438 -2.79688 0.460938]\n",
      "  [1.75781 8.875 2.48438 ... 3.23438 -2.79688 0.460938]\n",
      "  [1.75781 8.875 2.48438 ... 3.23438 -2.79688 0.460938]]]\n",
      "jax (1, 4128, 1280) [[[-2.23438 -5.4375 -2.79688 ... -1.875 2.65625 -1.50781]\n",
      "  [-2.17188 -4.71875 -2.92188 ... -1.67969 2.40625 -2.0625]\n",
      "  [-2.25 -4.6875 -2.75 ... -1.75 2.375 -2.14062]\n",
      "  ...\n",
      "  [-2.14062 -5.03125 -3.09375 ... -1.78125 2.57812 -2]\n",
      "  [-2.14062 -5.03125 -3.09375 ... -1.78125 2.57812 -2]\n",
      "  [-2.14062 -5.03125 -3.09375 ... -1.78125 2.57812 -2]]]\n",
      "jax (1, 4128, 1280) [[[-1.04688 3 1.39844 ... -3.04688 -3.125 2.125]\n",
      "  [-1.10156 2.78125 1.80469 ... -3.21875 -2.96875 1.79688]\n",
      "  [-0.613281 2.82812 1.82031 ... -3.39062 -3.04688 1.67969]\n",
      "  ...\n",
      "  [-1.82031 2.51562 1.24219 ... -3.0625 -3.6875 1.96875]\n",
      "  [-1.82031 2.51562 1.24219 ... -3.0625 -3.6875 1.96875]\n",
      "  [-1.82031 2.51562 1.24219 ... -3.0625 -3.6875 1.96875]]]\n",
      "jax (1, 4128, 1280) [[[4.6875 -4.8125 -5.53125 ... -1.72656 1.08594 1.375]\n",
      "  [3.60938 -4.71875 -4.59375 ... -2.26562 1.35156 1.79688]\n",
      "  [3.21875 -4.8125 -4.65625 ... -2.09375 1.21094 1.96875]\n",
      "  ...\n",
      "  [4.4375 -4.9375 -5.90625 ... -1.99219 1.3125 2.03125]\n",
      "  [4.4375 -4.9375 -5.90625 ... -1.99219 1.3125 2.03125]\n",
      "  [4.4375 -4.9375 -5.90625 ... -1.99219 1.3125 2.03125]]]\n"
     ]
    }
   ],
   "source": [
    "output_jax = model(**inputs_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (1, None)\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = inputs[\"aspect_ratio_mask\"].reshape(1, -1)\n",
    "torch_arr = _prepare_aspect_ratio_attention_mask_torch(\n",
    "            aspect_ratio_mask=attention_mask, # (batch_size * num_concurrent_media, 4)\n",
    "            num_patches=1025, # 1025 = 1024+1\n",
    "            target_length=1032, # 1032\n",
    "            dtype=torch.float16,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = inputs_jax[\"aspect_ratio_mask\"].reshape(1, -1)\n",
    "jax_arr = _prepare_aspect_ratio_attention_mask_jax(\n",
    "            aspect_ratio_mask=attention_mask, # (batch_size * num_concurrent_media, 4)\n",
    "            num_patches=1025, # 1025 = 1024+1\n",
    "            target_length=1032, # 1032\n",
    "            dtype=jnp.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "are_equivalent(torch_arr, jax_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[     0.,      0.,     -0.,  ...,      0.,     -0.,     -0.],\n",
       "          [     0.,      0.,     -0.,  ...,      0.,     -0.,     -0.],\n",
       "          [     0.,      0.,     -0.,  ...,      0.,     -0.,     -0.],\n",
       "          ...,\n",
       "          [     0.,      0.,     -0.,  ..., -65504., -65504., -65504.],\n",
       "          [     0.,      0.,     -0.,  ..., -65504., -65504., -65504.],\n",
       "          [     0.,      0.,     -0.,  ..., -65504., -65504., -65504.]]]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[-0.0000000e+00, -0.0000000e+00, -0.0000000e+00, ...,\n",
       "          -0.0000000e+00, -0.0000000e+00, -0.0000000e+00],\n",
       "         [-0.0000000e+00, -0.0000000e+00, -0.0000000e+00, ...,\n",
       "          -0.0000000e+00, -0.0000000e+00, -0.0000000e+00],\n",
       "         [-0.0000000e+00, -0.0000000e+00, -0.0000000e+00, ...,\n",
       "          -0.0000000e+00, -0.0000000e+00, -0.0000000e+00],\n",
       "         ...,\n",
       "         [-0.0000000e+00, -0.0000000e+00, -0.0000000e+00, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-0.0000000e+00, -0.0000000e+00, -0.0000000e+00, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38],\n",
       "         [-0.0000000e+00, -0.0000000e+00, -0.0000000e+00, ...,\n",
       "          -3.4028235e+38, -3.4028235e+38, -3.4028235e+38]]]],      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1]]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['aspect_ratio_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.53125000,  0.47265625, -0.35742188,  0.23925781, -2.00000000,\n",
       "        -0.24902344,  0.36718750, -0.90625000, -2.68750000, -4.12500000],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch.last_hidden_state[0,0,0,0,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.554688, 0.332031, -0.0800781, 4.34375, 7.125, -0.386719,\n",
       "       0.636719, -0.601562, -0.289062, -0.617188], dtype=bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_jax.last_hidden_state[0,0,0,0,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MllamaVisionModel, FlaxMllamaVisionModel\n",
    "from transformers import AutoProcessor, MllamaTextModel\n",
    "import requests\n",
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "hf_token = \"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\"\n",
    "login(hf_token)\n",
    "checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs_torch = processor(images=image, return_tensors=\"pt\")\n",
    "inputs_jax = processor(images=image, return_tensors=\"jax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class TorchConv(nn.Module):\n",
    "    def __init__(self, dtype=torch.bfloat16):\n",
    "        super(TorchConv, self).__init__()\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=3, #3\n",
    "            out_channels=1280, #1280\n",
    "            kernel_size=14, #14\n",
    "            stride=14, #14\n",
    "            padding=\"valid\",\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.Tensor,\n",
    "\n",
    "    ):\n",
    "        batch_size, num_concurrent_media, num_tiles, num_channels, height, width = pixel_values.shape #(1, 1, 4, 3, 448, 448)\n",
    "\n",
    "        pixel_values = pixel_values.reshape(batch_size * num_concurrent_media * num_tiles, num_channels, height, width) #(4, 3, 448, 448)\n",
    "        # Patch embedding\n",
    "        patch_embeds = self.patch_embedding(pixel_values.to(torch.bfloat16).to('cuda')) # (batch_size * num_concurrent_media * num_tiles, hidden_size=1280, 32, 32)\n",
    "        return patch_embeds\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "torch_model = TorchConv(dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "\n",
    "# Get the parameters from the TorchConv model\n",
    "torch_params = torch_model.patch_embedding.weight\n",
    "# # Example PyTorch tensor\n",
    "# torch_params_32 = torch.randn(torch_params.shape, dtype=torch.float32)\n",
    "\n",
    "# # Permute the tensor's dimensions\n",
    "# torch_params_permuted = torch_params_32.permute(2, 3, 1, 0)\n",
    "\n",
    "# # Convert the PyTorch tensor to a NumPy array\n",
    "# torch_params_numpy = torch_params_permuted.detach().cpu().numpy()\n",
    "\n",
    "# # Convert the NumPy array to a JAX tensor with dtype bfloat16\n",
    "# jax_params = jnp.array(torch_params_numpy, dtype=jnp.bfloat16)\n",
    "# torch_params = torch.tensor(torch_params_32, dtype=torch.bfloat16)\n",
    "# torch_params = torch.nn.Parameter(torch.tensor(torch_params_32, dtype=torch.bfloat16, device='cuda'))\n",
    "# torch_model.patch_embedding.weight = torch_params\n",
    "torch_model.patch_embedding.weight = model_torch.patch_embedding.weight\n",
    "# Perform a forward pass through the model\n",
    "output_torch = torch_model(inputs_torch.pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as fnn\n",
    "class JaxConv(fnn.Module):\n",
    "  dtype: jnp.dtype = jnp.bfloat16\n",
    "\n",
    "  def setup(self):\n",
    "\n",
    "    self.patch_embedding = fnn.Conv(\n",
    "        1280,\n",
    "        kernel_size=(14, 14),\n",
    "        strides=(14, 14),\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "        dtype=self.dtype,\n",
    "        kernel_init=jax.nn.initializers.normal(),\n",
    "    )\n",
    " \n",
    "  def __call__(\n",
    "      self,\n",
    "      pixel_values: jnp.ndarray,\n",
    "  ):\n",
    "        batch_size, num_concurrent_media, num_tiles, num_channels, height, width = pixel_values.shape\n",
    "\n",
    "        pixel_values = pixel_values.reshape((batch_size * num_concurrent_media * num_tiles, num_channels, height, width))\n",
    "\n",
    "        # Patch embedding\n",
    "        patch_embeds = self.patch_embedding(pixel_values.transpose((0, 2, 3, 1)))\n",
    "\n",
    "        patch_embeds = patch_embeds.transpose((0, 3, 1, 2))\n",
    "\n",
    "        return patch_embeds\n",
    "        \n",
    "# Initialize the model\n",
    "flax_model = JaxConv()\n",
    "# Initialize parameters\n",
    "params = flax_model.init(jax.random.PRNGKey(0), jnp.ones((1, 1, 4, 3, 448, 448)))\n",
    "\n",
    "# # Convert PyTorch parameters to JAX parameters\n",
    "params = unfreeze(params)\n",
    "params['params']['patch_embedding']['kernel'] = model.params['vision_model']['patch_embedding']['kernel']\n",
    "params = freeze(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_jax = flax_model.apply(params, inputs_jax.pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[-0.191406, -0.176758, -0.185547, ..., -0.175781, -0.174805,\n",
       "          -0.0688477],\n",
       "         [0.00186157, -0.0678711, -0.234375, ..., -0.175781, -0.172852,\n",
       "          -0.170898],\n",
       "         [0.0534668, 0.114258, 0.0119629, ..., -0.203125, -0.191406,\n",
       "          -0.192383],\n",
       "         ...,\n",
       "         [0.0219727, 0.0603027, 0.0556641, ..., -0.0135498, -0.0859375,\n",
       "          -0.121582],\n",
       "         [0.034668, 0.103516, 0.032959, ..., 0.00430298, 0.010376,\n",
       "          -0.111328],\n",
       "         [0.0181885, 0.0126953, 0.00482178, ..., 0.0419922, 0.0241699,\n",
       "          -0.0947266]],\n",
       "\n",
       "        [[0.0412598, 0.0356445, 0.0444336, ..., 0.0368652, 0.0410156,\n",
       "          0.176758],\n",
       "         [0.12207, 0.0373535, 0.090332, ..., 0.0291748, 0.0358887,\n",
       "          0.0664062],\n",
       "         [-0.0280762, 0.0534668, -0.0334473, ..., 0.0456543, 0.0317383,\n",
       "          0.0152588],\n",
       "         ...,\n",
       "         [-0.0566406, 0.0668945, 0.0385742, ..., 0.0158691, -0.022583,\n",
       "          0.0622559],\n",
       "         [0.0108032, 0.0429688, 0.0495605, ..., 0.0634766,\n",
       "          -0.000610352, 0.113281],\n",
       "         [0.00497437, 0.0568848, -0.0142212, ..., -0.0751953,\n",
       "          -0.0062561, 0.115723]],\n",
       "\n",
       "        [[-0.0961914, -0.287109, -0.386719, ..., -0.277344, -0.25,\n",
       "          0.139648],\n",
       "         [0.523438, 0.10498, 0.162109, ..., -0.242188, -0.275391,\n",
       "          -0.296875],\n",
       "         [0.0634766, -0.0185547, -0.203125, ..., -0.326172, -0.248047,\n",
       "          -0.21582],\n",
       "         ...,\n",
       "         [0.122559, -0.149414, -0.0917969, ..., 0.138672, -0.439453,\n",
       "          -0.273438],\n",
       "         [0.133789, -0.1875, 0.222656, ..., -0.335938, 0.375, 0.384766],\n",
       "         [-0.359375, -0.0527344, -0.150391, ..., 0.202148, 0.135742,\n",
       "          0.213867]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.451172, -0.410156, -0.402344, ..., -0.386719, -0.378906,\n",
       "          -0.224609],\n",
       "         [0.0898438, -0.157227, -0.181641, ..., -0.384766, -0.394531,\n",
       "          -0.416016],\n",
       "         [0.135742, 0.148438, 0.171875, ..., -0.5, -0.503906,\n",
       "          -0.464844],\n",
       "         ...,\n",
       "         [0.111328, -0.0615234, -0.213867, ..., 0.249023, 0.12793,\n",
       "          -0.419922],\n",
       "         [-0.0708008, -0.0563965, -0.134766, ..., 0.28125, 0.248047,\n",
       "          -0.378906],\n",
       "         [-0.267578, -0.158203, -0.0400391, ..., 0.324219, 0.25,\n",
       "          -0.104492]],\n",
       "\n",
       "        [[-0.136719, -0.427734, -0.332031, ..., -0.357422, -0.341797,\n",
       "          -2.20312],\n",
       "         [1.27344, 0.789062, 0.746094, ..., -0.355469, -0.423828,\n",
       "          -0.542969],\n",
       "         [-0.0466309, 0.621094, 1.03125, ..., -0.808594, -0.59375,\n",
       "          -0.503906],\n",
       "         ...,\n",
       "         [0.769531, 0.382812, -0.107422, ..., 0.361328, 3.76562,\n",
       "          -0.367188],\n",
       "         [-0.298828, 0.59375, 0.277344, ..., 0.0673828, 2.32812,\n",
       "          -0.386719],\n",
       "         [0.120117, 0.333984, 0.0795898, ..., 0.652344, 1.32812, 2.5]],\n",
       "\n",
       "        [[0.112793, 0.0751953, 0.0683594, ..., 0.0668945, 0.0664062,\n",
       "          0.106445],\n",
       "         [0.0795898, 0.117676, 0.0228271, ..., 0.0522461, 0.0703125,\n",
       "          0.065918],\n",
       "         [-0.00454712, 0.0388184, -0.0181885, ..., 0.0654297,\n",
       "          0.0673828, 0.0629883],\n",
       "         ...,\n",
       "         [-0.0124512, -0.0127563, 0.0255127, ..., -0.0588379, 0.036377,\n",
       "          0.0849609],\n",
       "         [-0.0742188, -0.0422363, 0.00994873, ..., -0.101562,\n",
       "          -0.0119629, 0.134766],\n",
       "         [0.043457, -0.0395508, -0.0280762, ..., -0.121582, -0.0532227,\n",
       "          0.0568848]]],\n",
       "\n",
       "\n",
       "       [[[-0.0134277, -0.0683594, 0.0361328, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [-0.163086, -0.196289, -0.0834961, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [-0.261719, -0.138672, -0.0473633, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         ...,\n",
       "         [-0.151367, -0.137695, -0.0412598, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [-0.123535, -0.0378418, 0.0219727, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [-0.105957, 0.00878906, -0.0339355, ..., 0.18457, 0.18457,\n",
       "          0.18457]],\n",
       "\n",
       "        [[0.101562, 0.103516, -0.0786133, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [0.0480957, 0.0854492, -0.172852, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [-0.118164, 0.0922852, -0.162109, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         ...,\n",
       "         [0.0117188, 0.0466309, 0.0322266, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [0.052002, 0.034668, 0.020752, ..., -0.0625, -0.0625, -0.0625],\n",
       "         [0.0888672, 0.0554199, -0.0168457, ..., -0.0625, -0.0625,\n",
       "          -0.0625]],\n",
       "\n",
       "        [[-1, -0.613281, 0.785156, ..., 0.292969, 0.292969, 0.292969],\n",
       "         [-0.155273, 0.020874, -0.349609, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [-0.00613403, 0.359375, -0.202148, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         ...,\n",
       "         [-0.207031, -0.166016, 0.106934, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [-0.257812, 0.408203, -0.28125, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [-0.166016, 0.241211, -0.249023, ..., 0.292969, 0.292969,\n",
       "          0.292969]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.429688, -0.4375, 0.00369263, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [-0.410156, -0.267578, -0.164062, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [-0.337891, -0.182617, -0.110352, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         ...,\n",
       "         [-0.431641, -0.410156, -0.244141, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [-0.373047, -0.211914, -0.175781, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [-0.314453, -0.185547, -0.246094, ..., 0.519531, 0.519531,\n",
       "          0.519531]],\n",
       "\n",
       "        [[0.433594, 0.675781, -0.527344, ..., 0.5, 0.5, 0.5],\n",
       "         [0.143555, -1.40625, -0.929688, ..., 0.5, 0.5, 0.5],\n",
       "         [-0.0136719, -1.625, 1.01562, ..., 0.5, 0.5, 0.5],\n",
       "         ...,\n",
       "         [-0.402344, -0.511719, -0.863281, ..., 0.5, 0.5, 0.5],\n",
       "         [-0.320312, -0.660156, -0.953125, ..., 0.5, 0.5, 0.5],\n",
       "         [-0.878906, -1.05469, -0.207031, ..., 0.5, 0.5, 0.5]],\n",
       "\n",
       "        [[0.0615234, 0.00187683, -0.0644531, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         [0.0373535, 0.050293, -0.0654297, ..., -0.0805664, -0.0805664,\n",
       "          -0.0805664],\n",
       "         [-0.0634766, 0.15332, -0.0290527, ..., -0.0805664, -0.0805664,\n",
       "          -0.0805664],\n",
       "         ...,\n",
       "         [0.0515137, 0.0634766, 0.0568848, ..., -0.0805664, -0.0805664,\n",
       "          -0.0805664],\n",
       "         [0.0551758, 0.0424805, 0.0127563, ..., -0.0805664, -0.0805664,\n",
       "          -0.0805664],\n",
       "         [0.0766602, 0.0285645, -0.0483398, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664]]],\n",
       "\n",
       "\n",
       "       [[[0.0112305, 0.0727539, 0.0344238, ..., 0.0952148, 0.0786133,\n",
       "          -0.0446777],\n",
       "         [0.0351562, 0.00805664, 0.100586, ..., 0.0649414, 0.12207,\n",
       "          0.122559],\n",
       "         [-0.0368652, -0.059082, -0.0332031, ..., 0.107422, 0.116699,\n",
       "          0.0854492],\n",
       "         ...,\n",
       "         [0.0859375, 0.130859, 0.235352, ..., 0.0908203, 0.144531,\n",
       "          -0.0177002],\n",
       "         [0.0722656, 0.198242, 0.125977, ..., 0.0961914, 0.0585938,\n",
       "          0.0649414],\n",
       "         [0.132812, 0.208008, 0.158203, ..., 0.0539551, 0.0693359,\n",
       "          0.0218506]],\n",
       "\n",
       "        [[0.0456543, -0.0218506, 0.0678711, ..., -0.160156, -0.0289307,\n",
       "          0.146484],\n",
       "         [-0.00379944, -0.0717773, -0.0598145, ..., -0.0820312,\n",
       "          -0.0098877, -0.052002],\n",
       "         [0.045166, 0.00300598, 0.00233459, ..., -0.139648, 0.0493164,\n",
       "          -0.0253906],\n",
       "         ...,\n",
       "         [-0.0172119, -0.0568848, -0.0830078, ..., -0.00939941,\n",
       "          0.0272217, -0.0791016],\n",
       "         [-0.0220947, 0.0664062, 0.138672, ..., -0.0400391, 0.0194092,\n",
       "          -0.00485229],\n",
       "         [-0.145508, 0.019043, -0.0834961, ..., 0.135742, -0.0415039,\n",
       "          -0.0378418]],\n",
       "\n",
       "        [[-0.314453, -0.396484, 0.554688, ..., 0.40625, 0.144531,\n",
       "          0.0544434],\n",
       "         [-0.3125, 0.0351562, 0.106445, ..., 0.0103149, 0.441406,\n",
       "          0.166992],\n",
       "         [-0.203125, -0.171875, -0.198242, ..., -0.125, 0.296875,\n",
       "          0.118164],\n",
       "         ...,\n",
       "         [0.0708008, 0.09375, 0.15332, ..., 0.392578, -0.0908203,\n",
       "          0.0751953],\n",
       "         [-0.539062, 0.306641, -0.101562, ..., 0.173828, 0.53125,\n",
       "          0.164062],\n",
       "         [0.511719, 0.617188, 0.365234, ..., 0.425781, 0.229492,\n",
       "          0.106934]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.259766, 0.015625, 0.0942383, ..., 0.365234, 0.232422,\n",
       "          0.104492],\n",
       "         [-0.140625, -0.15625, -0.0976562, ..., 0.322266, 0.376953,\n",
       "          0.213867],\n",
       "         [-0.3125, -0.287109, -0.243164, ..., 0.330078, 0.427734,\n",
       "          0.304688],\n",
       "         ...,\n",
       "         [0.231445, 0.419922, 0.357422, ..., 0.180664, 0.115234,\n",
       "          0.21582],\n",
       "         [0.265625, 0.378906, 0.40625, ..., 0.147461, 0.208984,\n",
       "          0.233398],\n",
       "         [0.457031, 0.335938, 0.330078, ..., 0.115723, 0.0693359,\n",
       "          -0.0341797]],\n",
       "\n",
       "        [[-0.539062, -0.871094, 0.367188, ..., 0.0717773, 1.52344,\n",
       "          3.28125],\n",
       "         [0.00141907, -0.22168, 0.104492, ..., -0.65625, 0.59375,\n",
       "          0.722656],\n",
       "         [-0.112305, 0.161133, 0.0751953, ..., -0.890625, 1.21094,\n",
       "          1.64844],\n",
       "         ...,\n",
       "         [0.423828, 0.0732422, 0.130859, ..., 0.238281, -0.101562,\n",
       "          0.367188],\n",
       "         [-0.707031, 0.511719, 1.00781, ..., 0.0844727, 0.601562,\n",
       "          0.00311279],\n",
       "         [0.644531, 0.246094, 0.878906, ..., 0.929688, -0.365234,\n",
       "          0.460938]],\n",
       "\n",
       "        [[-0.0117798, -0.00634766, -0.0349121, ..., -0.0106812,\n",
       "          -0.0512695, 0.0834961],\n",
       "         [-0.0146484, -0.0605469, 0.0200195, ..., -0.0498047,\n",
       "          -0.0378418, -0.0429688],\n",
       "         [-0.0510254, -0.078125, -0.0130615, ..., -0.105469,\n",
       "          -0.0673828, -0.0546875],\n",
       "         ...,\n",
       "         [-0.0678711, -0.0537109, -0.050293, ..., -0.0373535,\n",
       "          -0.0220947, -0.106934],\n",
       "         [-0.0712891, -0.0251465, -0.0108032, ..., 0.0766602,\n",
       "          -0.0195312, 0.0488281],\n",
       "         [-0.0849609, -0.0273438, -0.0402832, ..., -0.0255127,\n",
       "          0.0303955, -0.0534668]]],\n",
       "\n",
       "\n",
       "       [[[-0.0332031, 0.00720215, -0.0375977, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [0.0415039, 0.0196533, -0.0563965, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [0.0834961, -0.0378418, 0.0218506, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         ...,\n",
       "         [0.0649414, 0.0766602, 0.0996094, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [0.112793, 0.108398, 0.118652, ..., 0.18457, 0.18457, 0.18457],\n",
       "         [0.052002, 0.0673828, 0.0397949, ..., 0.18457, 0.18457,\n",
       "          0.18457]],\n",
       "\n",
       "        [[0.0544434, 0.0432129, 0.0649414, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [0.0227051, 3.19481e-05, 0.0693359, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [0.203125, -0.000888824, 0.0410156, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         ...,\n",
       "         [0.0534668, 0.0708008, -0.0957031, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [0.0585938, 0.0177002, 0.0541992, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [-0.00817871, 0.0131226, 0.0371094, ..., -0.0625, -0.0625,\n",
       "          -0.0625]],\n",
       "\n",
       "        [[0.347656, 0.128906, -0.310547, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [0.219727, 0.142578, -0.457031, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [1.01562, -0.322266, -0.359375, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         ...,\n",
       "         [0.28125, 0.193359, 0.371094, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [0.0649414, 0.0263672, 0.326172, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [0.065918, -0.144531, 0.197266, ..., 0.292969, 0.292969,\n",
       "          0.292969]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.249023, -0.251953, -0.298828, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [0.00775146, -0.193359, -0.380859, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [0.10498, -0.324219, -0.271484, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         ...,\n",
       "         [0.176758, 0.123535, 0.125977, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [0.114258, 0.209961, 0.292969, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [0.111816, 0.0556641, 0.166016, ..., 0.519531, 0.519531,\n",
       "          0.519531]],\n",
       "\n",
       "        [[-1.28906, -0.208008, -0.570312, ..., 0.5, 0.5, 0.5],\n",
       "         [1.16406, -0.369141, 0.335938, ..., 0.5, 0.5, 0.5],\n",
       "         [2.29688, 0.789062, -0.482422, ..., 0.5, 0.5, 0.5],\n",
       "         ...,\n",
       "         [-0.0466309, 0.457031, -0.484375, ..., 0.5, 0.5, 0.5],\n",
       "         [-0.157227, 0.123047, 0.259766, ..., 0.5, 0.5, 0.5],\n",
       "         [0.308594, -0.369141, 0.196289, ..., 0.5, 0.5, 0.5]],\n",
       "\n",
       "        [[0.0264893, -0.00463867, 0.0140381, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         [-0.00106049, -0.0119019, 0.027832, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         [-0.0267334, -0.0263672, 0.0776367, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         ...,\n",
       "         [-0.0186768, 0.0649414, -0.0581055, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         [-0.00787354, -0.03125, 0.0262451, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         [0.0141602, 0.0223389, -0.0272217, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664]]]], dtype=bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.91406250e-01, -1.76757812e-01, -1.85546875e-01,  ...,\n",
       "           -1.75781250e-01, -1.74804688e-01, -6.88476562e-02],\n",
       "          [ 1.86157227e-03, -6.78710938e-02, -2.34375000e-01,  ...,\n",
       "           -1.75781250e-01, -1.72851562e-01, -1.70898438e-01],\n",
       "          [ 5.34667969e-02,  1.14257812e-01,  1.19628906e-02,  ...,\n",
       "           -2.03125000e-01, -1.91406250e-01, -1.92382812e-01],\n",
       "          ...,\n",
       "          [ 2.19726562e-02,  6.03027344e-02,  5.56640625e-02,  ...,\n",
       "           -1.35498047e-02, -8.59375000e-02, -1.21582031e-01],\n",
       "          [ 3.46679688e-02,  1.03515625e-01,  3.29589844e-02,  ...,\n",
       "            4.30297852e-03,  1.03759766e-02, -1.11328125e-01],\n",
       "          [ 1.81884766e-02,  1.26953125e-02,  4.82177734e-03,  ...,\n",
       "            4.19921875e-02,  2.41699219e-02, -9.47265625e-02]],\n",
       "\n",
       "         [[ 4.12597656e-02,  3.56445312e-02,  4.44335938e-02,  ...,\n",
       "            3.68652344e-02,  4.10156250e-02,  1.76757812e-01],\n",
       "          [ 1.22070312e-01,  3.73535156e-02,  9.03320312e-02,  ...,\n",
       "            2.91748047e-02,  3.58886719e-02,  6.64062500e-02],\n",
       "          [-2.80761719e-02,  5.34667969e-02, -3.34472656e-02,  ...,\n",
       "            4.56542969e-02,  3.17382812e-02,  1.52587891e-02],\n",
       "          ...,\n",
       "          [-5.66406250e-02,  6.68945312e-02,  3.85742188e-02,  ...,\n",
       "            1.58691406e-02, -2.25830078e-02,  6.22558594e-02],\n",
       "          [ 1.08032227e-02,  4.29687500e-02,  4.95605469e-02,  ...,\n",
       "            6.34765625e-02, -6.10351562e-04,  1.13281250e-01],\n",
       "          [ 4.97436523e-03,  5.68847656e-02, -1.42211914e-02,  ...,\n",
       "           -7.51953125e-02, -6.25610352e-03,  1.15722656e-01]],\n",
       "\n",
       "         [[-9.61914062e-02, -2.87109375e-01, -3.86718750e-01,  ...,\n",
       "           -2.77343750e-01, -2.50000000e-01,  1.39648438e-01],\n",
       "          [ 5.23437500e-01,  1.04980469e-01,  1.62109375e-01,  ...,\n",
       "           -2.42187500e-01, -2.75390625e-01, -2.96875000e-01],\n",
       "          [ 6.34765625e-02, -1.85546875e-02, -2.03125000e-01,  ...,\n",
       "           -3.26171875e-01, -2.48046875e-01, -2.15820312e-01],\n",
       "          ...,\n",
       "          [ 1.22558594e-01, -1.49414062e-01, -9.17968750e-02,  ...,\n",
       "            1.38671875e-01, -4.39453125e-01, -2.73437500e-01],\n",
       "          [ 1.33789062e-01, -1.87500000e-01,  2.22656250e-01,  ...,\n",
       "           -3.35937500e-01,  3.75000000e-01,  3.84765625e-01],\n",
       "          [-3.59375000e-01, -5.27343750e-02, -1.50390625e-01,  ...,\n",
       "            2.02148438e-01,  1.35742188e-01,  2.13867188e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.51171875e-01, -4.10156250e-01, -4.02343750e-01,  ...,\n",
       "           -3.86718750e-01, -3.78906250e-01, -2.24609375e-01],\n",
       "          [ 8.98437500e-02, -1.57226562e-01, -1.81640625e-01,  ...,\n",
       "           -3.84765625e-01, -3.94531250e-01, -4.16015625e-01],\n",
       "          [ 1.35742188e-01,  1.48437500e-01,  1.71875000e-01,  ...,\n",
       "           -5.00000000e-01, -5.03906250e-01, -4.64843750e-01],\n",
       "          ...,\n",
       "          [ 1.11328125e-01, -6.15234375e-02, -2.13867188e-01,  ...,\n",
       "            2.49023438e-01,  1.27929688e-01, -4.19921875e-01],\n",
       "          [-7.08007812e-02, -5.63964844e-02, -1.34765625e-01,  ...,\n",
       "            2.81250000e-01,  2.48046875e-01, -3.78906250e-01],\n",
       "          [-2.67578125e-01, -1.58203125e-01, -4.00390625e-02,  ...,\n",
       "            3.24218750e-01,  2.50000000e-01, -1.04492188e-01]],\n",
       "\n",
       "         [[-1.36718750e-01, -4.27734375e-01, -3.32031250e-01,  ...,\n",
       "           -3.57421875e-01, -3.41796875e-01, -2.20312500e+00],\n",
       "          [ 1.27343750e+00,  7.89062500e-01,  7.46093750e-01,  ...,\n",
       "           -3.55468750e-01, -4.23828125e-01, -5.42968750e-01],\n",
       "          [-4.66308594e-02,  6.21093750e-01,  1.03125000e+00,  ...,\n",
       "           -8.08593750e-01, -5.93750000e-01, -5.03906250e-01],\n",
       "          ...,\n",
       "          [ 7.69531250e-01,  3.82812500e-01, -1.07421875e-01,  ...,\n",
       "            3.61328125e-01,  3.76562500e+00, -3.67187500e-01],\n",
       "          [-2.98828125e-01,  5.93750000e-01,  2.77343750e-01,  ...,\n",
       "            6.73828125e-02,  2.32812500e+00, -3.86718750e-01],\n",
       "          [ 1.20117188e-01,  3.33984375e-01,  7.95898438e-02,  ...,\n",
       "            6.52343750e-01,  1.32812500e+00,  2.50000000e+00]],\n",
       "\n",
       "         [[ 1.12792969e-01,  7.51953125e-02,  6.83593750e-02,  ...,\n",
       "            6.68945312e-02,  6.64062500e-02,  1.06445312e-01],\n",
       "          [ 7.95898438e-02,  1.17675781e-01,  2.28271484e-02,  ...,\n",
       "            5.22460938e-02,  7.03125000e-02,  6.59179688e-02],\n",
       "          [-4.54711914e-03,  3.88183594e-02, -1.81884766e-02,  ...,\n",
       "            6.54296875e-02,  6.73828125e-02,  6.29882812e-02],\n",
       "          ...,\n",
       "          [-1.24511719e-02, -1.27563477e-02,  2.55126953e-02,  ...,\n",
       "           -5.88378906e-02,  3.63769531e-02,  8.49609375e-02],\n",
       "          [-7.42187500e-02, -4.22363281e-02,  9.94873047e-03,  ...,\n",
       "           -1.01562500e-01, -1.19628906e-02,  1.34765625e-01],\n",
       "          [ 4.34570312e-02, -3.95507812e-02, -2.80761719e-02,  ...,\n",
       "           -1.21582031e-01, -5.32226562e-02,  5.68847656e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.34277344e-02, -6.83593750e-02,  3.61328125e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [-1.63085938e-01, -1.96289062e-01, -8.34960938e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [-2.61718750e-01, -1.38671875e-01, -4.73632812e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          ...,\n",
       "          [-1.51367188e-01, -1.37695312e-01, -4.12597656e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [-1.23535156e-01, -3.78417969e-02,  2.19726562e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [-1.05957031e-01,  8.78906250e-03, -3.39355469e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01]],\n",
       "\n",
       "         [[ 1.01562500e-01,  1.03515625e-01, -7.86132812e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 4.80957031e-02,  8.54492188e-02, -1.72851562e-01,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [-1.18164062e-01,  9.22851562e-02, -1.62109375e-01,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          ...,\n",
       "          [ 1.17187500e-02,  4.66308594e-02,  3.22265625e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 5.20019531e-02,  3.46679688e-02,  2.07519531e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 8.88671875e-02,  5.54199219e-02, -1.68457031e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02]],\n",
       "\n",
       "         [[-1.00000000e+00, -6.13281250e-01,  7.85156250e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [-1.55273438e-01,  2.08740234e-02, -3.49609375e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [-6.13403320e-03,  3.59375000e-01, -2.02148438e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          ...,\n",
       "          [-2.07031250e-01, -1.66015625e-01,  1.06933594e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [-2.57812500e-01,  4.08203125e-01, -2.81250000e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [-1.66015625e-01,  2.41210938e-01, -2.49023438e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.29687500e-01, -4.37500000e-01,  3.69262695e-03,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [-4.10156250e-01, -2.67578125e-01, -1.64062500e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [-3.37890625e-01, -1.82617188e-01, -1.10351562e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          ...,\n",
       "          [-4.31640625e-01, -4.10156250e-01, -2.44140625e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [-3.73046875e-01, -2.11914062e-01, -1.75781250e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [-3.14453125e-01, -1.85546875e-01, -2.46093750e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01]],\n",
       "\n",
       "         [[ 4.33593750e-01,  6.75781250e-01, -5.27343750e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [ 1.43554688e-01, -1.40625000e+00, -9.29687500e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [-1.36718750e-02, -1.62500000e+00,  1.01562500e+00,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          ...,\n",
       "          [-4.02343750e-01, -5.11718750e-01, -8.63281250e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [-3.20312500e-01, -6.60156250e-01, -9.53125000e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [-8.78906250e-01, -1.05468750e+00, -2.07031250e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01]],\n",
       "\n",
       "         [[ 6.15234375e-02,  1.87683105e-03, -6.44531250e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [ 3.73535156e-02,  5.02929688e-02, -6.54296875e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [-6.34765625e-02,  1.53320312e-01, -2.90527344e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          ...,\n",
       "          [ 5.15136719e-02,  6.34765625e-02,  5.68847656e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [ 5.51757812e-02,  4.24804688e-02,  1.27563477e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [ 7.66601562e-02,  2.85644531e-02, -4.83398438e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.12304688e-02,  7.27539062e-02,  3.44238281e-02,  ...,\n",
       "            9.52148438e-02,  7.86132812e-02, -4.46777344e-02],\n",
       "          [ 3.51562500e-02,  8.05664062e-03,  1.00585938e-01,  ...,\n",
       "            6.49414062e-02,  1.22070312e-01,  1.22558594e-01],\n",
       "          [-3.68652344e-02, -5.90820312e-02, -3.32031250e-02,  ...,\n",
       "            1.07421875e-01,  1.16699219e-01,  8.54492188e-02],\n",
       "          ...,\n",
       "          [ 8.59375000e-02,  1.30859375e-01,  2.35351562e-01,  ...,\n",
       "            9.08203125e-02,  1.44531250e-01, -1.77001953e-02],\n",
       "          [ 7.22656250e-02,  1.98242188e-01,  1.25976562e-01,  ...,\n",
       "            9.61914062e-02,  5.85937500e-02,  6.49414062e-02],\n",
       "          [ 1.32812500e-01,  2.08007812e-01,  1.58203125e-01,  ...,\n",
       "            5.39550781e-02,  6.93359375e-02,  2.18505859e-02]],\n",
       "\n",
       "         [[ 4.56542969e-02, -2.18505859e-02,  6.78710938e-02,  ...,\n",
       "           -1.60156250e-01, -2.89306641e-02,  1.46484375e-01],\n",
       "          [-3.79943848e-03, -7.17773438e-02, -5.98144531e-02,  ...,\n",
       "           -8.20312500e-02, -9.88769531e-03, -5.20019531e-02],\n",
       "          [ 4.51660156e-02,  3.00598145e-03,  2.33459473e-03,  ...,\n",
       "           -1.39648438e-01,  4.93164062e-02, -2.53906250e-02],\n",
       "          ...,\n",
       "          [-1.72119141e-02, -5.68847656e-02, -8.30078125e-02,  ...,\n",
       "           -9.39941406e-03,  2.72216797e-02, -7.91015625e-02],\n",
       "          [-2.20947266e-02,  6.64062500e-02,  1.38671875e-01,  ...,\n",
       "           -4.00390625e-02,  1.94091797e-02, -4.85229492e-03],\n",
       "          [-1.45507812e-01,  1.90429688e-02, -8.34960938e-02,  ...,\n",
       "            1.35742188e-01, -4.15039062e-02, -3.78417969e-02]],\n",
       "\n",
       "         [[-3.14453125e-01, -3.96484375e-01,  5.54687500e-01,  ...,\n",
       "            4.06250000e-01,  1.44531250e-01,  5.44433594e-02],\n",
       "          [-3.12500000e-01,  3.51562500e-02,  1.06445312e-01,  ...,\n",
       "            1.03149414e-02,  4.41406250e-01,  1.66992188e-01],\n",
       "          [-2.03125000e-01, -1.71875000e-01, -1.98242188e-01,  ...,\n",
       "           -1.25000000e-01,  2.96875000e-01,  1.18164062e-01],\n",
       "          ...,\n",
       "          [ 7.08007812e-02,  9.37500000e-02,  1.53320312e-01,  ...,\n",
       "            3.92578125e-01, -9.08203125e-02,  7.51953125e-02],\n",
       "          [-5.39062500e-01,  3.06640625e-01, -1.01562500e-01,  ...,\n",
       "            1.73828125e-01,  5.31250000e-01,  1.64062500e-01],\n",
       "          [ 5.11718750e-01,  6.17187500e-01,  3.65234375e-01,  ...,\n",
       "            4.25781250e-01,  2.29492188e-01,  1.06933594e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.59765625e-01,  1.56250000e-02,  9.42382812e-02,  ...,\n",
       "            3.65234375e-01,  2.32421875e-01,  1.04492188e-01],\n",
       "          [-1.40625000e-01, -1.56250000e-01, -9.76562500e-02,  ...,\n",
       "            3.22265625e-01,  3.76953125e-01,  2.13867188e-01],\n",
       "          [-3.12500000e-01, -2.87109375e-01, -2.43164062e-01,  ...,\n",
       "            3.30078125e-01,  4.27734375e-01,  3.04687500e-01],\n",
       "          ...,\n",
       "          [ 2.31445312e-01,  4.19921875e-01,  3.57421875e-01,  ...,\n",
       "            1.80664062e-01,  1.15234375e-01,  2.15820312e-01],\n",
       "          [ 2.65625000e-01,  3.78906250e-01,  4.06250000e-01,  ...,\n",
       "            1.47460938e-01,  2.08984375e-01,  2.33398438e-01],\n",
       "          [ 4.57031250e-01,  3.35937500e-01,  3.30078125e-01,  ...,\n",
       "            1.15722656e-01,  6.93359375e-02, -3.41796875e-02]],\n",
       "\n",
       "         [[-5.39062500e-01, -8.71093750e-01,  3.67187500e-01,  ...,\n",
       "            7.17773438e-02,  1.52343750e+00,  3.28125000e+00],\n",
       "          [ 1.41906738e-03, -2.21679688e-01,  1.04492188e-01,  ...,\n",
       "           -6.56250000e-01,  5.93750000e-01,  7.22656250e-01],\n",
       "          [-1.12304688e-01,  1.61132812e-01,  7.51953125e-02,  ...,\n",
       "           -8.90625000e-01,  1.21093750e+00,  1.64843750e+00],\n",
       "          ...,\n",
       "          [ 4.23828125e-01,  7.32421875e-02,  1.30859375e-01,  ...,\n",
       "            2.38281250e-01, -1.01562500e-01,  3.67187500e-01],\n",
       "          [-7.07031250e-01,  5.11718750e-01,  1.00781250e+00,  ...,\n",
       "            8.44726562e-02,  6.01562500e-01,  3.11279297e-03],\n",
       "          [ 6.44531250e-01,  2.46093750e-01,  8.78906250e-01,  ...,\n",
       "            9.29687500e-01, -3.65234375e-01,  4.60937500e-01]],\n",
       "\n",
       "         [[-1.17797852e-02, -6.34765625e-03, -3.49121094e-02,  ...,\n",
       "           -1.06811523e-02, -5.12695312e-02,  8.34960938e-02],\n",
       "          [-1.46484375e-02, -6.05468750e-02,  2.00195312e-02,  ...,\n",
       "           -4.98046875e-02, -3.78417969e-02, -4.29687500e-02],\n",
       "          [-5.10253906e-02, -7.81250000e-02, -1.30615234e-02,  ...,\n",
       "           -1.05468750e-01, -6.73828125e-02, -5.46875000e-02],\n",
       "          ...,\n",
       "          [-6.78710938e-02, -5.37109375e-02, -5.02929688e-02,  ...,\n",
       "           -3.73535156e-02, -2.20947266e-02, -1.06933594e-01],\n",
       "          [-7.12890625e-02, -2.51464844e-02, -1.08032227e-02,  ...,\n",
       "            7.66601562e-02, -1.95312500e-02,  4.88281250e-02],\n",
       "          [-8.49609375e-02, -2.73437500e-02, -4.02832031e-02,  ...,\n",
       "           -2.55126953e-02,  3.03955078e-02, -5.34667969e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.32031250e-02,  7.20214844e-03, -3.75976562e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [ 4.15039062e-02,  1.96533203e-02, -5.63964844e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [ 8.34960938e-02, -3.78417969e-02,  2.18505859e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          ...,\n",
       "          [ 6.49414062e-02,  7.66601562e-02,  9.96093750e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [ 1.12792969e-01,  1.08398438e-01,  1.18652344e-01,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [ 5.20019531e-02,  6.73828125e-02,  3.97949219e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01]],\n",
       "\n",
       "         [[ 5.44433594e-02,  4.32128906e-02,  6.49414062e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 2.27050781e-02,  3.19480896e-05,  6.93359375e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 2.03125000e-01, -8.88824463e-04,  4.10156250e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          ...,\n",
       "          [ 5.34667969e-02,  7.08007812e-02, -9.57031250e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 5.85937500e-02,  1.77001953e-02,  5.41992188e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [-8.17871094e-03,  1.31225586e-02,  3.71093750e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02]],\n",
       "\n",
       "         [[ 3.47656250e-01,  1.28906250e-01, -3.10546875e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [ 2.19726562e-01,  1.42578125e-01, -4.57031250e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [ 1.01562500e+00, -3.22265625e-01, -3.59375000e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          ...,\n",
       "          [ 2.81250000e-01,  1.93359375e-01,  3.71093750e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [ 6.49414062e-02,  2.63671875e-02,  3.26171875e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [ 6.59179688e-02, -1.44531250e-01,  1.97265625e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.49023438e-01, -2.51953125e-01, -2.98828125e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [ 7.75146484e-03, -1.93359375e-01, -3.80859375e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [ 1.04980469e-01, -3.24218750e-01, -2.71484375e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          ...,\n",
       "          [ 1.76757812e-01,  1.23535156e-01,  1.25976562e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [ 1.14257812e-01,  2.09960938e-01,  2.92968750e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [ 1.11816406e-01,  5.56640625e-02,  1.66015625e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01]],\n",
       "\n",
       "         [[-1.28906250e+00, -2.08007812e-01, -5.70312500e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [ 1.16406250e+00, -3.69140625e-01,  3.35937500e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [ 2.29687500e+00,  7.89062500e-01, -4.82421875e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          ...,\n",
       "          [-4.66308594e-02,  4.57031250e-01, -4.84375000e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [-1.57226562e-01,  1.23046875e-01,  2.59765625e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [ 3.08593750e-01, -3.69140625e-01,  1.96289062e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01]],\n",
       "\n",
       "         [[ 2.64892578e-02, -4.63867188e-03,  1.40380859e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [-1.06048584e-03, -1.19018555e-02,  2.78320312e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [-2.67333984e-02, -2.63671875e-02,  7.76367188e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          ...,\n",
       "          [-1.86767578e-02,  6.49414062e-02, -5.81054688e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [-7.87353516e-03, -3.12500000e-02,  2.62451172e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [ 1.41601562e-02,  2.23388672e-02, -2.72216797e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02]]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1.5507771, 1.477785 , 1.4047928, 1.3172023, 1.2442101, 1.2296118,\n",
       "        1.2442101, 1.2442101, 1.273407 , 1.273407 ],\n",
       "       [1.5215802, 1.4631865, 1.3901944, 1.3172023, 1.2442101, 1.2296118,\n",
       "        1.2442101, 1.2442101, 1.273407 , 1.273407 ],\n",
       "       [1.5069818, 1.4339896, 1.3609976, 1.2880055, 1.2296118, 1.2296118,\n",
       "        1.2442101, 1.2442101, 1.273407 , 1.273407 ],\n",
       "       [1.477785 , 1.3901944, 1.3172023, 1.2442101, 1.2150133, 1.2296118,\n",
       "        1.2442101, 1.2442101, 1.273407 , 1.273407 ],\n",
       "       [1.4047928, 1.3463992, 1.3026038, 1.2442101, 1.2150133, 1.2296118,\n",
       "        1.2442101, 1.2442101, 1.273407 , 1.273407 ],\n",
       "       [1.3318007, 1.3026038, 1.2880055, 1.273407 , 1.273407 , 1.2588086,\n",
       "        1.2588086, 1.2588086, 1.2880055, 1.2880055],\n",
       "       [1.2880055, 1.2880055, 1.2880055, 1.2880055, 1.2880055, 1.273407 ,\n",
       "        1.273407 , 1.273407 , 1.2880055, 1.2880055],\n",
       "       [1.2880055, 1.2880055, 1.2880055, 1.2880055, 1.2880055, 1.273407 ,\n",
       "        1.273407 , 1.273407 , 1.2880055, 1.2880055],\n",
       "       [1.2296118, 1.2588086, 1.2880055, 1.3026038, 1.3026038, 1.2880055,\n",
       "        1.273407 , 1.273407 , 1.2880055, 1.2880055],\n",
       "       [1.1858164, 1.2150133, 1.2588086, 1.273407 , 1.273407 , 1.273407 ,\n",
       "        1.273407 , 1.273407 , 1.2880055, 1.2880055]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_jax.pixel_values[0,0,0,0,:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_modules at 0x7ef9b8d478b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_torch.named_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43moutput_torch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlast_hidden_state\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[:, :, :, \u001b[38;5;241m0\u001b[39m, i:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m10\u001b[39m)]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "output_torch['last_hidden_state'][:, :, :, 0, i:(i+10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[5.65625, -5.25, -5.375, 5.8125, 4.90625, -4.21875, 1.55469,\n",
       "          -3.15625, 0.984375, 3.90625],\n",
       "         [6.34375, -3.03125, -6.4375, 6.03125, 7.8125, -3.59375,\n",
       "          2.48438, -7.15625, -0.753906, 4.1875],\n",
       "         [14, -4.09375, -7.625, 2.76562, 11.8125, -10.5, 0.722656,\n",
       "          -6.59375, 0.808594, 5.59375],\n",
       "         [4.21875, 3.5, -2.51562, -3.89062, -1.25, 0.566406, 4.65625,\n",
       "          2.09375, 8.8125, 4.5625]]]], dtype=bfloat16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_jax1['last_hidden_state'][:, :, :, 0, i:(i+10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[5.65625, -5.25, -5.375, 5.8125, 4.90625, -4.21875, 1.55469,\n",
       "          -3.15625, 0.984375, 3.90625],\n",
       "         [6.34375, -3.03125, -6.4375, 6.03125, 7.8125, -3.59375,\n",
       "          2.48438, -7.15625, -0.753906, 4.1875],\n",
       "         [14, -4.09375, -7.625, 2.76562, 11.8125, -10.5, 0.722656,\n",
       "          -6.59375, 0.808594, 5.59375],\n",
       "         [4.21875, 3.5, -2.51562, -3.89062, -1.25, 0.566406, 4.65625,\n",
       "          2.09375, 8.8125, 4.5625]]]], dtype=bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_jax2['last_hidden_state'][:, :, :, 0, i:(i+10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import tree_util\n",
    "\n",
    "def print_detailed_info(name, param):\n",
    "    print(f\"{name}   {param.shape}; {param.dtype}\")\n",
    "\n",
    "tree_util.tree_map_with_path(\n",
    "    lambda path, x: print_detailed_info(\"\".join(str(p) for p in path), x),\n",
    "    model.params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_torch.named_parameters():\n",
    "    print(f\"[\\\"{name}\\\"]   {param.shape}; {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_torch.state_dict()[\"transformer.layers.31.self_attn.q_proj.weight\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params['vision_model']['transformer']['layers.31']['self_attn']['q_proj']['kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_jax['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_jax['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
