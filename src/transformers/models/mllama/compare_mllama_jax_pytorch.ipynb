{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00,  7.15it/s]\n",
      "E1123 01:09:23.771134 1124874 buffer_comparator.cc:157] Difference at 19685: -inf, expected -2.88442e+38\n",
      "E1123 01:09:23.771169 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.771173 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.771175 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.771178 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.771270 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.771273 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.771275 1124874 buffer_comparator.cc:157] Difference at 83325: inf, expected 6.18091e+37\n",
      "E1123 01:09:23.771278 1124874 buffer_comparator.cc:157] Difference at 83399: 2.21981e+38, expected inf\n",
      "E1123 01:09:23.771280 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "2024-11-23 01:09:23.771290: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.773549 1124874 buffer_comparator.cc:157] Difference at 19473: -inf, expected -3.01735e+38\n",
      "E1123 01:09:23.773562 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.773565 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.773567 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.773570 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.773661 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.773664 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.773667 1124874 buffer_comparator.cc:157] Difference at 83325: inf, expected 6.18091e+37\n",
      "E1123 01:09:23.773669 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.773671 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "2024-11-23 01:09:23.773675: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.776273 1124874 buffer_comparator.cc:157] Difference at 19473: -inf, expected -3.01735e+38\n",
      "E1123 01:09:23.776285 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.776287 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.776290 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.776292 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.776294 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.776297 1124874 buffer_comparator.cc:157] Difference at 21477: -inf, expected -2.81796e+38\n",
      "E1123 01:09:23.776299 1124874 buffer_comparator.cc:157] Difference at 21523: inf, expected 3.24332e+38\n",
      "E1123 01:09:23.776391 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.776394 1124874 buffer_comparator.cc:157] Difference at 83251: -inf, expected 3.43938e+37\n",
      "2024-11-23 01:09:23.776400: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.778769 1124874 buffer_comparator.cc:157] Difference at 19473: -inf, expected -3.01735e+38\n",
      "E1123 01:09:23.778780 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.778783 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.778786 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.778788 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.778879 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.778883 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.778885 1124874 buffer_comparator.cc:157] Difference at 83325: inf, expected 6.18091e+37\n",
      "E1123 01:09:23.778887 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.778889 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "2024-11-23 01:09:23.778893: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.781533 1124874 buffer_comparator.cc:157] Difference at 19473: -inf, expected -3.01735e+38\n",
      "E1123 01:09:23.781542 1124874 buffer_comparator.cc:157] Difference at 19685: -inf, expected -2.88442e+38\n",
      "E1123 01:09:23.781545 1124874 buffer_comparator.cc:157] Difference at 20111: -inf, expected -3.08381e+38\n",
      "E1123 01:09:23.781547 1124874 buffer_comparator.cc:157] Difference at 20665: -inf, expected -2.76479e+38\n",
      "E1123 01:09:23.781550 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.781552 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.781554 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.781557 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.781648 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.781651 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "2024-11-23 01:09:23.781655: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.784123 1124874 buffer_comparator.cc:157] Difference at 19685: -inf, expected -2.88442e+38\n",
      "E1123 01:09:23.784134 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.784137 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.784139 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.784142 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.784234 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.784237 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.784239 1124874 buffer_comparator.cc:157] Difference at 83325: inf, expected 6.18091e+37\n",
      "E1123 01:09:23.784242 1124874 buffer_comparator.cc:157] Difference at 83399: 2.21981e+38, expected inf\n",
      "E1123 01:09:23.784244 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "2024-11-23 01:09:23.784247: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.787259 1124874 buffer_comparator.cc:157] Difference at 19473: -inf, expected -3.01735e+38\n",
      "E1123 01:09:23.787273 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.787275 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.787278 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.787280 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.787282 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.787285 1124874 buffer_comparator.cc:157] Difference at 21477: -inf, expected -2.81796e+38\n",
      "E1123 01:09:23.787287 1124874 buffer_comparator.cc:157] Difference at 21523: inf, expected 3.24332e+38\n",
      "E1123 01:09:23.787378 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.787381 1124874 buffer_comparator.cc:157] Difference at 83251: -inf, expected 3.43938e+37\n",
      "2024-11-23 01:09:23.787384: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.789797 1124874 buffer_comparator.cc:157] Difference at 19685: -inf, expected -2.88442e+38\n",
      "E1123 01:09:23.789807 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.789810 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.789812 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.789814 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.789906 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.789909 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.789911 1124874 buffer_comparator.cc:157] Difference at 83325: inf, expected 6.18091e+37\n",
      "E1123 01:09:23.789913 1124874 buffer_comparator.cc:157] Difference at 83399: 2.21981e+38, expected inf\n",
      "E1123 01:09:23.789915 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "2024-11-23 01:09:23.789919: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.792110 1124874 buffer_comparator.cc:157] Difference at 19473: -inf, expected -3.01735e+38\n",
      "E1123 01:09:23.792122 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.792125 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.792127 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.792130 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.792222 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.792225 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.792227 1124874 buffer_comparator.cc:157] Difference at 83325: inf, expected 6.18091e+37\n",
      "E1123 01:09:23.792229 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.792232 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "2024-11-23 01:09:23.792235: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.794813 1124874 buffer_comparator.cc:157] Difference at 19473: -inf, expected -3.01735e+38\n",
      "E1123 01:09:23.794826 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.794828 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.794831 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.794833 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.794835 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.794838 1124874 buffer_comparator.cc:157] Difference at 21477: -inf, expected -2.81796e+38\n",
      "E1123 01:09:23.794840 1124874 buffer_comparator.cc:157] Difference at 21523: inf, expected 3.24332e+38\n",
      "E1123 01:09:23.794931 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.794934 1124874 buffer_comparator.cc:157] Difference at 83251: -inf, expected 3.43938e+37\n",
      "2024-11-23 01:09:23.794938: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.797300 1124874 buffer_comparator.cc:157] Difference at 19473: -inf, expected -3.01735e+38\n",
      "E1123 01:09:23.797310 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.797313 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.797315 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.797318 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.797409 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.797412 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.797415 1124874 buffer_comparator.cc:157] Difference at 83325: inf, expected 6.18091e+37\n",
      "E1123 01:09:23.797417 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.797419 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "2024-11-23 01:09:23.797423: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.800057 1124874 buffer_comparator.cc:157] Difference at 19473: -inf, expected -3.01735e+38\n",
      "E1123 01:09:23.800067 1124874 buffer_comparator.cc:157] Difference at 19685: -inf, expected -2.88442e+38\n",
      "E1123 01:09:23.800069 1124874 buffer_comparator.cc:157] Difference at 20111: -inf, expected -3.08381e+38\n",
      "E1123 01:09:23.800072 1124874 buffer_comparator.cc:157] Difference at 20665: -inf, expected -2.76479e+38\n",
      "E1123 01:09:23.800075 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.800077 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.800079 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.800082 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.800173 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.800176 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "2024-11-23 01:09:23.800180: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.802642 1124874 buffer_comparator.cc:157] Difference at 19685: -inf, expected -2.88442e+38\n",
      "E1123 01:09:23.802653 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.802656 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.802658 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.802660 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.802752 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.802755 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.802757 1124874 buffer_comparator.cc:157] Difference at 83325: inf, expected 6.18091e+37\n",
      "E1123 01:09:23.802760 1124874 buffer_comparator.cc:157] Difference at 83399: 2.21981e+38, expected inf\n",
      "E1123 01:09:23.802762 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "2024-11-23 01:09:23.802765: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.805763 1124874 buffer_comparator.cc:157] Difference at 19473: -inf, expected -3.01735e+38\n",
      "E1123 01:09:23.805774 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.805777 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.805779 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.805781 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.805784 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.805786 1124874 buffer_comparator.cc:157] Difference at 21477: -inf, expected -2.81796e+38\n",
      "E1123 01:09:23.805788 1124874 buffer_comparator.cc:157] Difference at 21523: inf, expected 3.24332e+38\n",
      "E1123 01:09:23.805879 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.805882 1124874 buffer_comparator.cc:157] Difference at 83251: -inf, expected 3.43938e+37\n",
      "2024-11-23 01:09:23.805886: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.807499 1124874 buffer_comparator.cc:157] Difference at 20111: -inf, expected -3.08381e+38\n",
      "E1123 01:09:23.807511 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.807514 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.807518 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.807610 1124874 buffer_comparator.cc:157] Difference at 83329: -9.70336e+37, expected -inf\n",
      "E1123 01:09:23.807615 1124874 buffer_comparator.cc:157] Difference at 83335: -inf, expected -2.56541e+38\n",
      "E1123 01:09:23.807617 1124874 buffer_comparator.cc:157] Difference at 83345: -inf, expected -2.41919e+38\n",
      "E1123 01:09:23.807619 1124874 buffer_comparator.cc:157] Difference at 83347: 1.43557e+38, expected inf\n",
      "E1123 01:09:23.807621 1124874 buffer_comparator.cc:157] Difference at 83353: inf, expected 4.38645e+37\n",
      "E1123 01:09:23.807624 1124874 buffer_comparator.cc:157] Difference at 83361: 9.03875e+37, expected inf\n",
      "2024-11-23 01:09:23.807627: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.809233 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.809242 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.809245 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.809247 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.809339 1124874 buffer_comparator.cc:157] Difference at 83267: 2.04701e+38, expected inf\n",
      "E1123 01:09:23.809342 1124874 buffer_comparator.cc:157] Difference at 83281: -inf, expected 3.90461e+37\n",
      "E1123 01:09:23.809344 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.809346 1124874 buffer_comparator.cc:157] Difference at 83289: -inf, expected -1.74129e+38\n",
      "E1123 01:09:23.809348 1124874 buffer_comparator.cc:157] Difference at 83301: -inf, expected -2.48566e+38\n",
      "E1123 01:09:23.809350 1124874 buffer_comparator.cc:157] Difference at 83405: inf, expected 2.31286e+38\n",
      "2024-11-23 01:09:23.809354: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.810993 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.811003 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.811006 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.811008 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.811100 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.811105 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "E1123 01:09:23.811107 1124874 buffer_comparator.cc:157] Difference at 83697: inf, expected 2.87113e+38\n",
      "E1123 01:09:23.811110 1124874 buffer_comparator.cc:157] Difference at 83837: inf, expected -2.44578e+38\n",
      "E1123 01:09:23.811112 1124874 buffer_comparator.cc:157] Difference at 83843: -inf, expected -1.28935e+38\n",
      "E1123 01:09:23.811114 1124874 buffer_comparator.cc:157] Difference at 83861: -inf, expected 9.03875e+37\n",
      "2024-11-23 01:09:23.811118: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.812714 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.812723 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.812726 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.812728 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.812820 1124874 buffer_comparator.cc:157] Difference at 83267: 2.04701e+38, expected inf\n",
      "E1123 01:09:23.812823 1124874 buffer_comparator.cc:157] Difference at 83281: -inf, expected 3.90461e+37\n",
      "E1123 01:09:23.812825 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.812827 1124874 buffer_comparator.cc:157] Difference at 83289: -inf, expected -1.74129e+38\n",
      "E1123 01:09:23.812829 1124874 buffer_comparator.cc:157] Difference at 83301: -inf, expected -2.48566e+38\n",
      "E1123 01:09:23.812831 1124874 buffer_comparator.cc:157] Difference at 83405: inf, expected 2.31286e+38\n",
      "2024-11-23 01:09:23.812834: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.814430 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.814439 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.814441 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.814443 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.814535 1124874 buffer_comparator.cc:157] Difference at 83267: 2.04701e+38, expected inf\n",
      "E1123 01:09:23.814538 1124874 buffer_comparator.cc:157] Difference at 83281: -inf, expected 3.90461e+37\n",
      "E1123 01:09:23.814540 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.814542 1124874 buffer_comparator.cc:157] Difference at 83289: -inf, expected -1.74129e+38\n",
      "E1123 01:09:23.814544 1124874 buffer_comparator.cc:157] Difference at 83301: -inf, expected -2.48566e+38\n",
      "E1123 01:09:23.814546 1124874 buffer_comparator.cc:157] Difference at 83405: inf, expected 2.31286e+38\n",
      "2024-11-23 01:09:23.814550: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.816150 1124874 buffer_comparator.cc:157] Difference at 20111: -inf, expected -3.08381e+38\n",
      "E1123 01:09:23.816158 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.816161 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.816164 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.816255 1124874 buffer_comparator.cc:157] Difference at 83329: -9.70336e+37, expected -inf\n",
      "E1123 01:09:23.816258 1124874 buffer_comparator.cc:157] Difference at 83335: -inf, expected -2.56541e+38\n",
      "E1123 01:09:23.816261 1124874 buffer_comparator.cc:157] Difference at 83345: -inf, expected -2.41919e+38\n",
      "E1123 01:09:23.816263 1124874 buffer_comparator.cc:157] Difference at 83347: 1.43557e+38, expected inf\n",
      "E1123 01:09:23.816265 1124874 buffer_comparator.cc:157] Difference at 83353: inf, expected 4.38645e+37\n",
      "E1123 01:09:23.816267 1124874 buffer_comparator.cc:157] Difference at 83361: 9.03875e+37, expected inf\n",
      "2024-11-23 01:09:23.816270: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.817873 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.817881 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.817884 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.817886 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.817978 1124874 buffer_comparator.cc:157] Difference at 83267: 2.04701e+38, expected inf\n",
      "E1123 01:09:23.817981 1124874 buffer_comparator.cc:157] Difference at 83281: -inf, expected 3.90461e+37\n",
      "E1123 01:09:23.817983 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.817985 1124874 buffer_comparator.cc:157] Difference at 83289: -inf, expected -1.74129e+38\n",
      "E1123 01:09:23.817987 1124874 buffer_comparator.cc:157] Difference at 83301: -inf, expected -2.48566e+38\n",
      "E1123 01:09:23.817989 1124874 buffer_comparator.cc:157] Difference at 83405: inf, expected 2.31286e+38\n",
      "2024-11-23 01:09:23.817992: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.819753 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.819763 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.819765 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.819768 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.819860 1124874 buffer_comparator.cc:157] Difference at 83381: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.819864 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.819866 1124874 buffer_comparator.cc:157] Difference at 83411: -inf, expected -2.19323e+38\n",
      "E1123 01:09:23.819868 1124874 buffer_comparator.cc:157] Difference at 83427: -inf, expected -2.25969e+38\n",
      "E1123 01:09:23.819870 1124874 buffer_comparator.cc:157] Difference at 83435: -inf, expected 1.01686e+38\n",
      "E1123 01:09:23.819872 1124874 buffer_comparator.cc:157] Difference at 83443: inf, expected 3.0971e+38\n",
      "2024-11-23 01:09:23.819876: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.821531 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.821539 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.821542 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.821634 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.821637 1124874 buffer_comparator.cc:157] Difference at 83381: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.821640 1124874 buffer_comparator.cc:157] Difference at 83399: 2.21981e+38, expected inf\n",
      "E1123 01:09:23.821642 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.821644 1124874 buffer_comparator.cc:157] Difference at 83411: -inf, expected -2.19323e+38\n",
      "E1123 01:09:23.821646 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.821648 1124874 buffer_comparator.cc:157] Difference at 83427: -inf, expected -2.25969e+38\n",
      "2024-11-23 01:09:23.821651: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.823443 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.823451 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.823454 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.823457 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.823548 1124874 buffer_comparator.cc:157] Difference at 83381: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.823552 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.823554 1124874 buffer_comparator.cc:157] Difference at 83411: -inf, expected -2.19323e+38\n",
      "E1123 01:09:23.823556 1124874 buffer_comparator.cc:157] Difference at 83427: -inf, expected -2.25969e+38\n",
      "E1123 01:09:23.823558 1124874 buffer_comparator.cc:157] Difference at 83435: -inf, expected 1.01686e+38\n",
      "E1123 01:09:23.823560 1124874 buffer_comparator.cc:157] Difference at 83443: inf, expected 3.0971e+38\n",
      "2024-11-23 01:09:23.823563: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.825328 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.825335 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.825338 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.825340 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.825432 1124874 buffer_comparator.cc:157] Difference at 83267: 2.04701e+38, expected inf\n",
      "E1123 01:09:23.825435 1124874 buffer_comparator.cc:157] Difference at 83281: -inf, expected 3.90461e+37\n",
      "E1123 01:09:23.825437 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.825439 1124874 buffer_comparator.cc:157] Difference at 83289: -inf, expected -1.74129e+38\n",
      "E1123 01:09:23.825441 1124874 buffer_comparator.cc:157] Difference at 83301: -inf, expected -2.48566e+38\n",
      "E1123 01:09:23.825443 1124874 buffer_comparator.cc:157] Difference at 83305: inf, expected 2.03372e+38\n",
      "2024-11-23 01:09:23.825447: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.827104 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.827112 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.827115 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.827117 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.827209 1124874 buffer_comparator.cc:157] Difference at 83267: 2.04701e+38, expected inf\n",
      "E1123 01:09:23.827212 1124874 buffer_comparator.cc:157] Difference at 83281: -inf, expected 3.90461e+37\n",
      "E1123 01:09:23.827214 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.827216 1124874 buffer_comparator.cc:157] Difference at 83289: -inf, expected -1.74129e+38\n",
      "E1123 01:09:23.827218 1124874 buffer_comparator.cc:157] Difference at 83301: -inf, expected -2.48566e+38\n",
      "E1123 01:09:23.827220 1124874 buffer_comparator.cc:157] Difference at 83305: inf, expected 2.03372e+38\n",
      "2024-11-23 01:09:23.827223: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.828816 1124874 buffer_comparator.cc:157] Difference at 20111: -inf, expected -3.08381e+38\n",
      "E1123 01:09:23.828825 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.828828 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.828830 1124874 buffer_comparator.cc:157] Difference at 21477: -inf, expected -2.81796e+38\n",
      "E1123 01:09:23.828833 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.828923 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.828927 1124874 buffer_comparator.cc:157] Difference at 83329: -9.70336e+37, expected -inf\n",
      "E1123 01:09:23.828929 1124874 buffer_comparator.cc:157] Difference at 83335: -inf, expected -2.56541e+38\n",
      "E1123 01:09:23.828931 1124874 buffer_comparator.cc:157] Difference at 83345: -inf, expected -2.41919e+38\n",
      "E1123 01:09:23.828933 1124874 buffer_comparator.cc:157] Difference at 83347: 1.43557e+38, expected inf\n",
      "2024-11-23 01:09:23.828936: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.830524 1124874 buffer_comparator.cc:157] Difference at 20111: -inf, expected -3.08381e+38\n",
      "E1123 01:09:23.830534 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.830536 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.830539 1124874 buffer_comparator.cc:157] Difference at 21477: -inf, expected -2.81796e+38\n",
      "E1123 01:09:23.830541 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.830632 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.830636 1124874 buffer_comparator.cc:157] Difference at 83329: -9.70336e+37, expected -inf\n",
      "E1123 01:09:23.830638 1124874 buffer_comparator.cc:157] Difference at 83335: -inf, expected -2.56541e+38\n",
      "E1123 01:09:23.830640 1124874 buffer_comparator.cc:157] Difference at 83345: -inf, expected -2.41919e+38\n",
      "E1123 01:09:23.830642 1124874 buffer_comparator.cc:157] Difference at 83347: 1.43557e+38, expected inf\n",
      "2024-11-23 01:09:23.830645: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.832214 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.832222 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.832224 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.832227 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.832230 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.832321 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.832324 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.832326 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.832329 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.832331 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.832335: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.833872 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.833880 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.833883 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.833885 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.833976 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.833980 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.833982 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.833984 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.833987 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.833989 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.833993: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.835524 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.835533 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.835535 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.835538 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.835540 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.835631 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.835635 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.835637 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.835639 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.835641 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.835644: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.837210 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.837218 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.837220 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.837222 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.837225 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.837316 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.837319 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.837321 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.837324 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.837326 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.837329: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.838855 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.838865 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.838867 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.838870 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.838872 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.838963 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.838966 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.838968 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.838970 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.838972 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.838976: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.840509 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.840518 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.840520 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.840522 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.840525 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.840616 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.840619 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.840621 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.840623 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.840626 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.840630: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.842201 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.842210 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.842212 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.842215 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.842306 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.842309 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.842311 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.842313 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.842316 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.842319 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.842323: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.843879 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.843888 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.843890 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.843893 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.843895 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.843986 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.843990 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.843992 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.843994 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.843996 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.843999: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.845565 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.845574 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.845577 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.845579 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.845670 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.845673 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.845675 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.845677 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.845679 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.845682 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.845685: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.847265 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.847274 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.847276 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.847278 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.847281 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.847372 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.847375 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.847377 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.847379 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.847381 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.847388: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.848963 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.848972 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.848975 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.848977 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.848980 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.849071 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.849074 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.849076 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.849078 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.849080 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.849084: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.850673 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.850681 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.850684 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.850686 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.850689 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.850780 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.850783 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.850785 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.850787 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.850789 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.850793: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.852344 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.852352 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.852354 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.852357 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.852360 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.852450 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.852454 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.852456 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.852458 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.852460 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.852464: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.854067 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.854076 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.854078 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.854081 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.854172 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.854175 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.854177 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.854179 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.854181 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.854184 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.854187: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.855725 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.855735 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.855737 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.855740 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.855743 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.855833 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.855837 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.855839 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.855841 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.855843 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.855847: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.857428 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.857436 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.857438 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.857441 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.857444 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.857535 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.857538 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.857540 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.857542 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.857544 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.857547: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.859098 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.859120 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.859123 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.859125 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.859216 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.859219 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.859221 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.859224 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.859226 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.859228 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.859233: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.860815 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.860823 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.860825 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.860828 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.860918 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.860922 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.860924 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.860926 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.860928 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.860930 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.860933: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.862509 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.862517 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.862520 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.862522 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.862613 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.862617 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.862619 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.862621 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.862623 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.862625 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.862628: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.864212 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.864220 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.864223 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.864225 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.864316 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.864320 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.864322 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.864324 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.864326 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.864328 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.864332: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.865917 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.865925 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.865927 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.865930 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.865932 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.866023 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.866026 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.866028 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.866030 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.866033 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.866037: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.867623 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.867633 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.867636 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.867638 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.867641 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.867732 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.867735 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.867737 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.867739 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.867742 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.867745: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.869333 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.869342 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.869344 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.869347 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.869438 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.869441 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.869443 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.869445 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.869447 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.869450 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.869453: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.871045 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.871055 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.871058 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.871060 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.871152 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.871155 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.871157 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.871159 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.871161 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.871163 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.871167: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.872780 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.872789 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.872791 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.872793 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.872796 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.872887 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.872890 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.872892 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.872894 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.872896 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.872900: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.874495 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.874505 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.874507 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.874510 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.874513 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.874603 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.874607 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.874609 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.874611 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.874613 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.874617: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.876211 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.876220 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.876222 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.876225 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.876228 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.876319 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.876322 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.876324 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.876326 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.876328 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.876331: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.879217 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.879319 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.879323 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.879325 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.879328 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.879330 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.879332 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "E1123 01:09:23.879335 1124874 buffer_comparator.cc:157] Difference at 83697: inf, expected 2.87113e+38\n",
      "E1123 01:09:23.879337 1124874 buffer_comparator.cc:157] Difference at 83723: -inf, expected -2.76479e+38\n",
      "E1123 01:09:23.879339 1124874 buffer_comparator.cc:157] Difference at 83777: -2.32615e+38, expected -inf\n",
      "2024-11-23 01:09:23.879343: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.881005 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.881013 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.881016 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.881019 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.881110 1124874 buffer_comparator.cc:157] Difference at 83381: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.881114 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.881116 1124874 buffer_comparator.cc:157] Difference at 83411: -inf, expected -2.19323e+38\n",
      "E1123 01:09:23.881118 1124874 buffer_comparator.cc:157] Difference at 83427: -inf, expected -2.25969e+38\n",
      "E1123 01:09:23.881120 1124874 buffer_comparator.cc:157] Difference at 83435: -inf, expected 1.01686e+38\n",
      "E1123 01:09:23.881122 1124874 buffer_comparator.cc:157] Difference at 83443: inf, expected 3.0971e+38\n",
      "2024-11-23 01:09:23.881126: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.882766 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.882775 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.882778 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.882781 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.882873 1124874 buffer_comparator.cc:157] Difference at 83381: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.882877 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.882879 1124874 buffer_comparator.cc:157] Difference at 83411: -inf, expected -2.19323e+38\n",
      "E1123 01:09:23.882881 1124874 buffer_comparator.cc:157] Difference at 83427: -inf, expected -2.25969e+38\n",
      "E1123 01:09:23.882883 1124874 buffer_comparator.cc:157] Difference at 83435: -inf, expected 1.01686e+38\n",
      "E1123 01:09:23.882885 1124874 buffer_comparator.cc:157] Difference at 83443: inf, expected 3.0971e+38\n",
      "2024-11-23 01:09:23.882889: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.884534 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.884542 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.884544 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.884547 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.884641 1124874 buffer_comparator.cc:157] Difference at 83843: -inf, expected -1.28935e+38\n",
      "E1123 01:09:23.884645 1124874 buffer_comparator.cc:157] Difference at 83861: -inf, expected 9.03875e+37\n",
      "E1123 01:09:23.884647 1124874 buffer_comparator.cc:157] Difference at 83877: -inf, expected -1.65489e+38\n",
      "E1123 01:09:23.884649 1124874 buffer_comparator.cc:157] Difference at 83911: inf, expected 3.36295e+38\n",
      "E1123 01:09:23.884651 1124874 buffer_comparator.cc:157] Difference at 83917: 2.65846e+38, expected inf\n",
      "E1123 01:09:23.884654 1124874 buffer_comparator.cc:157] Difference at 83925: 2.44578e+38, expected inf\n",
      "2024-11-23 01:09:23.884658: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.886256 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.886264 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.886267 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.886269 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.886368 1124874 buffer_comparator.cc:157] Difference at 83521: -inf, expected -2.28627e+38\n",
      "E1123 01:09:23.886372 1124874 buffer_comparator.cc:157] Difference at 83535: inf, expected 2.68504e+38\n",
      "E1123 01:09:23.886374 1124874 buffer_comparator.cc:157] Difference at 83565: 1.22289e+38, expected inf\n",
      "E1123 01:09:23.886377 1124874 buffer_comparator.cc:157] Difference at 83585: -inf, expected inf\n",
      "E1123 01:09:23.886378 1124874 buffer_comparator.cc:157] Difference at 83593: -2.87113e+38, expected -inf\n",
      "E1123 01:09:23.886381 1124874 buffer_comparator.cc:157] Difference at 83605: inf, expected 2.45907e+38\n",
      "2024-11-23 01:09:23.886385: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.888239 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.888248 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.888251 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.888253 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.888344 1124874 buffer_comparator.cc:157] Difference at 83213: -inf, expected -1.15643e+38\n",
      "E1123 01:09:23.888347 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.888350 1124874 buffer_comparator.cc:157] Difference at 83245: -6.31383e+37, expected -inf\n",
      "E1123 01:09:23.888352 1124874 buffer_comparator.cc:157] Difference at 83255: inf, expected 8.39075e+36\n",
      "E1123 01:09:23.888354 1124874 buffer_comparator.cc:157] Difference at 83257: -inf, expected 2.2331e+38\n",
      "E1123 01:09:23.888356 1124874 buffer_comparator.cc:157] Difference at 83267: 2.04701e+38, expected inf\n",
      "2024-11-23 01:09:23.888360: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.890200 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.890207 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.890210 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.890212 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.890304 1124874 buffer_comparator.cc:157] Difference at 83213: -inf, expected -1.15643e+38\n",
      "E1123 01:09:23.890307 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.890309 1124874 buffer_comparator.cc:157] Difference at 83245: -6.31383e+37, expected -inf\n",
      "E1123 01:09:23.890311 1124874 buffer_comparator.cc:157] Difference at 83255: inf, expected 8.39075e+36\n",
      "E1123 01:09:23.890313 1124874 buffer_comparator.cc:157] Difference at 83257: -inf, expected 2.2331e+38\n",
      "E1123 01:09:23.890315 1124874 buffer_comparator.cc:157] Difference at 83267: 2.04701e+38, expected inf\n",
      "2024-11-23 01:09:23.890319: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.892163 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.892175 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.892177 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.892180 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.892271 1124874 buffer_comparator.cc:157] Difference at 83213: -inf, expected -1.15643e+38\n",
      "E1123 01:09:23.892274 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.892276 1124874 buffer_comparator.cc:157] Difference at 83245: -6.31383e+37, expected -inf\n",
      "E1123 01:09:23.892278 1124874 buffer_comparator.cc:157] Difference at 83255: inf, expected 8.39075e+36\n",
      "E1123 01:09:23.892280 1124874 buffer_comparator.cc:157] Difference at 83257: -inf, expected 2.2331e+38\n",
      "E1123 01:09:23.892283 1124874 buffer_comparator.cc:157] Difference at 83267: 2.04701e+38, expected inf\n",
      "2024-11-23 01:09:23.892286: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.894108 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.894115 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.894118 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.894120 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.894212 1124874 buffer_comparator.cc:157] Difference at 83213: -inf, expected -1.15643e+38\n",
      "E1123 01:09:23.894215 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.894217 1124874 buffer_comparator.cc:157] Difference at 83245: -6.31383e+37, expected -inf\n",
      "E1123 01:09:23.894219 1124874 buffer_comparator.cc:157] Difference at 83255: inf, expected 8.39075e+36\n",
      "E1123 01:09:23.894221 1124874 buffer_comparator.cc:157] Difference at 83257: -inf, expected 2.2331e+38\n",
      "E1123 01:09:23.894223 1124874 buffer_comparator.cc:157] Difference at 83267: 2.04701e+38, expected inf\n",
      "2024-11-23 01:09:23.894226: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.895800 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.895809 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.895811 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.895813 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.895905 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.895909 1124874 buffer_comparator.cc:157] Difference at 83399: 2.21981e+38, expected inf\n",
      "E1123 01:09:23.895911 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.895913 1124874 buffer_comparator.cc:157] Difference at 83411: -inf, expected -2.19323e+38\n",
      "E1123 01:09:23.895915 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.895918 1124874 buffer_comparator.cc:157] Difference at 83435: -inf, expected 1.01686e+38\n",
      "2024-11-23 01:09:23.895921: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.897486 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.897494 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.897496 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.897498 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.897590 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.897594 1124874 buffer_comparator.cc:157] Difference at 83399: 2.21981e+38, expected inf\n",
      "E1123 01:09:23.897596 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.897598 1124874 buffer_comparator.cc:157] Difference at 83411: -inf, expected -2.19323e+38\n",
      "E1123 01:09:23.897600 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.897602 1124874 buffer_comparator.cc:157] Difference at 83435: -inf, expected 1.01686e+38\n",
      "2024-11-23 01:09:23.897605: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.899203 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.899211 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.899213 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.899215 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.899307 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.899310 1124874 buffer_comparator.cc:157] Difference at 83399: 2.21981e+38, expected inf\n",
      "E1123 01:09:23.899312 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.899315 1124874 buffer_comparator.cc:157] Difference at 83411: -inf, expected -2.19323e+38\n",
      "E1123 01:09:23.899317 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.899319 1124874 buffer_comparator.cc:157] Difference at 83435: -inf, expected 1.01686e+38\n",
      "2024-11-23 01:09:23.899322: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.900920 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.900929 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.900931 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.900934 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.901026 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.901029 1124874 buffer_comparator.cc:157] Difference at 83399: 2.21981e+38, expected inf\n",
      "E1123 01:09:23.901031 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.901033 1124874 buffer_comparator.cc:157] Difference at 83411: -inf, expected -2.19323e+38\n",
      "E1123 01:09:23.901035 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.901037 1124874 buffer_comparator.cc:157] Difference at 83435: -inf, expected 1.01686e+38\n",
      "2024-11-23 01:09:23.901040: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.902769 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.902777 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.902780 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.902782 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.902873 1124874 buffer_comparator.cc:157] Difference at 83205: -inf, expected -2.83126e+38\n",
      "E1123 01:09:23.902876 1124874 buffer_comparator.cc:157] Difference at 83213: nan, expected -1.15643e+38\n",
      "E1123 01:09:23.902879 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.902881 1124874 buffer_comparator.cc:157] Difference at 83227: inf, expected -1.50203e+38\n",
      "E1123 01:09:23.902883 1124874 buffer_comparator.cc:157] Difference at 83229: -inf, expected -2.90769e+37\n",
      "E1123 01:09:23.902885 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "2024-11-23 01:09:23.902888: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.904708 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.904716 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.904719 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.904721 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.904724 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.904726 1124874 buffer_comparator.cc:157] Difference at 21477: -inf, expected -2.81796e+38\n",
      "E1123 01:09:23.904728 1124874 buffer_comparator.cc:157] Difference at 21523: inf, expected 3.24332e+38\n",
      "E1123 01:09:23.904820 1124874 buffer_comparator.cc:157] Difference at 83251: -inf, expected 3.43938e+37\n",
      "E1123 01:09:23.904823 1124874 buffer_comparator.cc:157] Difference at 83267: 2.04701e+38, expected inf\n",
      "E1123 01:09:23.904825 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "2024-11-23 01:09:23.904829: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.906546 1124874 buffer_comparator.cc:157] Difference at 20111: -inf, expected -3.08381e+38\n",
      "E1123 01:09:23.906555 1124874 buffer_comparator.cc:157] Difference at 20665: -inf, expected -2.76479e+38\n",
      "E1123 01:09:23.906557 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.906560 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.906562 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.906564 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.906567 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.906658 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.906661 1124874 buffer_comparator.cc:157] Difference at 83325: inf, expected 6.18091e+37\n",
      "E1123 01:09:23.906664 1124874 buffer_comparator.cc:157] Difference at 83327: inf, expected 3.04393e+38\n",
      "2024-11-23 01:09:23.906667: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.908312 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.908320 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.908322 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.908325 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.908416 1124874 buffer_comparator.cc:157] Difference at 83205: -inf, expected -2.83126e+38\n",
      "E1123 01:09:23.908419 1124874 buffer_comparator.cc:157] Difference at 83213: nan, expected -1.15643e+38\n",
      "E1123 01:09:23.908421 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.908423 1124874 buffer_comparator.cc:157] Difference at 83227: inf, expected -1.50203e+38\n",
      "E1123 01:09:23.908425 1124874 buffer_comparator.cc:157] Difference at 83229: -inf, expected -2.90769e+37\n",
      "E1123 01:09:23.908428 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "2024-11-23 01:09:23.908431: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.910156 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.910164 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.910166 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.910169 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.910261 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.910264 1124874 buffer_comparator.cc:157] Difference at 83399: 2.21981e+38, expected inf\n",
      "E1123 01:09:23.910266 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.910268 1124874 buffer_comparator.cc:157] Difference at 83411: -inf, expected -2.19323e+38\n",
      "E1123 01:09:23.910270 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.910272 1124874 buffer_comparator.cc:157] Difference at 83435: -inf, expected 1.01686e+38\n",
      "2024-11-23 01:09:23.910275: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.911893 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.911901 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.911904 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.911906 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.911909 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.912000 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.912003 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.912005 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.912008 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.912010 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.912013: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.913625 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.913633 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.913635 1124874 buffer_comparator.cc:157] Difference at 20965: -inf, expected -3.04393e+38\n",
      "E1123 01:09:23.913638 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.913729 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.913733 1124874 buffer_comparator.cc:157] Difference at 83399: 2.21981e+38, expected inf\n",
      "E1123 01:09:23.913735 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.913737 1124874 buffer_comparator.cc:157] Difference at 83411: -inf, expected -2.19323e+38\n",
      "E1123 01:09:23.913739 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.913741 1124874 buffer_comparator.cc:157] Difference at 83435: -inf, expected 1.01686e+38\n",
      "2024-11-23 01:09:23.913744: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.915337 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.915346 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.915348 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.915351 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.915353 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.915444 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.915452 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.915454 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.915456 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.915458 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.915462: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.917011 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.917018 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.917020 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.917023 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.917026 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.917117 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.917120 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.917122 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.917124 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.917126 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.917129: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.918677 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.918685 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.918687 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.918690 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.918693 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.918783 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.918787 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.918789 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.918791 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.918793 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.918796: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.920499 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.920508 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.920511 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.920513 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.920605 1124874 buffer_comparator.cc:157] Difference at 83205: -inf, expected -2.83126e+38\n",
      "E1123 01:09:23.920608 1124874 buffer_comparator.cc:157] Difference at 83213: nan, expected -1.15643e+38\n",
      "E1123 01:09:23.920610 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.920612 1124874 buffer_comparator.cc:157] Difference at 83227: inf, expected -1.50203e+38\n",
      "E1123 01:09:23.920614 1124874 buffer_comparator.cc:157] Difference at 83229: -inf, expected -2.90769e+37\n",
      "E1123 01:09:23.920616 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "2024-11-23 01:09:23.920619: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.922363 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.922372 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.922374 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.922377 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.922468 1124874 buffer_comparator.cc:157] Difference at 83205: -inf, expected -2.83126e+38\n",
      "E1123 01:09:23.922471 1124874 buffer_comparator.cc:157] Difference at 83213: nan, expected -1.15643e+38\n",
      "E1123 01:09:23.922473 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.922476 1124874 buffer_comparator.cc:157] Difference at 83227: inf, expected -1.50203e+38\n",
      "E1123 01:09:23.922478 1124874 buffer_comparator.cc:157] Difference at 83229: -inf, expected -2.90769e+37\n",
      "E1123 01:09:23.922480 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "2024-11-23 01:09:23.922483: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.924079 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.924087 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.924089 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.924092 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.924183 1124874 buffer_comparator.cc:157] Difference at 83215: -inf, expected -9.03875e+37\n",
      "E1123 01:09:23.924186 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.924188 1124874 buffer_comparator.cc:157] Difference at 83245: -6.31383e+37, expected -inf\n",
      "E1123 01:09:23.924190 1124874 buffer_comparator.cc:157] Difference at 83247: -inf, expected -1.14978e+38\n",
      "E1123 01:09:23.924193 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.924195 1124874 buffer_comparator.cc:157] Difference at 83291: 4.85168e+37, expected inf\n",
      "2024-11-23 01:09:23.924200: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.925788 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.925797 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.925799 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.925802 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.925893 1124874 buffer_comparator.cc:157] Difference at 83205: -inf, expected -2.83126e+38\n",
      "E1123 01:09:23.925896 1124874 buffer_comparator.cc:157] Difference at 83213: nan, expected -1.15643e+38\n",
      "E1123 01:09:23.925898 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.925900 1124874 buffer_comparator.cc:157] Difference at 83227: inf, expected -1.50203e+38\n",
      "E1123 01:09:23.925902 1124874 buffer_comparator.cc:157] Difference at 83229: -inf, expected -2.90769e+37\n",
      "E1123 01:09:23.925904 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "2024-11-23 01:09:23.925908: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.927501 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.927515 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.927517 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.927520 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.927611 1124874 buffer_comparator.cc:157] Difference at 83205: -inf, expected -2.83126e+38\n",
      "E1123 01:09:23.927614 1124874 buffer_comparator.cc:157] Difference at 83213: nan, expected -1.15643e+38\n",
      "E1123 01:09:23.927616 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.927618 1124874 buffer_comparator.cc:157] Difference at 83227: inf, expected -1.50203e+38\n",
      "E1123 01:09:23.927620 1124874 buffer_comparator.cc:157] Difference at 83229: -inf, expected -2.90769e+37\n",
      "E1123 01:09:23.927623 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "2024-11-23 01:09:23.927626: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.929237 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.929245 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.929248 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.929250 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.929342 1124874 buffer_comparator.cc:157] Difference at 83205: -inf, expected -2.83126e+38\n",
      "E1123 01:09:23.929346 1124874 buffer_comparator.cc:157] Difference at 83213: inf, expected -1.15643e+38\n",
      "E1123 01:09:23.929348 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.929350 1124874 buffer_comparator.cc:157] Difference at 83233: inf, expected 3.12369e+38\n",
      "E1123 01:09:23.929352 1124874 buffer_comparator.cc:157] Difference at 83245: -6.31383e+37, expected -inf\n",
      "E1123 01:09:23.929354 1124874 buffer_comparator.cc:157] Difference at 83259: inf, expected 1.7147e+38\n",
      "2024-11-23 01:09:23.929358: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.930961 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.930969 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.930971 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.930974 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.931065 1124874 buffer_comparator.cc:157] Difference at 83215: -inf, expected -9.03875e+37\n",
      "E1123 01:09:23.931068 1124874 buffer_comparator.cc:157] Difference at 83217: 1.75458e+38, expected inf\n",
      "E1123 01:09:23.931071 1124874 buffer_comparator.cc:157] Difference at 83245: -6.31383e+37, expected -inf\n",
      "E1123 01:09:23.931073 1124874 buffer_comparator.cc:157] Difference at 83247: -inf, expected -1.14978e+38\n",
      "E1123 01:09:23.931075 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.931077 1124874 buffer_comparator.cc:157] Difference at 83291: 4.85168e+37, expected inf\n",
      "2024-11-23 01:09:23.931081: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.932654 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.932664 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.932666 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.932669 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.932760 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.932763 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.932766 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.932768 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.932770 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.932773 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.932776: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.934346 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.934361 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.934363 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.934366 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.934369 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.934460 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.934463 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.934465 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.934467 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.934469 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.934473: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.936070 1124874 buffer_comparator.cc:157] Difference at 20777: -inf, expected -3.37624e+38\n",
      "E1123 01:09:23.936079 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.936081 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.936083 1124874 buffer_comparator.cc:157] Difference at 21183: -3.38953e+38, expected -inf\n",
      "E1123 01:09:23.936086 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.936177 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.936181 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.936183 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.936185 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.936187 1124874 buffer_comparator.cc:157] Difference at 83585: -inf, expected inf\n",
      "2024-11-23 01:09:23.936190: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.937734 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.937742 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.937745 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.937747 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.937839 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.937847 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.937849 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.937851 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.937854 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.937856 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.937860: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.939412 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.939421 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.939424 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.939426 1124874 buffer_comparator.cc:157] Difference at 21637: -inf, expected -3.11039e+38\n",
      "E1123 01:09:23.939518 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.939521 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.939523 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.939525 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.939528 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "E1123 01:09:23.939530 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "2024-11-23 01:09:23.939533: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.941120 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.941129 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.941132 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.941223 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.941227 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "E1123 01:09:23.941229 1124874 buffer_comparator.cc:157] Difference at 83697: inf, expected 2.87113e+38\n",
      "E1123 01:09:23.941232 1124874 buffer_comparator.cc:157] Difference at 83837: inf, expected -2.44578e+38\n",
      "E1123 01:09:23.941234 1124874 buffer_comparator.cc:157] Difference at 83843: -inf, expected -1.28935e+38\n",
      "E1123 01:09:23.941237 1124874 buffer_comparator.cc:157] Difference at 83861: -inf, expected 9.03875e+37\n",
      "E1123 01:09:23.941239 1124874 buffer_comparator.cc:157] Difference at 83877: -inf, expected -1.65489e+38\n",
      "2024-11-23 01:09:23.941243: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.942827 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.942836 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.942839 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.942841 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.942844 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.942847 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.942939 1124874 buffer_comparator.cc:157] Difference at 83843: -inf, expected -1.28935e+38\n",
      "E1123 01:09:23.942942 1124874 buffer_comparator.cc:157] Difference at 83861: -inf, expected 9.03875e+37\n",
      "E1123 01:09:23.942944 1124874 buffer_comparator.cc:157] Difference at 83877: -inf, expected -1.65489e+38\n",
      "E1123 01:09:23.942947 1124874 buffer_comparator.cc:157] Difference at 83911: inf, expected 3.36295e+38\n",
      "2024-11-23 01:09:23.942951: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.944555 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.944563 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.944566 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.944568 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.944571 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.944662 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.944665 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.944668 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.944670 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.944672 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.944676: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.954031 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.954047 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.954051 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.954054 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.954056 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.954148 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.954151 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.954154 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.954156 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.954159 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.954163: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.955774 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.955785 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.955787 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.955790 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.955793 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.955885 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.955889 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.955891 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.955893 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.955896 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.955900: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.957485 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.957493 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.957496 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.957499 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.957501 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.957592 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.957595 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.957598 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.957600 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.957603 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.957606: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.959193 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.959203 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.959205 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.959208 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.959211 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.959302 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.959305 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.959307 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.959309 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.959311 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.959315: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.960893 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.960903 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.960906 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.960908 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.960911 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.961002 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.961005 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.961007 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.961009 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.961012 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.961015: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.962611 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.962622 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.962624 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.962627 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.962630 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.962721 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.962725 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.962727 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.962729 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.962731 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.962735: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.964330 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.964338 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.964340 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.964343 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.964346 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.964436 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.964440 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.964442 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.964444 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.964446 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.964450: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.966044 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.966052 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.966054 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.966057 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.966059 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.966151 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.966154 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.966156 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.966158 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.966160 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.966164: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.967741 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.967751 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.967754 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.967756 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.967760 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.967851 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.967854 1124874 buffer_comparator.cc:157] Difference at 83401: 2.20652e+38, expected inf\n",
      "E1123 01:09:23.967856 1124874 buffer_comparator.cc:157] Difference at 83417: -inf, expected -2.77809e+38\n",
      "E1123 01:09:23.967858 1124874 buffer_comparator.cc:157] Difference at 83539: -3.23002e+38, expected -inf\n",
      "E1123 01:09:23.967860 1124874 buffer_comparator.cc:157] Difference at 83591: inf, expected 5.74891e+37\n",
      "2024-11-23 01:09:23.967864: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.969462 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.969471 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.969474 1124874 buffer_comparator.cc:157] Difference at 21523: inf, expected 3.24332e+38\n",
      "E1123 01:09:23.969565 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.969569 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "E1123 01:09:23.969571 1124874 buffer_comparator.cc:157] Difference at 83697: inf, expected 2.87113e+38\n",
      "E1123 01:09:23.969574 1124874 buffer_comparator.cc:157] Difference at 83837: inf, expected -2.44578e+38\n",
      "E1123 01:09:23.969576 1124874 buffer_comparator.cc:157] Difference at 83843: -inf, expected -1.28935e+38\n",
      "E1123 01:09:23.969578 1124874 buffer_comparator.cc:157] Difference at 83861: -inf, expected 9.03875e+37\n",
      "E1123 01:09:23.969581 1124874 buffer_comparator.cc:157] Difference at 83877: -inf, expected -1.65489e+38\n",
      "2024-11-23 01:09:23.969585: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.971182 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.971191 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.971194 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.971196 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.971198 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.971201 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.971292 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.971296 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "E1123 01:09:23.971299 1124874 buffer_comparator.cc:157] Difference at 83697: inf, expected 2.87113e+38\n",
      "E1123 01:09:23.971301 1124874 buffer_comparator.cc:157] Difference at 83837: inf, expected -2.44578e+38\n",
      "2024-11-23 01:09:23.971304: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.973006 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.973014 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.973017 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.973019 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.973022 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.973024 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.973115 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.973120 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "E1123 01:09:23.973122 1124874 buffer_comparator.cc:157] Difference at 83697: inf, expected 2.87113e+38\n",
      "E1123 01:09:23.973124 1124874 buffer_comparator.cc:157] Difference at 83837: inf, expected -2.44578e+38\n",
      "2024-11-23 01:09:23.973128: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.974718 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.974729 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.974732 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.974734 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.974737 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.974739 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.974830 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.974834 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "E1123 01:09:23.974836 1124874 buffer_comparator.cc:157] Difference at 83697: inf, expected 2.87113e+38\n",
      "E1123 01:09:23.974838 1124874 buffer_comparator.cc:157] Difference at 83837: inf, expected -2.44578e+38\n",
      "2024-11-23 01:09:23.974842: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.976459 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.976471 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.976473 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.976475 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.976478 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.976480 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.976571 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.976574 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "E1123 01:09:23.976576 1124874 buffer_comparator.cc:157] Difference at 83697: inf, expected 2.87113e+38\n",
      "E1123 01:09:23.976579 1124874 buffer_comparator.cc:157] Difference at 83837: inf, expected -2.44578e+38\n",
      "2024-11-23 01:09:23.976582: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.978175 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.978183 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.978185 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.978188 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.978190 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.978193 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.978284 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.978287 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "E1123 01:09:23.978289 1124874 buffer_comparator.cc:157] Difference at 83697: inf, expected 2.87113e+38\n",
      "E1123 01:09:23.978291 1124874 buffer_comparator.cc:157] Difference at 83837: inf, expected -2.44578e+38\n",
      "2024-11-23 01:09:23.978295: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.979884 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.979894 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.979897 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.979899 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.979901 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.979904 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.979995 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.979999 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "E1123 01:09:23.980001 1124874 buffer_comparator.cc:157] Difference at 83697: inf, expected 2.87113e+38\n",
      "E1123 01:09:23.980003 1124874 buffer_comparator.cc:157] Difference at 83837: inf, expected -2.44578e+38\n",
      "2024-11-23 01:09:23.980007: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n",
      "E1123 01:09:23.981596 1124874 buffer_comparator.cc:157] Difference at 20867: 2.96418e+38, expected inf\n",
      "E1123 01:09:23.981605 1124874 buffer_comparator.cc:157] Difference at 21049: 2.89772e+38, expected inf\n",
      "E1123 01:09:23.981607 1124874 buffer_comparator.cc:157] Difference at 21093: -inf, expected -2.71163e+38\n",
      "E1123 01:09:23.981609 1124874 buffer_comparator.cc:157] Difference at 21139: -inf, expected -2.89772e+38\n",
      "E1123 01:09:23.981612 1124874 buffer_comparator.cc:157] Difference at 21391: -3.37624e+38, expected -inf\n",
      "E1123 01:09:23.981614 1124874 buffer_comparator.cc:157] Difference at 21747: inf, expected 3.11039e+38\n",
      "E1123 01:09:23.981705 1124874 buffer_comparator.cc:157] Difference at 83283: -inf, expected -3.16356e+38\n",
      "E1123 01:09:23.981709 1124874 buffer_comparator.cc:157] Difference at 83669: 3.07052e+38, expected inf\n",
      "E1123 01:09:23.981711 1124874 buffer_comparator.cc:157] Difference at 83697: inf, expected 2.87113e+38\n",
      "E1123 01:09:23.981713 1124874 buffer_comparator.cc:157] Difference at 83837: inf, expected -2.44578e+38\n",
      "2024-11-23 01:09:23.981716: E external/xla/xla/service/gpu/autotuning/gemm_algorithm_picker.cc:350] Results mismatch between different GEMM algorithms. This is likely a bug/unexpected loss of precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax (1, 4128, 1280) [[[-0.00772095 0.00759888 -0.00427246 ... 0.0032959 0.00306702\n",
      "   0.00309753]\n",
      "  [-0.00772095 0.00759888 -0.00427246 ... 0.0032959 0.00306702\n",
      "   0.00309753]\n",
      "  [-0.00772095 0.00759888 -0.00427246 ... 0.0032959 0.00306702\n",
      "   0.00309753]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.0122681 0.00518799 0.000192642 ... -0.00994873 -0.00390625\n",
      "   0.0272217]\n",
      "  [-0.0122681 0.00518799 0.000192642 ... -0.00994873 -0.00390625\n",
      "   0.0272217]\n",
      "  [-0.0122681 0.00518799 0.000192642 ... -0.00994873 -0.00390625\n",
      "   0.0272217]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.012207 0.0045166 -0.0238037 ... -0.00436401 0.0118408 -0.0065918]\n",
      "  [0.012207 0.0045166 -0.0238037 ... -0.00436401 0.0118408 -0.0065918]\n",
      "  [0.012207 0.0045166 -0.0238037 ... -0.00436401 0.0118408 -0.0065918]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.0356445 -0.00123596 -0.032959 ... -0.00952148 0.0180664\n",
      "   -0.00921631]\n",
      "  [-0.0356445 -0.00123596 -0.032959 ... -0.00952148 0.0180664\n",
      "   -0.00921631]\n",
      "  [-0.0356445 -0.00123596 -0.032959 ... -0.00952148 0.0180664\n",
      "   -0.00921631]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.00469971 0.00448608 0.010376 ... 0.0327148 -0.00363159 0.0281982]\n",
      "  [0.00469971 0.00448608 0.010376 ... 0.0327148 -0.00363159 0.0281982]\n",
      "  [0.00469971 0.00448608 0.010376 ... 0.0327148 -0.00363159 0.0281982]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.000331879 -0.0257568 0.0466309 ... 0.0231934 -0.0139771\n",
      "   -0.000743866]\n",
      "  [-0.000331879 -0.0257568 0.0466309 ... 0.0231934 -0.0139771\n",
      "   -0.000743866]\n",
      "  [-0.000331879 -0.0257568 0.0466309 ... 0.0231934 -0.0139771\n",
      "   -0.000743866]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0187988 0.0172119 -0.0571289 ... 0.0537109 -0.00939941 0.026123]\n",
      "  [0.0187988 0.0172119 -0.0571289 ... 0.0537109 -0.00939941 0.026123]\n",
      "  [0.0187988 0.0172119 -0.0571289 ... 0.0537109 -0.00939941 0.026123]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.012146 -0.0322266 0.0032196 ... 0.0422363 0.0397949 0.0605469]\n",
      "  [0.012146 -0.0322266 0.0032196 ... 0.0422363 0.0397949 0.0605469]\n",
      "  [0.012146 -0.0322266 0.0032196 ... 0.0422363 0.0397949 0.0605469]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0678711 -0.0178223 -0.00189209 ... -0.0571289 0.0541992\n",
      "   -0.000637054]\n",
      "  [0.0678711 -0.0178223 -0.00189209 ... -0.0571289 0.0541992\n",
      "   -0.000637054]\n",
      "  [0.0678711 -0.0178223 -0.00189209 ... -0.0571289 0.0541992\n",
      "   -0.000637054]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.00521851 -0.00643921 0.0388184 ... -0.0178223 0.0393066 -0.0150757]\n",
      "  [0.00521851 -0.00643921 0.0388184 ... -0.0178223 0.0393066 -0.0150757]\n",
      "  [0.00521851 -0.00643921 0.0388184 ... -0.0178223 0.0393066 -0.0150757]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.0184326 0.0123291 0.0255127 ... 0.0279541 -0.0218506 -0.00204468]\n",
      "  [-0.0184326 0.0123291 0.0255127 ... 0.0279541 -0.0218506 -0.00204468]\n",
      "  [-0.0184326 0.0123291 0.0255127 ... 0.0279541 -0.0218506 -0.00204468]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0303955 0.0209961 0.0241699 ... -0.02771 0.0351562 -0.0131226]\n",
      "  [0.0303955 0.0209961 0.0241699 ... -0.02771 0.0351562 -0.0131226]\n",
      "  [0.0303955 0.0209961 0.0241699 ... -0.02771 0.0351562 -0.0131226]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.0942383 -0.0149536 -0.0233154 ... 0.019165 -0.0114746 -0.0178223]\n",
      "  [-0.0942383 -0.0149536 -0.0233154 ... 0.019165 -0.0114746 -0.0178223]\n",
      "  [-0.0942383 -0.0149536 -0.0233154 ... 0.019165 -0.0114746 -0.0178223]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0327148 -0.0288086 -0.034668 ... -0.0429688 0.0351562 0.0172119]\n",
      "  [0.0327148 -0.0288086 -0.034668 ... -0.0429688 0.0351562 0.0172119]\n",
      "  [0.0327148 -0.0288086 -0.034668 ... -0.0429688 0.0351562 0.0172119]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.010376 -0.0595703 -0.0163574 ... -0.0480957 0.000862122 0.0291748]\n",
      "  [0.010376 -0.0595703 -0.0163574 ... -0.0480957 0.000862122 0.0291748]\n",
      "  [0.010376 -0.0595703 -0.0163574 ... -0.0480957 0.000862122 0.0291748]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.041748 -0.0898438 -0.0184326 ... 0.00463867 -0.00778198 0.0600586]\n",
      "  [-0.041748 -0.0898438 -0.0184326 ... 0.00463867 -0.00778198 0.0600586]\n",
      "  [-0.041748 -0.0898438 -0.0184326 ... 0.00463867 -0.00778198 0.0600586]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.019165 0.0490723 -0.145508 ... 0.0151978 -0.0334473 -0.00915527]\n",
      "  [0.019165 0.0490723 -0.145508 ... 0.0151978 -0.0334473 -0.00915527]\n",
      "  [0.019165 0.0490723 -0.145508 ... 0.0151978 -0.0334473 -0.00915527]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0216064 -0.0766602 0.0189209 ... -0.0358887 -0.00518799 -0.0368652]\n",
      "  [0.0216064 -0.0766602 0.0189209 ... -0.0358887 -0.00518799 -0.0368652]\n",
      "  [0.0216064 -0.0766602 0.0189209 ... -0.0358887 -0.00518799 -0.0368652]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.019043 0.0126953 0.0615234 ... -0.0483398 -0.00775146 -0.0109253]\n",
      "  [-0.019043 0.0126953 0.0615234 ... -0.0483398 -0.00775146 -0.0109253]\n",
      "  [-0.019043 0.0126953 0.0615234 ... -0.0483398 -0.00775146 -0.0109253]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.0283203 0.0110474 -0.0273438 ... 0.0859375 -0.0200195 -0.0294189]\n",
      "  [-0.0283203 0.0110474 -0.0273438 ... 0.0859375 -0.0200195 -0.0294189]\n",
      "  [-0.0283203 0.0110474 -0.0273438 ... 0.0859375 -0.0200195 -0.0294189]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0148315 -0.0213623 0.0358887 ... -0.0534668 0.0441895 0.0441895]\n",
      "  [0.0148315 -0.0213623 0.0358887 ... -0.0534668 0.0441895 0.0441895]\n",
      "  [0.0148315 -0.0213623 0.0358887 ... -0.0534668 0.0441895 0.0441895]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.03125 -0.0712891 -0.00349426 ... 0.0578613 -0.0634766 -0.0150146]\n",
      "  [0.03125 -0.0712891 -0.00349426 ... 0.0578613 -0.0634766 -0.0150146]\n",
      "  [0.03125 -0.0712891 -0.00349426 ... 0.0578613 -0.0634766 -0.0150146]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.0466309 0.0834961 0.060791 ... -0.00216675 -0.0703125 0.050293]\n",
      "  [-0.0466309 0.0834961 0.060791 ... -0.00216675 -0.0703125 0.050293]\n",
      "  [-0.0466309 0.0834961 0.060791 ... -0.00216675 -0.0703125 0.050293]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0571289 0.0324707 -0.0336914 ... -0.0283203 0.0566406 -0.0349121]\n",
      "  [0.0571289 0.0324707 -0.0336914 ... -0.0283203 0.0566406 -0.0349121]\n",
      "  [0.0571289 0.0324707 -0.0336914 ... -0.0283203 0.0566406 -0.0349121]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0585938 -0.0498047 -0.0524902 ... -0.0429688 0.0668945 0.00335693]\n",
      "  [0.0585938 -0.0498047 -0.0524902 ... -0.0429688 0.0668945 0.00335693]\n",
      "  [0.0585938 -0.0498047 -0.0524902 ... -0.0429688 0.0668945 0.00335693]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0673828 0.0844727 0.0196533 ... 0.112305 -0.00708008 -0.0639648]\n",
      "  [0.0673828 0.0844727 0.0196533 ... 0.112305 -0.00708008 -0.0639648]\n",
      "  [0.0673828 0.0844727 0.0196533 ... 0.112305 -0.00708008 -0.0639648]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0571289 -0.0834961 -0.0240479 ... 0.116699 0.0405273 0.0583496]\n",
      "  [0.0571289 -0.0834961 -0.0240479 ... 0.116699 0.0405273 0.0583496]\n",
      "  [0.0571289 -0.0834961 -0.0240479 ... 0.116699 0.0405273 0.0583496]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0219727 -0.034668 0.0810547 ... 0.0100708 0.0119629 0.108398]\n",
      "  [0.0219727 -0.034668 0.0810547 ... 0.0100708 0.0119629 0.108398]\n",
      "  [0.0219727 -0.034668 0.0810547 ... 0.0100708 0.0119629 0.108398]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.139648 -0.0603027 -0.00756836 ... -0.00616455 -0.0664062 -0.0471191]\n",
      "  [0.139648 -0.0603027 -0.00756836 ... -0.00616455 -0.0664062 -0.0471191]\n",
      "  [0.139648 -0.0603027 -0.00756836 ... -0.00616455 -0.0664062 -0.0471191]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0117188 -0.0233154 0.0703125 ... 0.0825195 0.0878906 0.0306396]\n",
      "  [0.0117188 -0.0233154 0.0703125 ... 0.0825195 0.0878906 0.0306396]\n",
      "  [0.0117188 -0.0233154 0.0703125 ... 0.0825195 0.0878906 0.0306396]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.0563965 0.0202637 0.045166 ... 0.0727539 0.105957 0.0732422]\n",
      "  [-0.0563965 0.0202637 0.045166 ... 0.0727539 0.105957 0.0732422]\n",
      "  [-0.0563965 0.0202637 0.045166 ... 0.0727539 0.105957 0.0732422]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0286865 0.0285645 -0.0522461 ... 0.048584 0.0576172 0.0515137]\n",
      "  [0.0286865 0.0285645 -0.0522461 ... 0.048584 0.0576172 0.0515137]\n",
      "  [0.0286865 0.0285645 -0.0522461 ... 0.048584 0.0576172 0.0515137]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.121582 0.0664062 0.0358887 ... 0.0463867 0.0236816 -0.00314331]\n",
      "  [-0.121582 0.0664062 0.0358887 ... 0.0463867 0.0236816 -0.00314331]\n",
      "  [-0.121582 0.0664062 0.0358887 ... 0.0463867 0.0236816 -0.00314331]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.010437 0.0177002 -0.157227 ... -0.0192871 0.0693359 0.013855]\n",
      "  [0.010437 0.0177002 -0.157227 ... -0.0192871 0.0693359 0.013855]\n",
      "  [0.010437 0.0177002 -0.157227 ... -0.0192871 0.0693359 0.013855]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.125 0.0766602 -0.0568848 ... 0.0664062 0.0527344 0.140625]\n",
      "  [-0.125 0.0766602 -0.0568848 ... 0.0664062 0.0527344 0.140625]\n",
      "  [-0.125 0.0766602 -0.0568848 ... 0.0664062 0.0527344 0.140625]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.0556641 -0.0301514 -0.0224609 ... -0.00585938 0.0981445 -0.131836]\n",
      "  [-0.0556641 -0.0301514 -0.0224609 ... -0.00585938 0.0981445 -0.131836]\n",
      "  [-0.0556641 -0.0301514 -0.0224609 ... -0.00585938 0.0981445 -0.131836]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.0791016 0.0712891 -0.0644531 ... 0.0400391 -0.206055 -0.0133667]\n",
      "  [0.0791016 0.0712891 -0.0644531 ... 0.0400391 -0.206055 -0.0133667]\n",
      "  [0.0791016 0.0712891 -0.0644531 ... 0.0400391 -0.206055 -0.0133667]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[-0.0810547 -0.0224609 0.0883789 ... -0.00631714 0.052002 0.0272217]\n",
      "  [-0.0810547 -0.0224609 0.0883789 ... -0.00631714 0.052002 0.0272217]\n",
      "  [-0.0810547 -0.0224609 0.0883789 ... -0.00631714 0.052002 0.0272217]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.050293 0.00946045 0.0544434 ... 0.0275879 0.078125 -0.0131836]\n",
      "  [0.050293 0.00946045 0.0544434 ... 0.0275879 0.078125 -0.0131836]\n",
      "  [0.050293 0.00946045 0.0544434 ... 0.0275879 0.078125 -0.0131836]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "jax (1, 4128, 1280) [[[0.104004 0.0541992 0.130859 ... -0.052002 -0.137695 0.020874]\n",
      "  [0.104004 0.0541992 0.130859 ... -0.052002 -0.137695 0.020874]\n",
      "  [0.104004 0.0541992 0.130859 ... -0.052002 -0.137695 0.020874]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/amd/model/hub/models--meta-llama--Llama-3.2-11B-Vision/pytorch were not used when initializing FlaxMllamaVisionModel: {('language_model', 'model', 'layers.19', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.30', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'norm', 'kernel'), ('language_model', 'model', 'layers.25', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.10', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.38', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.29', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.4', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.23', 'cross_attn', 'q_norm', 'kernel'), ('language_model', 'model', 'layers.38', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.32', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.28', 'cross_attn_attn_gate'), ('language_model', 'model', 'layers.11', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.12', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.29', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.18', 'cross_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.13', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.26', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.18', 'cross_attn_attn_gate'), ('language_model', 'model', 'layers.3', 'cross_attn_mlp_gate'), ('language_model', 'model', 'layers.14', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.0', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.6', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.14', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.9', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.30', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.17', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.17', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.25', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.27', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.39', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.5', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.1', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.30', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.33', 'cross_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.0', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.6', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.12', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.38', 'cross_attn', 'q_norm', 'kernel'), ('language_model', 'model', 'layers.1', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.5', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.7', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.33', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.34', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.14', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.35', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.25', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.19', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.8', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.20', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.11', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.3', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.16', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.19', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.11', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.13', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.31', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.38', 'cross_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.18', 'cross_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.34', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.34', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.16', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.24', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.9', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.24', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.7', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.17', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.27', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.20', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.27', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.1', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.19', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.22', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.25', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.36', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.5', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.36', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.26', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.21', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.8', 'cross_attn_attn_gate'), ('language_model', 'model', 'layers.11', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.18', 'cross_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.2', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.3', 'cross_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.18', 'cross_attn', 'q_norm', 'kernel'), ('language_model', 'model', 'layers.6', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.11', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.1', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.4', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.2', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.28', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.24', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.14', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.23', 'cross_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.35', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.17', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.35', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.25', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.2', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.27', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.16', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.20', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.4', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.18', 'cross_attn', 'k_norm', 'kernel'), ('language_model', 'model', 'layers.38', 'cross_attn_mlp_gate'), ('language_model', 'model', 'layers.15', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.38', 'cross_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.8', 'cross_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.22', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.14', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.33', 'cross_attn', 'q_norm', 'kernel'), ('language_model', 'model', 'layers.28', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.13', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.22', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.37', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.8', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.2', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.36', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.37', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.23', 'cross_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.28', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.16', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.26', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.21', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.37', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.12', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.38', 'cross_attn', 'k_proj', 'kernel'), ('multi_modal_projector', 'kernel'), ('language_model', 'model', 'layers.3', 'cross_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.5', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.1', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.13', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.28', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.33', 'cross_attn_attn_gate'), ('language_model', 'model', 'layers.23', 'cross_attn', 'k_norm', 'kernel'), ('language_model', 'model', 'layers.37', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.35', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.32', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.20', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.13', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.36', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.22', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.37', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.4', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.2', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.26', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.9', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.21', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.39', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.8', 'cross_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.11', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.28', 'cross_attn', 'q_norm', 'kernel'), ('language_model', 'model', 'layers.7', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.10', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.34', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.1', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.13', 'cross_attn_mlp_gate'), ('language_model', 'model', 'layers.18', 'cross_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.22', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.14', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.37', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.11', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.3', 'cross_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.5', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.37', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.12', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.7', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.13', 'cross_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.19', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.15', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.10', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.29', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.25', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.29', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.38', 'cross_attn', 'k_norm', 'kernel'), ('language_model', 'model', 'layers.28', 'cross_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.34', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.0', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.23', 'cross_attn_mlp_gate'), ('language_model', 'model', 'layers.5', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.15', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.30', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.28', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.3', 'cross_attn', 'q_norm', 'kernel'), ('language_model', 'model', 'layers.32', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.35', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.17', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.29', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.37', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.39', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.17', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.16', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.0', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.29', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.10', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.33', 'cross_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.39', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.9', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.20', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.7', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.21', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.15', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.25', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.3', 'cross_attn', 'k_norm', 'kernel'), ('language_model', 'model', 'layers.8', 'cross_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.21', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.13', 'cross_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.11', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.31', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.2', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.25', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.21', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.26', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.5', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.10', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.16', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.34', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.39', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.17', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'embed_tokens', 'kernel'), ('language_model', 'model', 'layers.24', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.3', 'cross_attn_attn_gate'), ('language_model', 'model', 'layers.38', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.28', 'cross_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.18', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.33', 'cross_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.31', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.8', 'cross_attn', 'k_norm', 'kernel'), ('language_model', 'model', 'layers.12', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.38', 'cross_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.32', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.12', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.15', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.30', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.18', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.14', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.8', 'cross_attn', 'q_norm', 'kernel'), ('language_model', 'model', 'layers.31', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.14', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.35', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.18', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.15', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.13', 'cross_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.27', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.16', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.10', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.36', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.33', 'cross_attn', 'k_norm', 'kernel'), ('language_model', 'model', 'layers.20', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.0', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.26', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.21', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.27', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.30', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.28', 'cross_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.15', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.5', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.2', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.21', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.3', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.3', 'cross_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.13', 'cross_attn', 'k_norm', 'kernel'), ('language_model', 'model', 'layers.29', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.25', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.11', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.26', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.27', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.23', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.20', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.35', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.13', 'cross_attn', 'q_norm', 'kernel'), ('language_model', 'model', 'layers.24', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.23', 'cross_attn_attn_gate'), ('language_model', 'model', 'layers.10', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.6', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.21', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.34', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.18', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.28', 'cross_attn', 'k_norm', 'kernel'), ('language_model', 'model', 'layers.37', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.4', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.0', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.38', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.39', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.32', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.8', 'cross_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.23', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.31', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.31', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.18', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.14', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.2', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.6', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.32', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.31', 'post_attention_layernorm', 'kernel'), ('multi_modal_projector', 'bias'), ('language_model', 'model', 'layers.22', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.39', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.8', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.24', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.7', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.1', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.4', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.27', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.19', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.33', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.30', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.36', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.4', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.12', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.33', 'cross_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.3', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.7', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.19', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.1', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.12', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.15', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.28', 'cross_attn_mlp_gate'), ('language_model', 'model', 'layers.6', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.20', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.2', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.38', 'cross_attn_attn_gate'), ('language_model', 'model', 'layers.22', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.23', 'cross_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.39', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.18', 'cross_attn_mlp_gate'), ('language_model', 'model', 'layers.7', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.35', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.33', 'cross_attn_mlp_gate'), ('language_model', 'model', 'layers.35', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.36', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.9', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.10', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.19', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.24', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.3', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.31', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.32', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.23', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.29', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.22', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.6', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.32', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.16', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.36', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.13', 'cross_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.20', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.19', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.39', 'mlp', 'up_proj', 'kernel'), ('language_model', 'lm_head', 'kernel'), ('language_model', 'model', 'layers.23', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.27', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.33', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.9', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.28', 'cross_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.17', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.4', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.31', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.29', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.30', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.12', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.34', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.9', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.24', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.26', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.17', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.22', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.4', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.0', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.1', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.23', 'cross_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.0', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.3', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.10', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.13', 'cross_attn_attn_gate'), ('language_model', 'model', 'layers.33', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.24', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.8', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.26', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.0', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.15', 'self_attn', 'q_proj', 'kernel'), ('language_model', 'model', 'layers.16', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.30', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.6', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.9', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.38', 'mlp', 'down_proj', 'kernel'), ('language_model', 'model', 'layers.32', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.6', 'self_attn', 'o_proj', 'kernel'), ('language_model', 'model', 'layers.33', 'mlp', 'gate_proj', 'kernel'), ('language_model', 'model', 'layers.34', 'self_attn', 'k_proj', 'kernel'), ('language_model', 'model', 'layers.9', 'input_layernorm', 'kernel'), ('language_model', 'model', 'layers.5', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.8', 'cross_attn_mlp_gate'), ('language_model', 'model', 'layers.7', 'self_attn', 'v_proj', 'kernel'), ('language_model', 'model', 'layers.23', 'post_attention_layernorm', 'kernel'), ('language_model', 'model', 'layers.36', 'mlp', 'up_proj', 'kernel'), ('language_model', 'model', 'layers.8', 'mlp', 'up_proj', 'kernel')}\n",
      "- This IS expected if you are initializing FlaxMllamaVisionModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxMllamaVisionModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some of the weights of FlaxMllamaVisionModel were initialized in bfloat16 precision from the model checkpoint at /home/amd/model/hub/models--meta-llama--Llama-3.2-11B-Vision/pytorch:\n",
      "[('vision_model', 'class_embedding'), ('vision_model', 'gated_positional_embedding', 'embedding'), ('vision_model', 'gated_positional_embedding', 'gate'), ('vision_model', 'gated_positional_embedding', 'tile_embedding', 'embedding'), ('vision_model', 'global_transformer', 'layers.0', 'gate_attn'), ('vision_model', 'global_transformer', 'layers.0', 'gate_ffn'), ('vision_model', 'global_transformer', 'layers.0', 'input_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.0', 'input_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.0', 'mlp', 'fc1', 'bias'), ('vision_model', 'global_transformer', 'layers.0', 'mlp', 'fc1', 'kernel'), ('vision_model', 'global_transformer', 'layers.0', 'mlp', 'fc2', 'bias'), ('vision_model', 'global_transformer', 'layers.0', 'mlp', 'fc2', 'kernel'), ('vision_model', 'global_transformer', 'layers.0', 'post_attention_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.0', 'post_attention_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.0', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.0', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.0', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.0', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.1', 'gate_attn'), ('vision_model', 'global_transformer', 'layers.1', 'gate_ffn'), ('vision_model', 'global_transformer', 'layers.1', 'input_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.1', 'input_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.1', 'mlp', 'fc1', 'bias'), ('vision_model', 'global_transformer', 'layers.1', 'mlp', 'fc1', 'kernel'), ('vision_model', 'global_transformer', 'layers.1', 'mlp', 'fc2', 'bias'), ('vision_model', 'global_transformer', 'layers.1', 'mlp', 'fc2', 'kernel'), ('vision_model', 'global_transformer', 'layers.1', 'post_attention_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.1', 'post_attention_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.1', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.1', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.1', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.1', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.2', 'gate_attn'), ('vision_model', 'global_transformer', 'layers.2', 'gate_ffn'), ('vision_model', 'global_transformer', 'layers.2', 'input_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.2', 'input_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.2', 'mlp', 'fc1', 'bias'), ('vision_model', 'global_transformer', 'layers.2', 'mlp', 'fc1', 'kernel'), ('vision_model', 'global_transformer', 'layers.2', 'mlp', 'fc2', 'bias'), ('vision_model', 'global_transformer', 'layers.2', 'mlp', 'fc2', 'kernel'), ('vision_model', 'global_transformer', 'layers.2', 'post_attention_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.2', 'post_attention_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.2', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.2', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.2', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.2', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.3', 'gate_attn'), ('vision_model', 'global_transformer', 'layers.3', 'gate_ffn'), ('vision_model', 'global_transformer', 'layers.3', 'input_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.3', 'input_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.3', 'mlp', 'fc1', 'bias'), ('vision_model', 'global_transformer', 'layers.3', 'mlp', 'fc1', 'kernel'), ('vision_model', 'global_transformer', 'layers.3', 'mlp', 'fc2', 'bias'), ('vision_model', 'global_transformer', 'layers.3', 'mlp', 'fc2', 'kernel'), ('vision_model', 'global_transformer', 'layers.3', 'post_attention_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.3', 'post_attention_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.3', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.3', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.3', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.3', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.4', 'gate_attn'), ('vision_model', 'global_transformer', 'layers.4', 'gate_ffn'), ('vision_model', 'global_transformer', 'layers.4', 'input_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.4', 'input_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.4', 'mlp', 'fc1', 'bias'), ('vision_model', 'global_transformer', 'layers.4', 'mlp', 'fc1', 'kernel'), ('vision_model', 'global_transformer', 'layers.4', 'mlp', 'fc2', 'bias'), ('vision_model', 'global_transformer', 'layers.4', 'mlp', 'fc2', 'kernel'), ('vision_model', 'global_transformer', 'layers.4', 'post_attention_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.4', 'post_attention_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.4', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.4', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.4', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.4', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.5', 'gate_attn'), ('vision_model', 'global_transformer', 'layers.5', 'gate_ffn'), ('vision_model', 'global_transformer', 'layers.5', 'input_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.5', 'input_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.5', 'mlp', 'fc1', 'bias'), ('vision_model', 'global_transformer', 'layers.5', 'mlp', 'fc1', 'kernel'), ('vision_model', 'global_transformer', 'layers.5', 'mlp', 'fc2', 'bias'), ('vision_model', 'global_transformer', 'layers.5', 'mlp', 'fc2', 'kernel'), ('vision_model', 'global_transformer', 'layers.5', 'post_attention_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.5', 'post_attention_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.5', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.5', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.5', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.5', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.6', 'gate_attn'), ('vision_model', 'global_transformer', 'layers.6', 'gate_ffn'), ('vision_model', 'global_transformer', 'layers.6', 'input_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.6', 'input_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.6', 'mlp', 'fc1', 'bias'), ('vision_model', 'global_transformer', 'layers.6', 'mlp', 'fc1', 'kernel'), ('vision_model', 'global_transformer', 'layers.6', 'mlp', 'fc2', 'bias'), ('vision_model', 'global_transformer', 'layers.6', 'mlp', 'fc2', 'kernel'), ('vision_model', 'global_transformer', 'layers.6', 'post_attention_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.6', 'post_attention_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.6', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.6', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.6', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.6', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.7', 'gate_attn'), ('vision_model', 'global_transformer', 'layers.7', 'gate_ffn'), ('vision_model', 'global_transformer', 'layers.7', 'input_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.7', 'input_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.7', 'mlp', 'fc1', 'bias'), ('vision_model', 'global_transformer', 'layers.7', 'mlp', 'fc1', 'kernel'), ('vision_model', 'global_transformer', 'layers.7', 'mlp', 'fc2', 'bias'), ('vision_model', 'global_transformer', 'layers.7', 'mlp', 'fc2', 'kernel'), ('vision_model', 'global_transformer', 'layers.7', 'post_attention_layernorm', 'bias'), ('vision_model', 'global_transformer', 'layers.7', 'post_attention_layernorm', 'scale'), ('vision_model', 'global_transformer', 'layers.7', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.7', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.7', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'global_transformer', 'layers.7', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'layernorm_post', 'bias'), ('vision_model', 'layernorm_post', 'scale'), ('vision_model', 'layernorm_pre', 'bias'), ('vision_model', 'layernorm_pre', 'scale'), ('vision_model', 'patch_embedding', 'kernel'), ('vision_model', 'post_tile_positional_embedding', 'embedding', 'embedding'), ('vision_model', 'post_tile_positional_embedding', 'gate'), ('vision_model', 'pre_tile_positional_embedding', 'embedding', 'embedding'), ('vision_model', 'pre_tile_positional_embedding', 'gate'), ('vision_model', 'transformer', 'layers.0', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.0', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.0', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.0', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.0', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.0', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.0', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.0', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.0', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.0', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.0', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.0', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.1', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.1', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.1', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.1', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.1', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.1', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.1', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.1', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.1', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.1', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.1', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.1', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.10', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.10', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.10', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.10', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.10', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.10', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.10', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.10', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.10', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.10', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.10', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.10', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.11', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.11', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.11', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.11', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.11', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.11', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.11', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.11', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.11', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.11', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.11', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.11', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.12', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.12', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.12', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.12', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.12', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.12', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.12', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.12', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.12', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.12', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.12', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.12', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.13', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.13', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.13', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.13', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.13', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.13', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.13', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.13', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.13', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.13', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.13', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.13', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.14', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.14', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.14', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.14', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.14', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.14', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.14', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.14', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.14', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.14', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.14', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.14', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.15', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.15', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.15', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.15', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.15', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.15', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.15', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.15', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.15', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.15', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.15', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.15', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.16', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.16', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.16', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.16', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.16', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.16', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.16', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.16', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.16', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.16', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.16', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.16', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.17', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.17', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.17', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.17', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.17', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.17', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.17', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.17', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.17', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.17', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.17', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.17', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.18', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.18', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.18', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.18', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.18', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.18', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.18', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.18', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.18', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.18', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.18', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.18', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.19', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.19', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.19', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.19', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.19', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.19', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.19', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.19', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.19', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.19', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.19', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.19', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.2', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.2', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.2', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.2', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.2', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.2', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.2', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.2', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.2', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.2', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.2', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.2', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.20', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.20', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.20', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.20', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.20', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.20', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.20', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.20', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.20', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.20', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.20', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.20', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.21', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.21', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.21', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.21', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.21', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.21', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.21', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.21', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.21', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.21', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.21', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.21', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.22', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.22', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.22', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.22', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.22', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.22', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.22', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.22', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.22', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.22', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.22', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.22', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.23', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.23', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.23', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.23', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.23', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.23', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.23', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.23', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.23', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.23', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.23', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.23', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.24', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.24', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.24', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.24', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.24', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.24', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.24', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.24', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.24', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.24', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.24', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.24', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.25', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.25', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.25', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.25', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.25', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.25', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.25', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.25', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.25', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.25', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.25', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.25', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.26', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.26', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.26', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.26', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.26', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.26', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.26', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.26', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.26', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.26', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.26', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.26', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.27', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.27', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.27', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.27', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.27', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.27', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.27', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.27', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.27', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.27', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.27', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.27', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.28', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.28', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.28', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.28', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.28', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.28', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.28', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.28', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.28', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.28', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.28', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.28', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.29', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.29', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.29', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.29', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.29', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.29', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.29', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.29', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.29', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.29', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.29', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.29', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.3', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.3', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.3', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.3', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.3', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.3', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.3', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.3', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.3', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.3', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.3', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.3', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.30', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.30', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.30', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.30', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.30', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.30', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.30', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.30', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.30', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.30', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.30', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.30', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.31', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.31', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.31', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.31', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.31', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.31', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.31', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.31', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.31', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.31', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.31', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.31', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.4', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.4', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.4', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.4', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.4', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.4', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.4', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.4', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.4', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.4', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.4', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.4', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.5', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.5', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.5', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.5', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.5', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.5', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.5', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.5', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.5', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.5', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.5', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.5', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.6', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.6', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.6', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.6', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.6', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.6', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.6', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.6', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.6', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.6', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.6', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.6', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.7', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.7', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.7', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.7', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.7', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.7', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.7', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.7', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.7', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.7', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.7', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.7', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.8', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.8', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.8', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.8', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.8', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.8', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.8', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.8', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.8', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.8', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.8', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.8', 'self_attn', 'v_proj', 'kernel'), ('vision_model', 'transformer', 'layers.9', 'input_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.9', 'input_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.9', 'mlp', 'fc1', 'bias'), ('vision_model', 'transformer', 'layers.9', 'mlp', 'fc1', 'kernel'), ('vision_model', 'transformer', 'layers.9', 'mlp', 'fc2', 'bias'), ('vision_model', 'transformer', 'layers.9', 'mlp', 'fc2', 'kernel'), ('vision_model', 'transformer', 'layers.9', 'post_attention_layernorm', 'bias'), ('vision_model', 'transformer', 'layers.9', 'post_attention_layernorm', 'scale'), ('vision_model', 'transformer', 'layers.9', 'self_attn', 'k_proj', 'kernel'), ('vision_model', 'transformer', 'layers.9', 'self_attn', 'o_proj', 'kernel'), ('vision_model', 'transformer', 'layers.9', 'self_attn', 'q_proj', 'kernel'), ('vision_model', 'transformer', 'layers.9', 'self_attn', 'v_proj', 'kernel')]\n",
      "You should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MllamaVisionModel, FlaxMllamaVisionModel\n",
    "from transformers import AutoProcessor, MllamaTextModel\n",
    "import requests\n",
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "hf_token = \"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\"\n",
    "login(hf_token)\n",
    "checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "model_torch = MllamaVisionModel.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "model = FlaxMllamaVisionModel.from_pretrained(\"/home/amd/model/hub/models--meta-llama--Llama-3.2-11B-Vision/pytorch\", from_pt=True, dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.01129150,  0.20898438, -0.00903320,  ...,  0.01782227,\n",
      "          -0.02978516,  0.01867676],\n",
      "         [ 0.00625610, -0.13867188, -0.14453125,  ...,  0.13671875,\n",
      "           0.06542969, -0.11523438],\n",
      "         [ 0.01037598, -0.17968750, -0.16015625,  ...,  0.11279297,\n",
      "           0.13476562, -0.14062500],\n",
      "         ...,\n",
      "         [ 0.48242188,  1.35937500,  0.06054688,  ...,  0.05908203,\n",
      "           0.37109375,  0.16894531],\n",
      "         [ 0.48242188,  1.35937500,  0.06054688,  ...,  0.05908203,\n",
      "           0.37109375,  0.16894531],\n",
      "         [ 0.48242188,  1.35937500,  0.06054688,  ...,  0.05908203,\n",
      "           0.37109375,  0.16894531]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.02478027,  0.06835938, -0.05004883,  ..., -0.05908203,\n",
      "           0.05419922,  0.03247070],\n",
      "         [ 0.03198242,  0.05883789, -0.02478027,  ...,  0.13964844,\n",
      "           0.05859375, -0.06884766],\n",
      "         [ 0.03125000,  0.05908203, -0.02258301,  ...,  0.14550781,\n",
      "           0.09960938, -0.06884766],\n",
      "         ...,\n",
      "         [ 0.04663086,  0.41015625,  0.07666016,  ..., -0.12597656,\n",
      "          -0.01385498,  0.09033203],\n",
      "         [ 0.04663086,  0.41015625,  0.07666016,  ..., -0.12597656,\n",
      "          -0.01385498,  0.09033203],\n",
      "         [ 0.04663086,  0.41015625,  0.07666016,  ..., -0.12597656,\n",
      "          -0.01385498,  0.09033203]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.02355957,  0.00692749, -0.00430298,  ..., -0.02331543,\n",
      "          -0.03344727,  0.04833984],\n",
      "         [ 0.00866699,  0.00976562,  0.00393677,  ..., -0.02368164,\n",
      "          -0.03247070,  0.04785156],\n",
      "         [ 0.00125122,  0.03686523,  0.02185059,  ..., -0.02416992,\n",
      "          -0.03198242,  0.04711914],\n",
      "         ...,\n",
      "         [-0.13378906,  0.04760742,  0.02026367,  ..., -0.03247070,\n",
      "          -0.05859375, -0.04687500],\n",
      "         [-0.13378906,  0.04760742,  0.02026367,  ..., -0.03247070,\n",
      "          -0.05859375, -0.04687500],\n",
      "         [-0.13378906,  0.04760742,  0.02026367,  ..., -0.03247070,\n",
      "          -0.05859375, -0.04687500]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.00454712, -0.03613281,  0.00576782,  ...,  0.00285339,\n",
      "          -0.06201172,  0.14355469],\n",
      "         [ 0.03271484, -0.03808594, -0.01483154,  ...,  0.00172424,\n",
      "          -0.05395508,  0.13476562],\n",
      "         [ 0.03027344, -0.03759766, -0.01049805,  ...,  0.00086212,\n",
      "          -0.05078125,  0.13183594],\n",
      "         ...,\n",
      "         [-0.00357056,  0.10351562, -0.04028320,  ...,  0.10205078,\n",
      "          -0.31640625,  0.25390625],\n",
      "         [-0.00357056,  0.10351562, -0.04028320,  ...,  0.10205078,\n",
      "          -0.31640625,  0.25390625],\n",
      "         [-0.00357056,  0.10351562, -0.04028320,  ...,  0.10205078,\n",
      "          -0.31640625,  0.25390625]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.05078125,  0.02124023, -0.00139618,  ...,  0.08886719,\n",
      "          -0.05566406, -0.10253906],\n",
      "         [ 0.04711914,  0.02966309,  0.00194550,  ...,  0.08593750,\n",
      "          -0.05517578, -0.09375000],\n",
      "         [ 0.04687500,  0.03039551,  0.00225830,  ...,  0.08447266,\n",
      "          -0.05273438, -0.09082031],\n",
      "         ...,\n",
      "         [-0.20800781, -0.10351562,  0.07421875,  ..., -0.10009766,\n",
      "          -0.20703125, -0.15429688],\n",
      "         [-0.20800781, -0.10351562,  0.07421875,  ..., -0.10009766,\n",
      "          -0.20703125, -0.15429688],\n",
      "         [-0.20800781, -0.10351562,  0.07421875,  ..., -0.10009766,\n",
      "          -0.20703125, -0.15429688]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.08984375,  0.03710938,  0.03173828,  ...,  0.08105469,\n",
      "           0.06787109, -0.00897217],\n",
      "         [ 0.09033203,  0.03637695,  0.03063965,  ...,  0.08300781,\n",
      "           0.06787109, -0.00909424],\n",
      "         [ 0.08984375,  0.03637695,  0.02990723,  ...,  0.08398438,\n",
      "           0.06738281, -0.00933838],\n",
      "         ...,\n",
      "         [-0.03588867,  0.16503906, -0.05419922,  ..., -0.03784180,\n",
      "           0.07958984,  0.01855469],\n",
      "         [-0.03588867,  0.16503906, -0.05419922,  ..., -0.03784180,\n",
      "           0.07958984,  0.01855469],\n",
      "         [-0.03588867,  0.16503906, -0.05419922,  ..., -0.03784180,\n",
      "           0.07958984,  0.01855469]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-8.25195312e-02, -9.71679688e-02,  1.86523438e-01,  ...,\n",
      "          -1.80664062e-01,  4.21524048e-04, -2.79296875e-01],\n",
      "         [-1.52343750e-01, -3.46679688e-02,  1.24023438e-01,  ...,\n",
      "          -7.66601562e-02,  1.22558594e-01, -7.81250000e-01],\n",
      "         [-1.13281250e-01, -2.38037109e-02,  1.15722656e-01,  ...,\n",
      "           6.73828125e-02,  6.44531250e-02, -5.66406250e-01],\n",
      "         ...,\n",
      "         [-3.95507812e-02, -5.73730469e-02,  1.47460938e-01,  ...,\n",
      "          -9.61914062e-02, -8.93554688e-02, -4.45312500e-01],\n",
      "         [-3.95507812e-02, -5.73730469e-02,  1.47460938e-01,  ...,\n",
      "          -9.61914062e-02, -8.93554688e-02, -4.45312500e-01],\n",
      "         [-3.95507812e-02, -5.73730469e-02,  1.47460938e-01,  ...,\n",
      "          -9.61914062e-02, -8.93554688e-02, -4.45312500e-01]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.12988281, -0.08007812, -0.08300781,  ..., -0.10986328,\n",
      "          -0.01672363, -0.04345703],\n",
      "         [-0.06884766, -0.04443359, -0.09619141,  ..., -0.11376953,\n",
      "          -0.01721191, -0.04956055],\n",
      "         [-0.07324219, -0.06396484, -0.08544922,  ..., -0.11083984,\n",
      "          -0.01696777, -0.04418945],\n",
      "         ...,\n",
      "         [-0.13964844,  0.14355469, -0.06591797,  ..., -0.11083984,\n",
      "          -0.11572266, -0.08203125],\n",
      "         [-0.13964844,  0.14355469, -0.06591797,  ..., -0.11083984,\n",
      "          -0.11572266, -0.08203125],\n",
      "         [-0.13964844,  0.14355469, -0.06591797,  ..., -0.11083984,\n",
      "          -0.11572266, -0.08203125]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.14941406,  0.01574707,  0.06494141,  ..., -0.40234375,\n",
      "          -0.11572266,  0.22949219],\n",
      "         [-0.20898438,  0.10302734, -0.13183594,  ..., -0.39843750,\n",
      "          -0.15039062,  0.23339844],\n",
      "         [-0.29687500,  0.12353516, -0.11376953,  ..., -0.33984375,\n",
      "          -0.15917969,  0.24902344],\n",
      "         ...,\n",
      "         [ 0.15625000,  0.05004883,  0.05786133,  ..., -0.40039062,\n",
      "          -0.15136719,  0.09130859],\n",
      "         [ 0.15625000,  0.05004883,  0.05786133,  ..., -0.40039062,\n",
      "          -0.15136719,  0.09130859],\n",
      "         [ 0.15625000,  0.05004883,  0.05786133,  ..., -0.40039062,\n",
      "          -0.15136719,  0.09130859]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.22656250,  0.05810547,  0.03784180,  ..., -0.21777344,\n",
      "          -0.05932617,  0.09912109],\n",
      "         [-0.20996094,  0.26367188,  0.06835938,  ...,  0.00325012,\n",
      "           0.17187500, -0.03613281],\n",
      "         [-0.38867188,  0.29687500, -0.00823975,  ..., -0.09667969,\n",
      "           0.06933594,  0.00842285],\n",
      "         ...,\n",
      "         [ 0.22949219, -0.00233459,  0.06225586,  ..., -0.03247070,\n",
      "           0.05590820, -0.00518799],\n",
      "         [ 0.22949219, -0.00233459,  0.06225586,  ..., -0.03247070,\n",
      "           0.05590820, -0.00518799],\n",
      "         [ 0.22949219, -0.00233459,  0.06225586,  ..., -0.03247070,\n",
      "           0.05590820, -0.00518799]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.05712891, -0.15234375, -0.06787109,  ..., -0.00646973,\n",
      "          -0.48437500, -0.04321289],\n",
      "         [ 0.40625000,  0.06640625, -0.09179688,  ...,  0.59375000,\n",
      "           1.52343750, -0.22851562],\n",
      "         [ 0.43554688,  0.02246094, -0.11230469,  ...,  0.67187500,\n",
      "           1.35937500, -0.13574219],\n",
      "         ...,\n",
      "         [ 0.19238281,  0.08496094, -0.14062500,  ...,  0.09912109,\n",
      "           0.22753906,  0.24804688],\n",
      "         [ 0.19238281,  0.08496094, -0.14062500,  ...,  0.09912109,\n",
      "           0.22753906,  0.24804688],\n",
      "         [ 0.19238281,  0.08496094, -0.14062500,  ...,  0.09912109,\n",
      "           0.22753906,  0.24804688]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-6.13403320e-03, -4.64843750e-01,  7.42187500e-02,  ...,\n",
      "           1.32812500e-01,  5.76171875e-02, -9.34600830e-05],\n",
      "         [-7.37304688e-02,  2.91015625e-01,  5.66406250e-01,  ...,\n",
      "           8.10546875e-02,  1.77734375e-01,  1.24511719e-01],\n",
      "         [-5.02929688e-02,  4.35546875e-01,  5.85937500e-01,  ...,\n",
      "           7.03125000e-02,  1.77734375e-01,  3.20312500e-01],\n",
      "         ...,\n",
      "         [-1.49414062e-01, -2.14843750e-01,  8.25195312e-02,  ...,\n",
      "           2.16796875e-01,  1.54296875e-01,  1.32812500e-01],\n",
      "         [-1.49414062e-01, -2.14843750e-01,  8.25195312e-02,  ...,\n",
      "           2.16796875e-01,  1.54296875e-01,  1.32812500e-01],\n",
      "         [-1.49414062e-01, -2.14843750e-01,  8.25195312e-02,  ...,\n",
      "           2.16796875e-01,  1.54296875e-01,  1.32812500e-01]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.23046875,  0.05102539, -0.27539062,  ..., -0.04321289,\n",
      "           0.26562500,  0.09228516],\n",
      "         [ 0.42968750,  0.22851562, -0.33398438,  ...,  0.05224609,\n",
      "           0.58203125, -0.94921875],\n",
      "         [ 0.27343750, -0.39648438, -0.17089844,  ..., -0.49218750,\n",
      "           1.04687500, -0.80859375],\n",
      "         ...,\n",
      "         [ 0.15039062, -0.04248047, -0.16210938,  ...,  0.29882812,\n",
      "           0.83203125,  0.25976562],\n",
      "         [ 0.15039062, -0.04248047, -0.16210938,  ...,  0.29882812,\n",
      "           0.83203125,  0.25976562],\n",
      "         [ 0.15039062, -0.04248047, -0.16210938,  ...,  0.29882812,\n",
      "           0.83203125,  0.25976562]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.10546875,  0.03442383, -0.06298828,  ...,  0.24218750,\n",
      "          -0.25585938,  0.12792969],\n",
      "         [-0.30859375,  0.15039062,  0.14843750,  ..., -0.07226562,\n",
      "           0.07080078,  0.01416016],\n",
      "         [-0.36523438, -0.01770020,  0.33789062,  ..., -0.14648438,\n",
      "           0.04833984, -0.07617188],\n",
      "         ...,\n",
      "         [-0.10742188,  0.20214844,  0.12207031,  ...,  0.13378906,\n",
      "          -0.25390625,  0.06542969],\n",
      "         [-0.10742188,  0.20214844,  0.12207031,  ...,  0.13378906,\n",
      "          -0.25390625,  0.06542969],\n",
      "         [-0.10742188,  0.20214844,  0.12207031,  ...,  0.13378906,\n",
      "          -0.25390625,  0.06542969]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.17089844,  0.32031250,  0.22363281,  ..., -0.13574219,\n",
      "           0.18261719,  0.07275391],\n",
      "         [ 0.14160156,  0.54687500,  0.42578125,  ..., -0.67968750,\n",
      "           0.46875000, -0.03027344],\n",
      "         [ 0.20507812,  0.53515625,  0.40429688,  ..., -0.69531250,\n",
      "           0.53515625,  0.07275391],\n",
      "         ...,\n",
      "         [-0.08593750, -0.34375000,  0.01196289,  ..., -0.23730469,\n",
      "           0.11914062,  0.26953125],\n",
      "         [-0.08593750, -0.34375000,  0.01196289,  ..., -0.23730469,\n",
      "           0.11914062,  0.26953125],\n",
      "         [-0.08593750, -0.34375000,  0.01196289,  ..., -0.23730469,\n",
      "           0.11914062,  0.26953125]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.43359375, -0.40039062, -0.22167969,  ...,  0.19726562,\n",
      "           0.09375000, -0.35351562],\n",
      "         [-0.09863281, -0.20703125, -0.75390625,  ...,  0.20703125,\n",
      "           0.02111816,  0.11523438],\n",
      "         [-0.13085938, -0.28125000, -0.74609375,  ...,  0.25390625,\n",
      "           0.02429199,  0.20214844],\n",
      "         ...,\n",
      "         [ 0.03979492,  0.03759766, -0.28125000,  ...,  0.06347656,\n",
      "           0.17480469, -0.12792969],\n",
      "         [ 0.03979492,  0.03759766, -0.28125000,  ...,  0.06347656,\n",
      "           0.17480469, -0.12792969],\n",
      "         [ 0.03979492,  0.03759766, -0.28125000,  ...,  0.06347656,\n",
      "           0.17480469, -0.12792969]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.41210938, -0.24414062,  0.08203125,  ...,  0.03295898,\n",
      "           0.19433594, -0.13671875],\n",
      "         [-0.53906250, -0.34179688, -0.26953125,  ...,  0.23730469,\n",
      "           0.26953125,  0.18261719],\n",
      "         [-0.20800781, -0.48632812,  0.03540039,  ...,  0.19726562,\n",
      "           0.22167969,  0.48242188],\n",
      "         ...,\n",
      "         [ 0.08789062, -0.26562500,  0.05078125,  ..., -0.00137329,\n",
      "           0.37890625, -0.34179688],\n",
      "         [ 0.08789062, -0.26562500,  0.05078125,  ..., -0.00137329,\n",
      "           0.37890625, -0.34179688],\n",
      "         [ 0.08789062, -0.26562500,  0.05078125,  ..., -0.00137329,\n",
      "           0.37890625, -0.34179688]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.03442383,  0.29296875, -0.02990723,  ...,  0.30273438,\n",
      "          -0.29101562, -0.28125000],\n",
      "         [-0.30664062,  0.34570312,  0.17773438,  ...,  0.29296875,\n",
      "           1.01562500,  1.15625000],\n",
      "         [-0.35937500,  0.26562500,  0.01239014,  ...,  0.29687500,\n",
      "           1.94531250,  1.60937500],\n",
      "         ...,\n",
      "         [-0.50390625,  0.00448608,  0.04443359,  ..., -0.36914062,\n",
      "           0.87500000,  0.04370117],\n",
      "         [-0.50390625,  0.00448608,  0.04443359,  ..., -0.36914062,\n",
      "           0.87500000,  0.04370117],\n",
      "         [-0.50390625,  0.00448608,  0.04443359,  ..., -0.36914062,\n",
      "           0.87500000,  0.04370117]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.45117188,  0.30273438, -0.28710938,  ..., -0.06298828,\n",
      "          -0.23632812,  0.24804688],\n",
      "         [ 0.29296875, -0.46875000,  0.81640625,  ..., -0.10791016,\n",
      "          -0.09912109,  0.16210938],\n",
      "         [ 0.20703125,  0.58203125, -0.06835938,  ..., -0.09570312,\n",
      "           0.00100708,  0.25585938],\n",
      "         ...,\n",
      "         [-0.32617188,  0.10693359,  0.31835938,  ..., -0.06591797,\n",
      "          -0.25585938,  0.19042969],\n",
      "         [-0.32617188,  0.10693359,  0.31835938,  ..., -0.06591797,\n",
      "          -0.25585938,  0.19042969],\n",
      "         [-0.32617188,  0.10693359,  0.31835938,  ..., -0.06591797,\n",
      "          -0.25585938,  0.19042969]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.11572266, -0.05151367, -0.18652344,  ...,  0.12597656,\n",
      "           0.13281250,  0.12695312],\n",
      "         [ 0.55078125, -0.63671875, -0.21679688,  ..., -0.08300781,\n",
      "          -0.16699219, -0.57812500],\n",
      "         [ 0.57031250, -1.04687500, -0.51953125,  ..., -0.03491211,\n",
      "          -0.15234375, -0.17187500],\n",
      "         ...,\n",
      "         [-0.14160156, -0.13476562,  0.13867188,  ...,  0.01940918,\n",
      "           0.02136230,  0.02856445],\n",
      "         [-0.14160156, -0.13476562,  0.13867188,  ...,  0.01940918,\n",
      "           0.02136230,  0.02856445],\n",
      "         [-0.14160156, -0.13476562,  0.13867188,  ...,  0.01940918,\n",
      "           0.02136230,  0.02856445]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.03588867, -0.36523438, -0.06542969,  ...,  0.76171875,\n",
      "           0.93359375,  0.27539062],\n",
      "         [-0.09863281,  0.11572266, -0.37500000,  ..., -0.05712891,\n",
      "          -0.10058594,  0.06933594],\n",
      "         [-0.09716797, -0.15136719, -0.38281250,  ...,  0.23632812,\n",
      "           0.10839844, -0.00454712],\n",
      "         ...,\n",
      "         [-0.03759766, -0.08007812, -0.24218750,  ...,  0.01348877,\n",
      "           0.34960938,  0.04541016],\n",
      "         [-0.03759766, -0.08007812, -0.24218750,  ...,  0.01348877,\n",
      "           0.34960938,  0.04541016],\n",
      "         [-0.03759766, -0.08007812, -0.24218750,  ...,  0.01348877,\n",
      "           0.34960938,  0.04541016]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.22363281,  0.07714844,  0.01080322,  ..., -0.19628906,\n",
      "           0.50000000,  0.22363281],\n",
      "         [-0.15527344,  0.25585938,  0.05981445,  ...,  0.23437500,\n",
      "           0.60156250, -0.10449219],\n",
      "         [-0.03955078,  0.31640625,  0.14355469,  ...,  0.25000000,\n",
      "           0.36718750, -0.20898438],\n",
      "         ...,\n",
      "         [-0.55859375, -0.25781250, -0.34960938,  ...,  0.02050781,\n",
      "          -0.07226562, -0.05004883],\n",
      "         [-0.55859375, -0.25781250, -0.34960938,  ...,  0.02050781,\n",
      "          -0.07226562, -0.05004883],\n",
      "         [-0.55859375, -0.25781250, -0.34960938,  ...,  0.02050781,\n",
      "          -0.07226562, -0.05004883]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.03125000,  0.02880859,  0.05688477,  ...,  0.18261719,\n",
      "          -0.41992188, -0.15527344],\n",
      "         [ 0.03442383, -0.01977539,  0.56640625,  ..., -0.26953125,\n",
      "          -0.15039062, -0.23437500],\n",
      "         [ 0.07177734,  0.00643921,  0.45507812,  ..., -0.23632812,\n",
      "          -0.13671875, -0.13867188],\n",
      "         ...,\n",
      "         [-0.11376953, -0.25390625,  0.30273438,  ...,  0.06787109,\n",
      "          -0.29101562, -0.03125000],\n",
      "         [-0.11376953, -0.25390625,  0.30273438,  ...,  0.06787109,\n",
      "          -0.29101562, -0.03125000],\n",
      "         [-0.11376953, -0.25390625,  0.30273438,  ...,  0.06787109,\n",
      "          -0.29101562, -0.03125000]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.05834961,  0.08251953, -0.08154297,  ...,  0.22558594,\n",
      "          -0.01123047,  0.20410156],\n",
      "         [ 0.26953125,  0.48632812, -0.41796875,  ...,  0.35351562,\n",
      "           0.15332031,  0.28906250],\n",
      "         [ 0.08642578,  0.14941406, -0.16992188,  ...,  0.45312500,\n",
      "          -0.13476562,  0.38476562],\n",
      "         ...,\n",
      "         [-0.05590820, -0.19824219, -0.09179688,  ...,  0.27343750,\n",
      "           0.01306152,  0.28320312],\n",
      "         [-0.05590820, -0.19824219, -0.09179688,  ...,  0.27343750,\n",
      "           0.01306152,  0.28320312],\n",
      "         [-0.05590820, -0.19824219, -0.09179688,  ...,  0.27343750,\n",
      "           0.01306152,  0.28320312]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.09423828, -0.08837891, -0.44335938,  ...,  0.12597656,\n",
      "          -0.01770020, -0.10058594],\n",
      "         [-0.05834961, -0.08056641, -0.05346680,  ...,  0.04418945,\n",
      "           0.00946045, -0.08496094],\n",
      "         [-0.19921875, -0.35546875,  0.22167969,  ...,  0.06030273,\n",
      "          -0.08740234, -0.14453125],\n",
      "         ...,\n",
      "         [ 0.24316406, -0.17480469, -0.51953125,  ...,  0.13085938,\n",
      "           0.07958984, -0.06640625],\n",
      "         [ 0.24316406, -0.17480469, -0.51953125,  ...,  0.13085938,\n",
      "           0.07958984, -0.06640625],\n",
      "         [ 0.24316406, -0.17480469, -0.51953125,  ...,  0.13085938,\n",
      "           0.07958984, -0.06640625]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.09960938, -0.10253906,  0.14648438,  ..., -0.05786133,\n",
      "          -0.02099609, -0.07861328],\n",
      "         [-0.15429688, -0.07080078, -0.06054688,  ..., -0.22656250,\n",
      "          -0.09472656, -0.24511719],\n",
      "         [-0.08203125, -0.13183594, -0.05859375,  ..., -0.06152344,\n",
      "           0.01287842,  0.16308594],\n",
      "         ...,\n",
      "         [ 0.11718750,  0.05224609,  0.35546875,  ..., -0.51953125,\n",
      "          -0.11279297, -0.68750000],\n",
      "         [ 0.11718750,  0.05224609,  0.35546875,  ..., -0.51953125,\n",
      "          -0.11279297, -0.68750000],\n",
      "         [ 0.11718750,  0.05224609,  0.35546875,  ..., -0.51953125,\n",
      "          -0.11279297, -0.68750000]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.01855469, -0.05419922, -0.09960938,  ..., -0.18847656,\n",
      "           0.14257812, -0.16699219],\n",
      "         [ 0.12451172,  0.18945312, -0.18066406,  ..., -0.93750000,\n",
      "          -0.17382812,  0.02209473],\n",
      "         [ 0.09326172,  0.28320312, -0.13769531,  ..., -0.69531250,\n",
      "           0.08740234,  0.07958984],\n",
      "         ...,\n",
      "         [ 0.11718750, -0.13867188,  0.02453613,  ..., -0.26367188,\n",
      "           0.07373047, -0.14257812],\n",
      "         [ 0.11718750, -0.13867188,  0.02453613,  ..., -0.26367188,\n",
      "           0.07373047, -0.14257812],\n",
      "         [ 0.11718750, -0.13867188,  0.02453613,  ..., -0.26367188,\n",
      "           0.07373047, -0.14257812]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 1.82617188e-01, -8.54492188e-02,  1.25976562e-01,  ...,\n",
      "           1.83582306e-05,  8.78906250e-03, -4.90722656e-02],\n",
      "         [-7.08007812e-02, -5.41992188e-02,  1.20117188e-01,  ...,\n",
      "          -1.07421875e-02,  2.64892578e-02, -1.45263672e-02],\n",
      "         [-4.29687500e-02, -1.12304688e-01,  1.68945312e-01,  ...,\n",
      "          -5.27343750e-02,  8.59375000e-02,  1.86920166e-03],\n",
      "         ...,\n",
      "         [ 3.00781250e-01,  2.89062500e-01,  1.95312500e-01,  ...,\n",
      "          -1.14257812e-01,  3.67736816e-03,  1.48437500e-01],\n",
      "         [ 3.00781250e-01,  2.89062500e-01,  1.95312500e-01,  ...,\n",
      "          -1.14257812e-01,  3.67736816e-03,  1.48437500e-01],\n",
      "         [ 3.00781250e-01,  2.89062500e-01,  1.95312500e-01,  ...,\n",
      "          -1.14257812e-01,  3.67736816e-03,  1.48437500e-01]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.08251953, -0.15136719, -0.04736328,  ..., -0.19335938,\n",
      "          -0.23144531,  0.18066406],\n",
      "         [ 0.05395508, -0.07666016, -0.01190186,  ..., -0.15332031,\n",
      "          -0.04296875, -0.10351562],\n",
      "         [ 0.09033203, -0.06591797,  0.08007812,  ..., -0.19433594,\n",
      "           0.09082031, -0.20898438],\n",
      "         ...,\n",
      "         [-0.34570312,  0.37500000,  0.06591797,  ..., -0.00066376,\n",
      "           0.27343750,  0.07861328],\n",
      "         [-0.34570312,  0.37500000,  0.06591797,  ..., -0.00066376,\n",
      "           0.27343750,  0.07861328],\n",
      "         [-0.34570312,  0.37500000,  0.06591797,  ..., -0.00066376,\n",
      "           0.27343750,  0.07861328]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.05810547, -0.76562500, -0.15722656,  ..., -0.36718750,\n",
      "          -0.13085938,  0.21582031],\n",
      "         [-0.38085938, -0.23339844, -0.00613403,  ..., -0.29492188,\n",
      "          -0.93359375,  1.07812500],\n",
      "         [-0.91015625, -0.28906250,  0.22460938,  ..., -0.16113281,\n",
      "          -0.23144531,  0.37500000],\n",
      "         ...,\n",
      "         [-0.31445312, -0.29882812, -0.12890625,  ..., -0.55078125,\n",
      "           0.35546875, -0.33007812],\n",
      "         [-0.31445312, -0.29882812, -0.12890625,  ..., -0.55078125,\n",
      "           0.35546875, -0.33007812],\n",
      "         [-0.31445312, -0.29882812, -0.12890625,  ..., -0.55078125,\n",
      "           0.35546875, -0.33007812]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.18066406, -0.40820312,  0.15136719,  ..., -0.23339844,\n",
      "           0.09082031, -0.23046875],\n",
      "         [-0.17382812, -0.20312500,  0.33007812,  ..., -0.15917969,\n",
      "           0.06250000, -0.24121094],\n",
      "         [-0.37890625, -0.03833008,  0.31445312,  ..., -0.36132812,\n",
      "           0.11083984, -0.30468750],\n",
      "         ...,\n",
      "         [-0.32421875, -0.20312500,  0.41992188,  ...,  0.16992188,\n",
      "           0.63671875, -0.04956055],\n",
      "         [-0.32421875, -0.20312500,  0.41992188,  ...,  0.16992188,\n",
      "           0.63671875, -0.04956055],\n",
      "         [-0.32421875, -0.20312500,  0.41992188,  ...,  0.16992188,\n",
      "           0.63671875, -0.04956055]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.01013184,  0.56250000, -0.62890625,  ..., -1.80468750,\n",
      "           0.13964844,  1.44531250],\n",
      "         [-0.15917969, -0.19628906,  0.24218750,  ..., -0.11669922,\n",
      "          -0.12890625,  0.50390625],\n",
      "         [ 0.19042969, -0.14355469,  0.17578125,  ..., -0.46093750,\n",
      "           0.27734375,  0.32421875],\n",
      "         ...,\n",
      "         [ 0.00729370, -0.10400391, -0.59375000,  ...,  0.08203125,\n",
      "          -0.43164062,  0.11279297],\n",
      "         [ 0.00729370, -0.10400391, -0.59375000,  ...,  0.08203125,\n",
      "          -0.43164062,  0.11279297],\n",
      "         [ 0.00729370, -0.10400391, -0.59375000,  ...,  0.08203125,\n",
      "          -0.43164062,  0.11279297]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.06884766, -0.25585938, -0.12060547,  ...,  0.02038574,\n",
      "          -0.45703125,  0.41796875],\n",
      "         [-0.08789062, -0.07031250, -0.01953125,  ...,  0.55078125,\n",
      "           0.05053711,  0.71484375],\n",
      "         [-0.21777344,  0.20703125,  0.01092529,  ...,  0.47656250,\n",
      "          -0.06176758,  0.67187500],\n",
      "         ...,\n",
      "         [-0.13476562, -0.21972656, -0.10253906,  ...,  0.10058594,\n",
      "          -0.33398438,  0.50781250],\n",
      "         [-0.13476562, -0.21972656, -0.10253906,  ...,  0.10058594,\n",
      "          -0.33398438,  0.50781250],\n",
      "         [-0.13476562, -0.21972656, -0.10253906,  ...,  0.10058594,\n",
      "          -0.33398438,  0.50781250]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.16210938, -0.05712891,  0.12792969,  ...,  0.08349609,\n",
      "          -0.07373047, -0.00189209],\n",
      "         [-0.41406250, -0.00427246,  0.45898438,  ...,  0.59765625,\n",
      "          -0.98046875,  0.23144531],\n",
      "         [-0.17285156, -0.06445312,  0.18847656,  ...,  0.39257812,\n",
      "          -0.81250000,  0.03051758],\n",
      "         ...,\n",
      "         [-0.14648438,  0.03662109,  0.30078125,  ...,  0.48828125,\n",
      "           0.09423828, -0.15234375],\n",
      "         [-0.14648438,  0.03662109,  0.30078125,  ...,  0.48828125,\n",
      "           0.09423828, -0.15234375],\n",
      "         [-0.14648438,  0.03662109,  0.30078125,  ...,  0.48828125,\n",
      "           0.09423828, -0.15234375]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.95703125,  0.77734375, -0.22363281,  ...,  0.25195312,\n",
      "           0.26562500,  0.33398438],\n",
      "         [ 3.62500000,  4.90625000, -0.75390625,  ...,  0.15332031,\n",
      "           0.76953125,  0.41015625],\n",
      "         [ 2.34375000,  3.35937500, -0.48046875,  ...,  0.04541016,\n",
      "           0.88281250,  0.34570312],\n",
      "         ...,\n",
      "         [-0.47656250, -0.34375000,  0.45312500,  ...,  0.25390625,\n",
      "           0.13769531,  0.34570312],\n",
      "         [-0.47656250, -0.34375000,  0.45312500,  ...,  0.25390625,\n",
      "           0.13769531,  0.34570312],\n",
      "         [-0.47656250, -0.34375000,  0.45312500,  ...,  0.25390625,\n",
      "           0.13769531,  0.34570312]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.14648438, -0.11230469,  0.11718750,  ..., -0.23730469,\n",
      "          -0.40039062,  0.24511719],\n",
      "         [ 0.44140625, -0.24121094, -0.09570312,  ..., -0.30664062,\n",
      "          -0.42968750,  0.03955078],\n",
      "         [ 0.33593750, -0.33203125,  0.10498047,  ..., -0.11718750,\n",
      "          -0.12597656, -0.04052734],\n",
      "         ...,\n",
      "         [ 0.32617188, -0.22167969,  0.22558594,  ..., -0.02087402,\n",
      "           0.01336670,  0.01043701],\n",
      "         [ 0.32617188, -0.22167969,  0.22558594,  ..., -0.02087402,\n",
      "           0.01336670,  0.01043701],\n",
      "         [ 0.32617188, -0.22167969,  0.22558594,  ..., -0.02087402,\n",
      "           0.01336670,  0.01043701]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.03710938,  0.17675781,  0.04492188,  ..., -0.40039062,\n",
      "           0.41406250,  0.00711060],\n",
      "         [-0.19628906, -0.12060547,  0.01904297,  ..., -0.07080078,\n",
      "           0.06152344, -0.25976562],\n",
      "         [-0.22949219, -0.06982422,  0.06250000,  ..., -0.11865234,\n",
      "           0.05786133, -0.20312500],\n",
      "         ...,\n",
      "         [-0.47656250,  0.06738281, -0.07617188,  ...,  0.20410156,\n",
      "          -0.20996094, -0.04003906],\n",
      "         [-0.47656250,  0.06738281, -0.07617188,  ...,  0.20410156,\n",
      "          -0.20996094, -0.04003906],\n",
      "         [-0.47656250,  0.06738281, -0.07617188,  ...,  0.20410156,\n",
      "          -0.20996094, -0.04003906]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[ 0.06298828,  0.24218750, -0.07519531,  ..., -0.05615234,\n",
      "           0.13281250, -0.23535156],\n",
      "         [-0.10791016,  0.41210938, -0.04809570,  ..., -0.06225586,\n",
      "           0.04931641,  0.05468750],\n",
      "         [ 0.11865234,  0.23046875, -0.03857422,  ..., -0.03515625,\n",
      "          -0.25390625,  0.33593750],\n",
      "         ...,\n",
      "         [-0.03930664,  0.36328125, -0.11914062,  ...,  0.01794434,\n",
      "          -0.14746094, -0.22851562],\n",
      "         [-0.03930664,  0.36328125, -0.11914062,  ...,  0.01794434,\n",
      "          -0.14746094, -0.22851562],\n",
      "         [-0.03930664,  0.36328125, -0.11914062,  ...,  0.01794434,\n",
      "          -0.14746094, -0.22851562]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.08544922, -0.08349609,  0.17578125,  ..., -0.04931641,\n",
      "           0.00775146, -0.07617188],\n",
      "         [-0.35156250, -0.10302734,  0.06982422,  ...,  0.38281250,\n",
      "           0.57812500,  0.52343750],\n",
      "         [-0.30273438, -0.17382812,  0.20507812,  ...,  0.33203125,\n",
      "           0.55859375,  0.28515625],\n",
      "         ...,\n",
      "         [-0.82421875, -0.33593750,  0.53515625,  ..., -0.10205078,\n",
      "          -0.03515625, -0.03222656],\n",
      "         [-0.82421875, -0.33593750,  0.53515625,  ..., -0.10205078,\n",
      "          -0.03515625, -0.03222656],\n",
      "         [-0.82421875, -0.33593750,  0.53515625,  ..., -0.10205078,\n",
      "          -0.03515625, -0.03222656]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch torch.Size([1, 4128, 1280]) tensor([[[-0.74218750,  0.49023438,  0.37500000,  ..., -0.04589844,\n",
      "           0.32812500,  0.21191406],\n",
      "         [ 0.18066406, -0.61328125, -0.13281250,  ..., -0.01531982,\n",
      "           0.67578125,  0.51562500],\n",
      "         [-0.59765625, -0.50390625,  0.28125000,  ..., -0.05761719,\n",
      "           0.43359375,  0.19335938],\n",
      "         ...,\n",
      "         [-0.33593750,  0.07275391, -0.03491211,  ..., -0.37500000,\n",
      "          -0.14746094,  0.57031250],\n",
      "         [-0.33593750,  0.07275391, -0.03491211,  ..., -0.37500000,\n",
      "          -0.14746094,  0.57031250],\n",
      "         [-0.33593750,  0.07275391, -0.03491211,  ..., -0.37500000,\n",
      "          -0.14746094,  0.57031250]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs_jax = processor(images=image, return_tensors=\"jax\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_torch = model_torch(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax (1, 4128, 1280) [[[-0.0703125 -0.210938 0.0288086 ... 0.000736237 0.0216064 0.103516]\n",
      "  [-0.0703125 -0.210938 0.0288086 ... 0.000736237 0.0216064 0.103516]\n",
      "  [-0.0703125 -0.210938 0.0288086 ... 0.000736237 0.0216064 0.103516]\n",
      "  ...\n",
      "  [-0.081543 0.057373 -0.0206299 ... -0.0693359 -0.0205078 -0.0324707]\n",
      "  [-0.081543 0.057373 -0.0206299 ... -0.0693359 -0.0205078 -0.0324707]\n",
      "  [-0.081543 0.057373 -0.0206299 ... -0.0693359 -0.0205078 -0.0324707]]]\n",
      "jax (1, 4128, 1280) [[[-0.00202942 0.269531 0.00460815 ... -0.0737305 -0.090332 0.0142212]\n",
      "  [-0.00202942 0.269531 0.00460815 ... -0.0737305 -0.090332 0.0142212]\n",
      "  [-0.00202942 0.269531 0.00460815 ... -0.0737305 -0.090332 0.0142212]\n",
      "  ...\n",
      "  [-0.123047 -0.425781 0.566406 ... -0.139648 0.236328 0.181641]\n",
      "  [-0.123047 -0.425781 0.566406 ... -0.139648 0.236328 0.181641]\n",
      "  [-0.123047 -0.425781 0.566406 ... -0.139648 0.236328 0.181641]]]\n",
      "jax (1, 4128, 1280) [[[-0.142578 -0.0090332 0.0158691 ... -0.0311279 0.0133057 -0.0217285]\n",
      "  [-0.142578 -0.0090332 0.0158691 ... -0.0311279 0.0133057 -0.0217285]\n",
      "  [-0.142578 -0.0090332 0.0158691 ... -0.0311279 0.0133057 -0.0217285]\n",
      "  ...\n",
      "  [0.0289307 -0.0286865 0.052002 ... -0.154297 -0.116211 0.0639648]\n",
      "  [0.0289307 -0.0286865 0.052002 ... -0.154297 -0.116211 0.0639648]\n",
      "  [0.0289307 -0.0286865 0.052002 ... -0.154297 -0.116211 0.0639648]]]\n",
      "jax (1, 4128, 1280) [[[0.0336914 0.0859375 -0.0219727 ... 0.109375 -0.25 0.0179443]\n",
      "  [0.0336914 0.0859375 -0.0219727 ... 0.109375 -0.25 0.0179443]\n",
      "  [0.0336914 0.0859375 -0.0219727 ... 0.109375 -0.25 0.0179443]\n",
      "  ...\n",
      "  [-0.0107422 -0.0756836 0.15625 ... -0.238281 -0.0776367 0.589844]\n",
      "  [-0.0107422 -0.0756836 0.15625 ... -0.238281 -0.0776367 0.589844]\n",
      "  [-0.0107422 -0.0756836 0.15625 ... -0.238281 -0.0776367 0.589844]]]\n",
      "jax (1, 4128, 1280) [[[0.0517578 0.0010376 -0.0181885 ... -0.0473633 0.0101929 -0.0595703]\n",
      "  [0.0517578 0.0010376 -0.0181885 ... -0.0473633 0.0101929 -0.0595703]\n",
      "  [0.0517578 0.0010376 -0.0181885 ... -0.0473633 0.0101929 -0.0595703]\n",
      "  ...\n",
      "  [-0.236328 -0.208008 -0.0576172 ... 0.384766 0.0241699 -0.178711]\n",
      "  [-0.236328 -0.208008 -0.0576172 ... 0.384766 0.0241699 -0.178711]\n",
      "  [-0.236328 -0.208008 -0.0576172 ... 0.384766 0.0241699 -0.178711]]]\n",
      "jax (1, 4128, 1280) [[[0.11377 0.0576172 -0.0281982 ... 0.0444336 0.0922852 0.0339355]\n",
      "  [0.11377 0.0576172 -0.0281982 ... 0.0444336 0.0922852 0.0339355]\n",
      "  [0.11377 0.0576172 -0.0281982 ... 0.0444336 0.0922852 0.0339355]\n",
      "  ...\n",
      "  [0.388672 0.228516 0.255859 ... 0.376953 0.232422 0.355469]\n",
      "  [0.388672 0.228516 0.255859 ... 0.376953 0.232422 0.355469]\n",
      "  [0.388672 0.228516 0.255859 ... 0.376953 0.232422 0.355469]]]\n",
      "jax (1, 4128, 1280) [[[-0.0649414 -0.041748 0.25 ... 0.0578613 0.0385742 -0.371094]\n",
      "  [-0.0649414 -0.041748 0.25 ... 0.0578613 0.0385742 -0.371094]\n",
      "  [-0.0649414 -0.041748 0.25 ... 0.0578613 0.0385742 -0.371094]\n",
      "  ...\n",
      "  [-0.398438 0.188477 0.371094 ... 0.00482178 0.120117 -0.511719]\n",
      "  [-0.398438 0.188477 0.371094 ... 0.00482178 0.120117 -0.511719]\n",
      "  [-0.398438 0.188477 0.371094 ... 0.00482178 0.120117 -0.511719]]]\n",
      "jax (1, 4128, 1280) [[[-0.090332 0.0742188 0.0795898 ... -0.263672 -0.097168 -0.0405273]\n",
      "  [-0.090332 0.0742188 0.0795898 ... -0.263672 -0.097168 -0.0405273]\n",
      "  [-0.090332 0.0742188 0.0795898 ... -0.263672 -0.097168 -0.0405273]\n",
      "  ...\n",
      "  [0.261719 0.347656 -0.277344 ... -0.0678711 0.144531 0.322266]\n",
      "  [0.261719 0.347656 -0.277344 ... -0.0678711 0.144531 0.322266]\n",
      "  [0.261719 0.347656 -0.277344 ... -0.0678711 0.144531 0.322266]]]\n",
      "jax (1, 4128, 1280) [[[-0.0078125 -0.11084 -0.194336 ... -0.376953 0.0849609 0.117188]\n",
      "  [-0.0078125 -0.11084 -0.194336 ... -0.376953 0.0849609 0.117188]\n",
      "  [-0.0078125 -0.11084 -0.194336 ... -0.376953 0.0849609 0.117188]\n",
      "  ...\n",
      "  [-0.316406 -0.0639648 -0.188477 ... -0.310547 -0.498047 -0.351562]\n",
      "  [-0.316406 -0.0639648 -0.188477 ... -0.310547 -0.498047 -0.351562]\n",
      "  [-0.316406 -0.0639648 -0.188477 ... -0.310547 -0.498047 -0.351562]]]\n",
      "jax (1, 4128, 1280) [[[0.0612793 0.209961 0.217773 ... 0.287109 0.300781 0.163086]\n",
      "  [0.0612793 0.209961 0.217773 ... 0.287109 0.300781 0.163086]\n",
      "  [0.0612793 0.209961 0.217773 ... 0.287109 0.300781 0.163086]\n",
      "  ...\n",
      "  [0.578125 0.0878906 0.129883 ... 0.179688 0.511719 0.472656]\n",
      "  [0.578125 0.0878906 0.129883 ... 0.179688 0.511719 0.472656]\n",
      "  [0.578125 0.0878906 0.129883 ... 0.179688 0.511719 0.472656]]]\n",
      "jax (1, 4128, 1280) [[[0.324219 -0.0395508 -0.257812 ... -0.220703 -0.18457 0.0291748]\n",
      "  [0.324219 -0.0395508 -0.257812 ... -0.220703 -0.18457 0.0291748]\n",
      "  [0.324219 -0.0395508 -0.257812 ... -0.220703 -0.18457 0.0291748]\n",
      "  ...\n",
      "  [-0.191406 -0.710938 -0.710938 ... 0.22168 -0.386719 -0.632812]\n",
      "  [-0.191406 -0.710938 -0.710938 ... 0.22168 -0.386719 -0.632812]\n",
      "  [-0.191406 -0.710938 -0.710938 ... 0.22168 -0.386719 -0.632812]]]\n",
      "jax (1, 4128, 1280) [[[-0.0678711 -0.18457 -0.144531 ... -0.0532227 0.0791016 -0.18457]\n",
      "  [-0.0678711 -0.18457 -0.144531 ... -0.0532227 0.0791016 -0.18457]\n",
      "  [-0.0678711 -0.18457 -0.144531 ... -0.0532227 0.0791016 -0.18457]\n",
      "  ...\n",
      "  [-0.130859 0.248047 0.0273438 ... 0.625 0.439453 0.40625]\n",
      "  [-0.130859 0.248047 0.0273438 ... 0.625 0.439453 0.40625]\n",
      "  [-0.130859 0.248047 0.0273438 ... 0.625 0.439453 0.40625]]]\n",
      "jax (1, 4128, 1280) [[[-0.0324707 -0.0267334 0.0255127 ... 0.714844 0.539062 0.0791016]\n",
      "  [-0.0324707 -0.0267334 0.0255127 ... 0.714844 0.539062 0.0791016]\n",
      "  [-0.0324707 -0.0267334 0.0255127 ... 0.714844 0.539062 0.0791016]\n",
      "  ...\n",
      "  [1.0625 -0.0957031 -0.289062 ... 0.0810547 0.149414 -0.0356445]\n",
      "  [1.0625 -0.0957031 -0.289062 ... 0.0810547 0.149414 -0.0356445]\n",
      "  [1.0625 -0.0957031 -0.289062 ... 0.0810547 0.149414 -0.0356445]]]\n",
      "jax (1, 4128, 1280) [[[0.0341797 -0.320312 -0.273438 ... 0.0324707 -0.269531 0.291016]\n",
      "  [0.0341797 -0.320312 -0.273438 ... 0.0324707 -0.269531 0.291016]\n",
      "  [0.0341797 -0.320312 -0.273438 ... 0.0324707 -0.269531 0.291016]\n",
      "  ...\n",
      "  [-0.328125 0.441406 -0.78125 ... -0.628906 -0.0844727 0.585938]\n",
      "  [-0.328125 0.441406 -0.78125 ... -0.628906 -0.0844727 0.585938]\n",
      "  [-0.328125 0.441406 -0.78125 ... -0.628906 -0.0844727 0.585938]]]\n",
      "jax (1, 4128, 1280) [[[0.0668945 0.210938 0.185547 ... -0.359375 0.320312 0.237305]\n",
      "  [0.0668945 0.210938 0.185547 ... -0.359375 0.320312 0.237305]\n",
      "  [0.0668945 0.210938 0.185547 ... -0.359375 0.320312 0.237305]\n",
      "  ...\n",
      "  [-0.0976562 0.917969 0.464844 ... -0.0522461 1.21875 0.225586]\n",
      "  [-0.0976562 0.917969 0.464844 ... -0.0522461 1.21875 0.225586]\n",
      "  [-0.0976562 0.917969 0.464844 ... -0.0522461 1.21875 0.225586]]]\n",
      "jax (1, 4128, 1280) [[[-0.234375 -0.330078 0.427734 ... 0.0427246 0.378906 -0.174805]\n",
      "  [-0.234375 -0.330078 0.427734 ... 0.0427246 0.378906 -0.174805]\n",
      "  [-0.234375 -0.330078 0.427734 ... 0.0427246 0.378906 -0.174805]\n",
      "  ...\n",
      "  [0.253906 0.0654297 -0.3125 ... 0.667969 0.166992 -0.4375]\n",
      "  [0.253906 0.0654297 -0.3125 ... 0.667969 0.166992 -0.4375]\n",
      "  [0.253906 0.0654297 -0.3125 ... 0.667969 0.166992 -0.4375]]]\n",
      "jax (1, 4128, 1280) [[[0.0148926 0.239258 0.124023 ... -0.59375 0.00196838 -1.21094]\n",
      "  [0.0148926 0.239258 0.124023 ... -0.59375 0.00196838 -1.21094]\n",
      "  [0.0148926 0.239258 0.124023 ... -0.59375 0.00196838 -1.21094]\n",
      "  ...\n",
      "  [-0.347656 0.53125 -0.0098877 ... 0.294922 0.137695 -0.851562]\n",
      "  [-0.347656 0.53125 -0.0098877 ... 0.294922 0.137695 -0.851562]\n",
      "  [-0.347656 0.53125 -0.0098877 ... 0.294922 0.137695 -0.851562]]]\n",
      "jax (1, 4128, 1280) [[[0.0593262 0.0483398 -0.322266 ... -0.308594 0.59375 -0.589844]\n",
      "  [0.0593262 0.0483398 -0.322266 ... -0.308594 0.59375 -0.589844]\n",
      "  [0.0593262 0.0483398 -0.322266 ... -0.308594 0.59375 -0.589844]\n",
      "  ...\n",
      "  [-0.882812 0.048584 -0.0308838 ... -0.46875 1.79688 0.232422]\n",
      "  [-0.882812 0.048584 -0.0308838 ... -0.46875 1.79688 0.232422]\n",
      "  [-0.882812 0.048584 -0.0308838 ... -0.46875 1.79688 0.232422]]]\n",
      "jax (1, 4128, 1280) [[[-0.621094 -0.0986328 -0.0888672 ... 0.1875 -0.128906 0.451172]\n",
      "  [-0.621094 -0.0986328 -0.0888672 ... 0.1875 -0.128906 0.451172]\n",
      "  [-0.621094 -0.0986328 -0.0888672 ... 0.1875 -0.128906 0.451172]\n",
      "  ...\n",
      "  [-0.800781 2.15625 0.257812 ... 1.44531 -0.392578 0.433594]\n",
      "  [-0.800781 2.15625 0.257812 ... 1.44531 -0.392578 0.433594]\n",
      "  [-0.800781 2.15625 0.257812 ... 1.44531 -0.392578 0.433594]]]\n",
      "jax (1, 4128, 1280) [[[0.225586 -0.0517578 0.597656 ... -0.119629 0.166992 0.367188]\n",
      "  [0.225586 -0.0517578 0.597656 ... -0.119629 0.166992 0.367188]\n",
      "  [0.225586 -0.0517578 0.597656 ... -0.119629 0.166992 0.367188]\n",
      "  ...\n",
      "  [0.636719 0.0019989 1.39844 ... -0.0800781 -0.460938 0.24707]\n",
      "  [0.636719 0.0019989 1.39844 ... -0.0800781 -0.460938 0.24707]\n",
      "  [0.636719 0.0019989 1.39844 ... -0.0800781 -0.460938 0.24707]]]\n",
      "jax (1, 4128, 1280) [[[0.341797 -0.060791 -0.542969 ... -0.292969 -0.0610352 -0.498047]\n",
      "  [0.341797 -0.060791 -0.542969 ... -0.292969 -0.0610352 -0.498047]\n",
      "  [0.341797 -0.060791 -0.542969 ... -0.292969 -0.0610352 -0.498047]\n",
      "  ...\n",
      "  [-0.25 0.269531 0.0874023 ... 0.15918 -0.589844 0.757812]\n",
      "  [-0.25 0.269531 0.0874023 ... 0.15918 -0.589844 0.757812]\n",
      "  [-0.25 0.269531 0.0874023 ... 0.15918 -0.589844 0.757812]]]\n",
      "jax (1, 4128, 1280) [[[0.408203 0.0201416 -0.277344 ... -0.211914 -0.168945 0.302734]\n",
      "  [0.408203 0.0201416 -0.277344 ... -0.211914 -0.168945 0.302734]\n",
      "  [0.408203 0.0201416 -0.277344 ... -0.211914 -0.168945 0.302734]\n",
      "  ...\n",
      "  [-1.82031 -0.273438 0.695312 ... -0.605469 -0.202148 0.0055542]\n",
      "  [-1.82031 -0.273438 0.695312 ... -0.605469 -0.202148 0.0055542]\n",
      "  [-1.82031 -0.273438 0.695312 ... -0.605469 -0.202148 0.0055542]]]\n",
      "jax (1, 4128, 1280) [[[-0.0717773 -0.126953 0.041748 ... -0.382812 -0.0917969 -0.0307617]\n",
      "  [-0.0717773 -0.126953 0.041748 ... -0.382812 -0.0917969 -0.0307617]\n",
      "  [-0.0717773 -0.126953 0.041748 ... -0.382812 -0.0917969 -0.0307617]\n",
      "  ...\n",
      "  [-0.761719 -0.0166016 1 ... -0.376953 -0.229492 0.337891]\n",
      "  [-0.761719 -0.0166016 1 ... -0.376953 -0.229492 0.337891]\n",
      "  [-0.761719 -0.0166016 1 ... -0.376953 -0.229492 0.337891]]]\n",
      "jax (1, 4128, 1280) [[[0.223633 -0.292969 0.128906 ... -0.519531 -0.519531 -0.0805664]\n",
      "  [0.223633 -0.292969 0.128906 ... -0.519531 -0.519531 -0.0805664]\n",
      "  [0.223633 -0.292969 0.128906 ... -0.519531 -0.519531 -0.0805664]\n",
      "  ...\n",
      "  [0.285156 0.155273 -0.632812 ... 0.332031 0.3125 0.273438]\n",
      "  [0.285156 0.155273 -0.632812 ... 0.332031 0.3125 0.273438]\n",
      "  [0.285156 0.155273 -0.632812 ... 0.332031 0.3125 0.273438]]]\n",
      "jax (1, 4128, 1280) [[[0.292969 -0.157227 -0.425781 ... 0.133789 -0.386719 -0.207031]\n",
      "  [0.292969 -0.157227 -0.425781 ... 0.133789 -0.386719 -0.207031]\n",
      "  [0.292969 -0.157227 -0.425781 ... 0.133789 -0.386719 -0.207031]\n",
      "  ...\n",
      "  [1.39062 -0.980469 0.0152588 ... -1.21094 0.890625 -0.141602]\n",
      "  [1.39062 -0.980469 0.0152588 ... -1.21094 0.890625 -0.141602]\n",
      "  [1.39062 -0.980469 0.0152588 ... -1.21094 0.890625 -0.141602]]]\n",
      "jax (1, 4128, 1280) [[[-0.25 0.081543 -0.00735474 ... -0.464844 -0.214844 0.134766]\n",
      "  [-0.25 0.081543 -0.00735474 ... -0.464844 -0.214844 0.134766]\n",
      "  [-0.25 0.081543 -0.00735474 ... -0.464844 -0.214844 0.134766]\n",
      "  ...\n",
      "  [0.429688 -0.341797 -0.367188 ... -1.13281 0.186523 0.0510254]\n",
      "  [0.429688 -0.341797 -0.367188 ... -1.13281 0.186523 0.0510254]\n",
      "  [0.429688 -0.341797 -0.367188 ... -1.13281 0.186523 0.0510254]]]\n",
      "jax (1, 4128, 1280) [[[0.306641 0.178711 -0.121094 ... -0.130859 0.847656 -0.0405273]\n",
      "  [0.306641 0.178711 -0.121094 ... -0.130859 0.847656 -0.0405273]\n",
      "  [0.306641 0.178711 -0.121094 ... -0.130859 0.847656 -0.0405273]\n",
      "  ...\n",
      "  [0.451172 -0.371094 0.867188 ... 0.253906 0.382812 -0.457031]\n",
      "  [0.451172 -0.371094 0.867188 ... 0.253906 0.382812 -0.457031]\n",
      "  [0.451172 -0.371094 0.867188 ... 0.253906 0.382812 -0.457031]]]\n",
      "jax (1, 4128, 1280) [[[0.0795898 0.263672 -0.147461 ... 0.318359 0.212891 0.0947266]\n",
      "  [0.0795898 0.263672 -0.147461 ... 0.318359 0.212891 0.0947266]\n",
      "  [0.0795898 0.263672 -0.147461 ... 0.318359 0.212891 0.0947266]\n",
      "  ...\n",
      "  [0.213867 -0.0927734 -0.242188 ... 0.310547 -0.78125 -0.847656]\n",
      "  [0.213867 -0.0927734 -0.242188 ... 0.310547 -0.78125 -0.847656]\n",
      "  [0.213867 -0.0927734 -0.242188 ... 0.310547 -0.78125 -0.847656]]]\n",
      "jax (1, 4128, 1280) [[[0.0429688 0.271484 0.318359 ... -0.129883 -0.170898 0.0336914]\n",
      "  [0.0429688 0.271484 0.318359 ... -0.129883 -0.170898 0.0336914]\n",
      "  [0.0429688 0.271484 0.318359 ... -0.129883 -0.170898 0.0336914]\n",
      "  ...\n",
      "  [0.382812 -0.746094 -0.675781 ... 0.396484 0.8125 0.227539]\n",
      "  [0.382812 -0.746094 -0.675781 ... 0.396484 0.8125 0.227539]\n",
      "  [0.382812 -0.746094 -0.675781 ... 0.396484 0.8125 0.227539]]]\n",
      "jax (1, 4128, 1280) [[[0.445312 0.100586 0.259766 ... -0.143555 -0.363281 0.589844]\n",
      "  [0.445312 0.100586 0.259766 ... -0.143555 -0.363281 0.589844]\n",
      "  [0.445312 0.100586 0.259766 ... -0.143555 -0.363281 0.589844]\n",
      "  ...\n",
      "  [0.65625 -0.761719 -0.165039 ... -0.0693359 0.675781 0.180664]\n",
      "  [0.65625 -0.761719 -0.165039 ... -0.0693359 0.675781 0.180664]\n",
      "  [0.65625 -0.761719 -0.165039 ... -0.0693359 0.675781 0.180664]]]\n",
      "jax (1, 4128, 1280) [[[-1.16406 0.0571289 0.192383 ... 0.148438 -0.390625 0.550781]\n",
      "  [-1.16406 0.0571289 0.192383 ... 0.148438 -0.390625 0.550781]\n",
      "  [-1.16406 0.0571289 0.192383 ... 0.148438 -0.390625 0.550781]\n",
      "  ...\n",
      "  [0.172852 1.47656 -1.33594 ... 0.46875 1.29688 1.03906]\n",
      "  [0.172852 1.47656 -1.33594 ... 0.46875 1.29688 1.03906]\n",
      "  [0.172852 1.47656 -1.33594 ... 0.46875 1.29688 1.03906]]]\n",
      "jax (1, 4128, 1280) [[[0.0202637 -0.0268555 0.24707 ... 0.074707 -0.275391 0.578125]\n",
      "  [0.0202637 -0.0268555 0.24707 ... 0.074707 -0.275391 0.578125]\n",
      "  [0.0202637 -0.0268555 0.24707 ... 0.074707 -0.275391 0.578125]\n",
      "  ...\n",
      "  [-0.404297 0.535156 0.108887 ... -1.89844 1.0625 1.67188]\n",
      "  [-0.404297 0.535156 0.108887 ... -1.89844 1.0625 1.67188]\n",
      "  [-0.404297 0.535156 0.108887 ... -1.89844 1.0625 1.67188]]]\n",
      "jax (1, 4128, 1280) [[[0.722656 0.554688 0.166016 ... -0.0708008 -0.566406 0.257812]\n",
      "  [0.722656 0.554688 0.166016 ... -0.0708008 -0.566406 0.257812]\n",
      "  [0.722656 0.554688 0.166016 ... -0.0708008 -0.566406 0.257812]\n",
      "  ...\n",
      "  [-1.24219 -1.65625 -0.894531 ... -0.625 2.09375 -0.824219]\n",
      "  [-1.24219 -1.65625 -0.894531 ... -0.625 2.09375 -0.824219]\n",
      "  [-1.24219 -1.65625 -0.894531 ... -0.625 2.09375 -0.824219]]]\n",
      "jax (1, 4128, 1280) [[[-2.1875 0.0981445 2.20312 ... 0.738281 -1.03906 -0.138672]\n",
      "  [-2.1875 0.0981445 2.20312 ... 0.738281 -1.03906 -0.138672]\n",
      "  [-2.1875 0.0981445 2.20312 ... 0.738281 -1.03906 -0.138672]\n",
      "  ...\n",
      "  [-1.92188 1.08594 -1.70312 ... -3.03125 0.316406 -2.28125]\n",
      "  [-1.92188 1.08594 -1.70312 ... -3.03125 0.316406 -2.28125]\n",
      "  [-1.92188 1.08594 -1.70312 ... -3.03125 0.316406 -2.28125]]]\n",
      "jax (1, 4128, 1280) [[[-0.53125 -1.02344 0.458984 ... -0.65625 -0.792969 -0.558594]\n",
      "  [-0.53125 -1.02344 0.458984 ... -0.65625 -0.792969 -0.558594]\n",
      "  [-0.53125 -1.02344 0.458984 ... -0.65625 -0.792969 -0.558594]\n",
      "  ...\n",
      "  [0.976562 -0.100586 1.3125 ... 1.71875 0.515625 -1.47656]\n",
      "  [0.976562 -0.100586 1.3125 ... 1.71875 0.515625 -1.47656]\n",
      "  [0.976562 -0.100586 1.3125 ... 1.71875 0.515625 -1.47656]]]\n",
      "jax (1, 4128, 1280) [[[-0.0844727 -0.78125 0.00198364 ... -0.191406 1.26562 -2.07812]\n",
      "  [-0.0844727 -0.78125 0.00198364 ... -0.191406 1.26562 -2.07812]\n",
      "  [-0.0844727 -0.78125 0.00198364 ... -0.191406 1.26562 -2.07812]\n",
      "  ...\n",
      "  [0.15332 0.0493164 1.27344 ... -0.269531 -0.828125 0.0412598]\n",
      "  [0.15332 0.0493164 1.27344 ... -0.269531 -0.828125 0.0412598]\n",
      "  [0.15332 0.0493164 1.27344 ... -0.269531 -0.828125 0.0412598]]]\n",
      "jax (1, 4128, 1280) [[[1.85938 -0.492188 -2.8125 ... -0.107422 -0.738281 2.15625]\n",
      "  [1.85938 -0.492188 -2.8125 ... -0.107422 -0.738281 2.15625]\n",
      "  [1.85938 -0.492188 -2.8125 ... -0.107422 -0.738281 2.15625]\n",
      "  ...\n",
      "  [0.585938 -1.59375 -2.75 ... -1.58594 -0.180664 -1.14844]\n",
      "  [0.585938 -1.59375 -2.75 ... -1.58594 -0.180664 -1.14844]\n",
      "  [0.585938 -1.59375 -2.75 ... -1.58594 -0.180664 -1.14844]]]\n",
      "jax (1, 4128, 1280) [[[-2.65625 -0.300781 -0.421875 ... 0.200195 -2.65625 0.71875]\n",
      "  [-2.65625 -0.300781 -0.421875 ... 0.200195 -2.65625 0.71875]\n",
      "  [-2.65625 -0.300781 -0.421875 ... 0.200195 -2.65625 0.71875]\n",
      "  ...\n",
      "  [-1.99219 0.357422 0.695312 ... -0.330078 -3.5 1.17188]\n",
      "  [-1.99219 0.357422 0.695312 ... -0.330078 -3.5 1.17188]\n",
      "  [-1.99219 0.357422 0.695312 ... -0.330078 -3.5 1.17188]]]\n",
      "jax (1, 4128, 1280) [[[-0.5625 0.458984 0.109375 ... -0.882812 0.851562 -1.38281]\n",
      "  [-0.5625 0.458984 0.109375 ... -0.882812 0.851562 -1.38281]\n",
      "  [-0.5625 0.458984 0.109375 ... -0.882812 0.851562 -1.38281]\n",
      "  ...\n",
      "  [-0.172852 0.00723267 -0.0186768 ... -0.210938 -0.0932617 1.17188]\n",
      "  [-0.172852 0.00723267 -0.0186768 ... -0.210938 -0.0932617 1.17188]\n",
      "  [-0.172852 0.00723267 -0.0186768 ... -0.210938 -0.0932617 1.17188]]]\n",
      "jax (1, 4128, 1280) [[[0.738281 0.5625 0.652344 ... 0.177734 -1.11719 -1.65625]\n",
      "  [0.738281 0.5625 0.652344 ... 0.177734 -1.11719 -1.65625]\n",
      "  [0.738281 0.5625 0.652344 ... 0.177734 -1.11719 -1.65625]\n",
      "  ...\n",
      "  [-0.734375 -1.36719 -1.51562 ... -1.34375 -0.523438 2.75]\n",
      "  [-0.734375 -1.36719 -1.51562 ... -1.34375 -0.523438 2.75]\n",
      "  [-0.734375 -1.36719 -1.51562 ... -1.34375 -0.523438 2.75]]]\n"
     ]
    }
   ],
   "source": [
    "output_jax = model(**inputs_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.53125000,  0.47265625, -0.35742188,  0.23925781, -2.00000000,\n",
       "        -0.24902344,  0.36718750, -0.90625000, -2.68750000, -4.12500000],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch.last_hidden_state[0,0,0,0,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.554688, 0.332031, -0.0800781, 4.34375, 7.125, -0.386719,\n",
       "       0.636719, -0.601562, -0.289062, -0.617188], dtype=bfloat16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_jax.last_hidden_state[0,0,0,0,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MllamaVisionModel, FlaxMllamaVisionModel\n",
    "from transformers import AutoProcessor, MllamaTextModel\n",
    "import requests\n",
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from PIL import Image\n",
    "from huggingface_hub import login\n",
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "hf_token = \"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\"\n",
    "login(hf_token)\n",
    "checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs_torch = processor(images=image, return_tensors=\"pt\")\n",
    "inputs_jax = processor(images=image, return_tensors=\"jax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class TorchConv(nn.Module):\n",
    "    def __init__(self, dtype=torch.bfloat16):\n",
    "        super(TorchConv, self).__init__()\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=3, #3\n",
    "            out_channels=1280, #1280\n",
    "            kernel_size=14, #14\n",
    "            stride=14, #14\n",
    "            padding=\"valid\",\n",
    "            bias=False,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.Tensor,\n",
    "\n",
    "    ):\n",
    "        batch_size, num_concurrent_media, num_tiles, num_channels, height, width = pixel_values.shape #(1, 1, 4, 3, 448, 448)\n",
    "\n",
    "        pixel_values = pixel_values.reshape(batch_size * num_concurrent_media * num_tiles, num_channels, height, width) #(4, 3, 448, 448)\n",
    "        # Patch embedding\n",
    "        patch_embeds = self.patch_embedding(pixel_values.to(torch.bfloat16).to('cuda')) # (batch_size * num_concurrent_media * num_tiles, hidden_size=1280, 32, 32)\n",
    "        return patch_embeds\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "torch_model = TorchConv(dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "\n",
    "# Get the parameters from the TorchConv model\n",
    "torch_params = torch_model.patch_embedding.weight\n",
    "# # Example PyTorch tensor\n",
    "# torch_params_32 = torch.randn(torch_params.shape, dtype=torch.float32)\n",
    "\n",
    "# # Permute the tensor's dimensions\n",
    "# torch_params_permuted = torch_params_32.permute(2, 3, 1, 0)\n",
    "\n",
    "# # Convert the PyTorch tensor to a NumPy array\n",
    "# torch_params_numpy = torch_params_permuted.detach().cpu().numpy()\n",
    "\n",
    "# # Convert the NumPy array to a JAX tensor with dtype bfloat16\n",
    "# jax_params = jnp.array(torch_params_numpy, dtype=jnp.bfloat16)\n",
    "# torch_params = torch.tensor(torch_params_32, dtype=torch.bfloat16)\n",
    "# torch_params = torch.nn.Parameter(torch.tensor(torch_params_32, dtype=torch.bfloat16, device='cuda'))\n",
    "# torch_model.patch_embedding.weight = torch_params\n",
    "torch_model.patch_embedding.weight = model_torch.patch_embedding.weight\n",
    "# Perform a forward pass through the model\n",
    "output_torch = torch_model(inputs_torch.pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as fnn\n",
    "class JaxConv(fnn.Module):\n",
    "  dtype: jnp.dtype = jnp.bfloat16\n",
    "\n",
    "  def setup(self):\n",
    "\n",
    "    self.patch_embedding = fnn.Conv(\n",
    "        1280,\n",
    "        kernel_size=(14, 14),\n",
    "        strides=(14, 14),\n",
    "        padding=\"VALID\",\n",
    "        use_bias=False,\n",
    "        dtype=self.dtype,\n",
    "        kernel_init=jax.nn.initializers.normal(),\n",
    "    )\n",
    " \n",
    "  def __call__(\n",
    "      self,\n",
    "      pixel_values: jnp.ndarray,\n",
    "  ):\n",
    "        batch_size, num_concurrent_media, num_tiles, num_channels, height, width = pixel_values.shape\n",
    "\n",
    "        pixel_values = pixel_values.reshape((batch_size * num_concurrent_media * num_tiles, num_channels, height, width))\n",
    "\n",
    "        # Patch embedding\n",
    "        patch_embeds = self.patch_embedding(pixel_values.transpose((0, 2, 3, 1)))\n",
    "\n",
    "        patch_embeds = patch_embeds.transpose((0, 3, 1, 2))\n",
    "\n",
    "        return patch_embeds\n",
    "        \n",
    "# Initialize the model\n",
    "flax_model = JaxConv()\n",
    "# Initialize parameters\n",
    "params = flax_model.init(jax.random.PRNGKey(0), jnp.ones((1, 1, 4, 3, 448, 448)))\n",
    "\n",
    "# # Convert PyTorch parameters to JAX parameters\n",
    "params = unfreeze(params)\n",
    "params['params']['patch_embedding']['kernel'] = model.params['vision_model']['patch_embedding']['kernel']\n",
    "params = freeze(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_jax = flax_model.apply(params, inputs_jax.pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[-0.191406, -0.176758, -0.185547, ..., -0.175781, -0.174805,\n",
       "          -0.0688477],\n",
       "         [0.00186157, -0.0678711, -0.234375, ..., -0.175781, -0.172852,\n",
       "          -0.170898],\n",
       "         [0.0534668, 0.114258, 0.0119629, ..., -0.203125, -0.191406,\n",
       "          -0.192383],\n",
       "         ...,\n",
       "         [0.0219727, 0.0603027, 0.0556641, ..., -0.0135498, -0.0859375,\n",
       "          -0.121582],\n",
       "         [0.034668, 0.103516, 0.032959, ..., 0.00430298, 0.010376,\n",
       "          -0.111328],\n",
       "         [0.0181885, 0.0126953, 0.00482178, ..., 0.0419922, 0.0241699,\n",
       "          -0.0947266]],\n",
       "\n",
       "        [[0.0412598, 0.0356445, 0.0444336, ..., 0.0368652, 0.0410156,\n",
       "          0.176758],\n",
       "         [0.12207, 0.0373535, 0.090332, ..., 0.0291748, 0.0358887,\n",
       "          0.0664062],\n",
       "         [-0.0280762, 0.0534668, -0.0334473, ..., 0.0456543, 0.0317383,\n",
       "          0.0152588],\n",
       "         ...,\n",
       "         [-0.0566406, 0.0668945, 0.0385742, ..., 0.0158691, -0.022583,\n",
       "          0.0622559],\n",
       "         [0.0108032, 0.0429688, 0.0495605, ..., 0.0634766,\n",
       "          -0.000610352, 0.113281],\n",
       "         [0.00497437, 0.0568848, -0.0142212, ..., -0.0751953,\n",
       "          -0.0062561, 0.115723]],\n",
       "\n",
       "        [[-0.0961914, -0.287109, -0.386719, ..., -0.277344, -0.25,\n",
       "          0.139648],\n",
       "         [0.523438, 0.10498, 0.162109, ..., -0.242188, -0.275391,\n",
       "          -0.296875],\n",
       "         [0.0634766, -0.0185547, -0.203125, ..., -0.326172, -0.248047,\n",
       "          -0.21582],\n",
       "         ...,\n",
       "         [0.122559, -0.149414, -0.0917969, ..., 0.138672, -0.439453,\n",
       "          -0.273438],\n",
       "         [0.133789, -0.1875, 0.222656, ..., -0.335938, 0.375, 0.384766],\n",
       "         [-0.359375, -0.0527344, -0.150391, ..., 0.202148, 0.135742,\n",
       "          0.213867]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.451172, -0.410156, -0.402344, ..., -0.386719, -0.378906,\n",
       "          -0.224609],\n",
       "         [0.0898438, -0.157227, -0.181641, ..., -0.384766, -0.394531,\n",
       "          -0.416016],\n",
       "         [0.135742, 0.148438, 0.171875, ..., -0.5, -0.503906,\n",
       "          -0.464844],\n",
       "         ...,\n",
       "         [0.111328, -0.0615234, -0.213867, ..., 0.249023, 0.12793,\n",
       "          -0.419922],\n",
       "         [-0.0708008, -0.0563965, -0.134766, ..., 0.28125, 0.248047,\n",
       "          -0.378906],\n",
       "         [-0.267578, -0.158203, -0.0400391, ..., 0.324219, 0.25,\n",
       "          -0.104492]],\n",
       "\n",
       "        [[-0.136719, -0.427734, -0.332031, ..., -0.357422, -0.341797,\n",
       "          -2.20312],\n",
       "         [1.27344, 0.789062, 0.746094, ..., -0.355469, -0.423828,\n",
       "          -0.542969],\n",
       "         [-0.0466309, 0.621094, 1.03125, ..., -0.808594, -0.59375,\n",
       "          -0.503906],\n",
       "         ...,\n",
       "         [0.769531, 0.382812, -0.107422, ..., 0.361328, 3.76562,\n",
       "          -0.367188],\n",
       "         [-0.298828, 0.59375, 0.277344, ..., 0.0673828, 2.32812,\n",
       "          -0.386719],\n",
       "         [0.120117, 0.333984, 0.0795898, ..., 0.652344, 1.32812, 2.5]],\n",
       "\n",
       "        [[0.112793, 0.0751953, 0.0683594, ..., 0.0668945, 0.0664062,\n",
       "          0.106445],\n",
       "         [0.0795898, 0.117676, 0.0228271, ..., 0.0522461, 0.0703125,\n",
       "          0.065918],\n",
       "         [-0.00454712, 0.0388184, -0.0181885, ..., 0.0654297,\n",
       "          0.0673828, 0.0629883],\n",
       "         ...,\n",
       "         [-0.0124512, -0.0127563, 0.0255127, ..., -0.0588379, 0.036377,\n",
       "          0.0849609],\n",
       "         [-0.0742188, -0.0422363, 0.00994873, ..., -0.101562,\n",
       "          -0.0119629, 0.134766],\n",
       "         [0.043457, -0.0395508, -0.0280762, ..., -0.121582, -0.0532227,\n",
       "          0.0568848]]],\n",
       "\n",
       "\n",
       "       [[[-0.0134277, -0.0683594, 0.0361328, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [-0.163086, -0.196289, -0.0834961, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [-0.261719, -0.138672, -0.0473633, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         ...,\n",
       "         [-0.151367, -0.137695, -0.0412598, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [-0.123535, -0.0378418, 0.0219727, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [-0.105957, 0.00878906, -0.0339355, ..., 0.18457, 0.18457,\n",
       "          0.18457]],\n",
       "\n",
       "        [[0.101562, 0.103516, -0.0786133, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [0.0480957, 0.0854492, -0.172852, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [-0.118164, 0.0922852, -0.162109, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         ...,\n",
       "         [0.0117188, 0.0466309, 0.0322266, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [0.052002, 0.034668, 0.020752, ..., -0.0625, -0.0625, -0.0625],\n",
       "         [0.0888672, 0.0554199, -0.0168457, ..., -0.0625, -0.0625,\n",
       "          -0.0625]],\n",
       "\n",
       "        [[-1, -0.613281, 0.785156, ..., 0.292969, 0.292969, 0.292969],\n",
       "         [-0.155273, 0.020874, -0.349609, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [-0.00613403, 0.359375, -0.202148, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         ...,\n",
       "         [-0.207031, -0.166016, 0.106934, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [-0.257812, 0.408203, -0.28125, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [-0.166016, 0.241211, -0.249023, ..., 0.292969, 0.292969,\n",
       "          0.292969]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.429688, -0.4375, 0.00369263, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [-0.410156, -0.267578, -0.164062, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [-0.337891, -0.182617, -0.110352, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         ...,\n",
       "         [-0.431641, -0.410156, -0.244141, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [-0.373047, -0.211914, -0.175781, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [-0.314453, -0.185547, -0.246094, ..., 0.519531, 0.519531,\n",
       "          0.519531]],\n",
       "\n",
       "        [[0.433594, 0.675781, -0.527344, ..., 0.5, 0.5, 0.5],\n",
       "         [0.143555, -1.40625, -0.929688, ..., 0.5, 0.5, 0.5],\n",
       "         [-0.0136719, -1.625, 1.01562, ..., 0.5, 0.5, 0.5],\n",
       "         ...,\n",
       "         [-0.402344, -0.511719, -0.863281, ..., 0.5, 0.5, 0.5],\n",
       "         [-0.320312, -0.660156, -0.953125, ..., 0.5, 0.5, 0.5],\n",
       "         [-0.878906, -1.05469, -0.207031, ..., 0.5, 0.5, 0.5]],\n",
       "\n",
       "        [[0.0615234, 0.00187683, -0.0644531, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         [0.0373535, 0.050293, -0.0654297, ..., -0.0805664, -0.0805664,\n",
       "          -0.0805664],\n",
       "         [-0.0634766, 0.15332, -0.0290527, ..., -0.0805664, -0.0805664,\n",
       "          -0.0805664],\n",
       "         ...,\n",
       "         [0.0515137, 0.0634766, 0.0568848, ..., -0.0805664, -0.0805664,\n",
       "          -0.0805664],\n",
       "         [0.0551758, 0.0424805, 0.0127563, ..., -0.0805664, -0.0805664,\n",
       "          -0.0805664],\n",
       "         [0.0766602, 0.0285645, -0.0483398, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664]]],\n",
       "\n",
       "\n",
       "       [[[0.0112305, 0.0727539, 0.0344238, ..., 0.0952148, 0.0786133,\n",
       "          -0.0446777],\n",
       "         [0.0351562, 0.00805664, 0.100586, ..., 0.0649414, 0.12207,\n",
       "          0.122559],\n",
       "         [-0.0368652, -0.059082, -0.0332031, ..., 0.107422, 0.116699,\n",
       "          0.0854492],\n",
       "         ...,\n",
       "         [0.0859375, 0.130859, 0.235352, ..., 0.0908203, 0.144531,\n",
       "          -0.0177002],\n",
       "         [0.0722656, 0.198242, 0.125977, ..., 0.0961914, 0.0585938,\n",
       "          0.0649414],\n",
       "         [0.132812, 0.208008, 0.158203, ..., 0.0539551, 0.0693359,\n",
       "          0.0218506]],\n",
       "\n",
       "        [[0.0456543, -0.0218506, 0.0678711, ..., -0.160156, -0.0289307,\n",
       "          0.146484],\n",
       "         [-0.00379944, -0.0717773, -0.0598145, ..., -0.0820312,\n",
       "          -0.0098877, -0.052002],\n",
       "         [0.045166, 0.00300598, 0.00233459, ..., -0.139648, 0.0493164,\n",
       "          -0.0253906],\n",
       "         ...,\n",
       "         [-0.0172119, -0.0568848, -0.0830078, ..., -0.00939941,\n",
       "          0.0272217, -0.0791016],\n",
       "         [-0.0220947, 0.0664062, 0.138672, ..., -0.0400391, 0.0194092,\n",
       "          -0.00485229],\n",
       "         [-0.145508, 0.019043, -0.0834961, ..., 0.135742, -0.0415039,\n",
       "          -0.0378418]],\n",
       "\n",
       "        [[-0.314453, -0.396484, 0.554688, ..., 0.40625, 0.144531,\n",
       "          0.0544434],\n",
       "         [-0.3125, 0.0351562, 0.106445, ..., 0.0103149, 0.441406,\n",
       "          0.166992],\n",
       "         [-0.203125, -0.171875, -0.198242, ..., -0.125, 0.296875,\n",
       "          0.118164],\n",
       "         ...,\n",
       "         [0.0708008, 0.09375, 0.15332, ..., 0.392578, -0.0908203,\n",
       "          0.0751953],\n",
       "         [-0.539062, 0.306641, -0.101562, ..., 0.173828, 0.53125,\n",
       "          0.164062],\n",
       "         [0.511719, 0.617188, 0.365234, ..., 0.425781, 0.229492,\n",
       "          0.106934]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.259766, 0.015625, 0.0942383, ..., 0.365234, 0.232422,\n",
       "          0.104492],\n",
       "         [-0.140625, -0.15625, -0.0976562, ..., 0.322266, 0.376953,\n",
       "          0.213867],\n",
       "         [-0.3125, -0.287109, -0.243164, ..., 0.330078, 0.427734,\n",
       "          0.304688],\n",
       "         ...,\n",
       "         [0.231445, 0.419922, 0.357422, ..., 0.180664, 0.115234,\n",
       "          0.21582],\n",
       "         [0.265625, 0.378906, 0.40625, ..., 0.147461, 0.208984,\n",
       "          0.233398],\n",
       "         [0.457031, 0.335938, 0.330078, ..., 0.115723, 0.0693359,\n",
       "          -0.0341797]],\n",
       "\n",
       "        [[-0.539062, -0.871094, 0.367188, ..., 0.0717773, 1.52344,\n",
       "          3.28125],\n",
       "         [0.00141907, -0.22168, 0.104492, ..., -0.65625, 0.59375,\n",
       "          0.722656],\n",
       "         [-0.112305, 0.161133, 0.0751953, ..., -0.890625, 1.21094,\n",
       "          1.64844],\n",
       "         ...,\n",
       "         [0.423828, 0.0732422, 0.130859, ..., 0.238281, -0.101562,\n",
       "          0.367188],\n",
       "         [-0.707031, 0.511719, 1.00781, ..., 0.0844727, 0.601562,\n",
       "          0.00311279],\n",
       "         [0.644531, 0.246094, 0.878906, ..., 0.929688, -0.365234,\n",
       "          0.460938]],\n",
       "\n",
       "        [[-0.0117798, -0.00634766, -0.0349121, ..., -0.0106812,\n",
       "          -0.0512695, 0.0834961],\n",
       "         [-0.0146484, -0.0605469, 0.0200195, ..., -0.0498047,\n",
       "          -0.0378418, -0.0429688],\n",
       "         [-0.0510254, -0.078125, -0.0130615, ..., -0.105469,\n",
       "          -0.0673828, -0.0546875],\n",
       "         ...,\n",
       "         [-0.0678711, -0.0537109, -0.050293, ..., -0.0373535,\n",
       "          -0.0220947, -0.106934],\n",
       "         [-0.0712891, -0.0251465, -0.0108032, ..., 0.0766602,\n",
       "          -0.0195312, 0.0488281],\n",
       "         [-0.0849609, -0.0273438, -0.0402832, ..., -0.0255127,\n",
       "          0.0303955, -0.0534668]]],\n",
       "\n",
       "\n",
       "       [[[-0.0332031, 0.00720215, -0.0375977, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [0.0415039, 0.0196533, -0.0563965, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [0.0834961, -0.0378418, 0.0218506, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         ...,\n",
       "         [0.0649414, 0.0766602, 0.0996094, ..., 0.18457, 0.18457,\n",
       "          0.18457],\n",
       "         [0.112793, 0.108398, 0.118652, ..., 0.18457, 0.18457, 0.18457],\n",
       "         [0.052002, 0.0673828, 0.0397949, ..., 0.18457, 0.18457,\n",
       "          0.18457]],\n",
       "\n",
       "        [[0.0544434, 0.0432129, 0.0649414, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [0.0227051, 3.19481e-05, 0.0693359, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [0.203125, -0.000888824, 0.0410156, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         ...,\n",
       "         [0.0534668, 0.0708008, -0.0957031, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [0.0585938, 0.0177002, 0.0541992, ..., -0.0625, -0.0625,\n",
       "          -0.0625],\n",
       "         [-0.00817871, 0.0131226, 0.0371094, ..., -0.0625, -0.0625,\n",
       "          -0.0625]],\n",
       "\n",
       "        [[0.347656, 0.128906, -0.310547, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [0.219727, 0.142578, -0.457031, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [1.01562, -0.322266, -0.359375, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         ...,\n",
       "         [0.28125, 0.193359, 0.371094, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [0.0649414, 0.0263672, 0.326172, ..., 0.292969, 0.292969,\n",
       "          0.292969],\n",
       "         [0.065918, -0.144531, 0.197266, ..., 0.292969, 0.292969,\n",
       "          0.292969]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.249023, -0.251953, -0.298828, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [0.00775146, -0.193359, -0.380859, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [0.10498, -0.324219, -0.271484, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         ...,\n",
       "         [0.176758, 0.123535, 0.125977, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [0.114258, 0.209961, 0.292969, ..., 0.519531, 0.519531,\n",
       "          0.519531],\n",
       "         [0.111816, 0.0556641, 0.166016, ..., 0.519531, 0.519531,\n",
       "          0.519531]],\n",
       "\n",
       "        [[-1.28906, -0.208008, -0.570312, ..., 0.5, 0.5, 0.5],\n",
       "         [1.16406, -0.369141, 0.335938, ..., 0.5, 0.5, 0.5],\n",
       "         [2.29688, 0.789062, -0.482422, ..., 0.5, 0.5, 0.5],\n",
       "         ...,\n",
       "         [-0.0466309, 0.457031, -0.484375, ..., 0.5, 0.5, 0.5],\n",
       "         [-0.157227, 0.123047, 0.259766, ..., 0.5, 0.5, 0.5],\n",
       "         [0.308594, -0.369141, 0.196289, ..., 0.5, 0.5, 0.5]],\n",
       "\n",
       "        [[0.0264893, -0.00463867, 0.0140381, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         [-0.00106049, -0.0119019, 0.027832, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         [-0.0267334, -0.0263672, 0.0776367, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         ...,\n",
       "         [-0.0186768, 0.0649414, -0.0581055, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         [-0.00787354, -0.03125, 0.0262451, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664],\n",
       "         [0.0141602, 0.0223389, -0.0272217, ..., -0.0805664,\n",
       "          -0.0805664, -0.0805664]]]], dtype=bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.91406250e-01, -1.76757812e-01, -1.85546875e-01,  ...,\n",
       "           -1.75781250e-01, -1.74804688e-01, -6.88476562e-02],\n",
       "          [ 1.86157227e-03, -6.78710938e-02, -2.34375000e-01,  ...,\n",
       "           -1.75781250e-01, -1.72851562e-01, -1.70898438e-01],\n",
       "          [ 5.34667969e-02,  1.14257812e-01,  1.19628906e-02,  ...,\n",
       "           -2.03125000e-01, -1.91406250e-01, -1.92382812e-01],\n",
       "          ...,\n",
       "          [ 2.19726562e-02,  6.03027344e-02,  5.56640625e-02,  ...,\n",
       "           -1.35498047e-02, -8.59375000e-02, -1.21582031e-01],\n",
       "          [ 3.46679688e-02,  1.03515625e-01,  3.29589844e-02,  ...,\n",
       "            4.30297852e-03,  1.03759766e-02, -1.11328125e-01],\n",
       "          [ 1.81884766e-02,  1.26953125e-02,  4.82177734e-03,  ...,\n",
       "            4.19921875e-02,  2.41699219e-02, -9.47265625e-02]],\n",
       "\n",
       "         [[ 4.12597656e-02,  3.56445312e-02,  4.44335938e-02,  ...,\n",
       "            3.68652344e-02,  4.10156250e-02,  1.76757812e-01],\n",
       "          [ 1.22070312e-01,  3.73535156e-02,  9.03320312e-02,  ...,\n",
       "            2.91748047e-02,  3.58886719e-02,  6.64062500e-02],\n",
       "          [-2.80761719e-02,  5.34667969e-02, -3.34472656e-02,  ...,\n",
       "            4.56542969e-02,  3.17382812e-02,  1.52587891e-02],\n",
       "          ...,\n",
       "          [-5.66406250e-02,  6.68945312e-02,  3.85742188e-02,  ...,\n",
       "            1.58691406e-02, -2.25830078e-02,  6.22558594e-02],\n",
       "          [ 1.08032227e-02,  4.29687500e-02,  4.95605469e-02,  ...,\n",
       "            6.34765625e-02, -6.10351562e-04,  1.13281250e-01],\n",
       "          [ 4.97436523e-03,  5.68847656e-02, -1.42211914e-02,  ...,\n",
       "           -7.51953125e-02, -6.25610352e-03,  1.15722656e-01]],\n",
       "\n",
       "         [[-9.61914062e-02, -2.87109375e-01, -3.86718750e-01,  ...,\n",
       "           -2.77343750e-01, -2.50000000e-01,  1.39648438e-01],\n",
       "          [ 5.23437500e-01,  1.04980469e-01,  1.62109375e-01,  ...,\n",
       "           -2.42187500e-01, -2.75390625e-01, -2.96875000e-01],\n",
       "          [ 6.34765625e-02, -1.85546875e-02, -2.03125000e-01,  ...,\n",
       "           -3.26171875e-01, -2.48046875e-01, -2.15820312e-01],\n",
       "          ...,\n",
       "          [ 1.22558594e-01, -1.49414062e-01, -9.17968750e-02,  ...,\n",
       "            1.38671875e-01, -4.39453125e-01, -2.73437500e-01],\n",
       "          [ 1.33789062e-01, -1.87500000e-01,  2.22656250e-01,  ...,\n",
       "           -3.35937500e-01,  3.75000000e-01,  3.84765625e-01],\n",
       "          [-3.59375000e-01, -5.27343750e-02, -1.50390625e-01,  ...,\n",
       "            2.02148438e-01,  1.35742188e-01,  2.13867188e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.51171875e-01, -4.10156250e-01, -4.02343750e-01,  ...,\n",
       "           -3.86718750e-01, -3.78906250e-01, -2.24609375e-01],\n",
       "          [ 8.98437500e-02, -1.57226562e-01, -1.81640625e-01,  ...,\n",
       "           -3.84765625e-01, -3.94531250e-01, -4.16015625e-01],\n",
       "          [ 1.35742188e-01,  1.48437500e-01,  1.71875000e-01,  ...,\n",
       "           -5.00000000e-01, -5.03906250e-01, -4.64843750e-01],\n",
       "          ...,\n",
       "          [ 1.11328125e-01, -6.15234375e-02, -2.13867188e-01,  ...,\n",
       "            2.49023438e-01,  1.27929688e-01, -4.19921875e-01],\n",
       "          [-7.08007812e-02, -5.63964844e-02, -1.34765625e-01,  ...,\n",
       "            2.81250000e-01,  2.48046875e-01, -3.78906250e-01],\n",
       "          [-2.67578125e-01, -1.58203125e-01, -4.00390625e-02,  ...,\n",
       "            3.24218750e-01,  2.50000000e-01, -1.04492188e-01]],\n",
       "\n",
       "         [[-1.36718750e-01, -4.27734375e-01, -3.32031250e-01,  ...,\n",
       "           -3.57421875e-01, -3.41796875e-01, -2.20312500e+00],\n",
       "          [ 1.27343750e+00,  7.89062500e-01,  7.46093750e-01,  ...,\n",
       "           -3.55468750e-01, -4.23828125e-01, -5.42968750e-01],\n",
       "          [-4.66308594e-02,  6.21093750e-01,  1.03125000e+00,  ...,\n",
       "           -8.08593750e-01, -5.93750000e-01, -5.03906250e-01],\n",
       "          ...,\n",
       "          [ 7.69531250e-01,  3.82812500e-01, -1.07421875e-01,  ...,\n",
       "            3.61328125e-01,  3.76562500e+00, -3.67187500e-01],\n",
       "          [-2.98828125e-01,  5.93750000e-01,  2.77343750e-01,  ...,\n",
       "            6.73828125e-02,  2.32812500e+00, -3.86718750e-01],\n",
       "          [ 1.20117188e-01,  3.33984375e-01,  7.95898438e-02,  ...,\n",
       "            6.52343750e-01,  1.32812500e+00,  2.50000000e+00]],\n",
       "\n",
       "         [[ 1.12792969e-01,  7.51953125e-02,  6.83593750e-02,  ...,\n",
       "            6.68945312e-02,  6.64062500e-02,  1.06445312e-01],\n",
       "          [ 7.95898438e-02,  1.17675781e-01,  2.28271484e-02,  ...,\n",
       "            5.22460938e-02,  7.03125000e-02,  6.59179688e-02],\n",
       "          [-4.54711914e-03,  3.88183594e-02, -1.81884766e-02,  ...,\n",
       "            6.54296875e-02,  6.73828125e-02,  6.29882812e-02],\n",
       "          ...,\n",
       "          [-1.24511719e-02, -1.27563477e-02,  2.55126953e-02,  ...,\n",
       "           -5.88378906e-02,  3.63769531e-02,  8.49609375e-02],\n",
       "          [-7.42187500e-02, -4.22363281e-02,  9.94873047e-03,  ...,\n",
       "           -1.01562500e-01, -1.19628906e-02,  1.34765625e-01],\n",
       "          [ 4.34570312e-02, -3.95507812e-02, -2.80761719e-02,  ...,\n",
       "           -1.21582031e-01, -5.32226562e-02,  5.68847656e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.34277344e-02, -6.83593750e-02,  3.61328125e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [-1.63085938e-01, -1.96289062e-01, -8.34960938e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [-2.61718750e-01, -1.38671875e-01, -4.73632812e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          ...,\n",
       "          [-1.51367188e-01, -1.37695312e-01, -4.12597656e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [-1.23535156e-01, -3.78417969e-02,  2.19726562e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [-1.05957031e-01,  8.78906250e-03, -3.39355469e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01]],\n",
       "\n",
       "         [[ 1.01562500e-01,  1.03515625e-01, -7.86132812e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 4.80957031e-02,  8.54492188e-02, -1.72851562e-01,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [-1.18164062e-01,  9.22851562e-02, -1.62109375e-01,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          ...,\n",
       "          [ 1.17187500e-02,  4.66308594e-02,  3.22265625e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 5.20019531e-02,  3.46679688e-02,  2.07519531e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 8.88671875e-02,  5.54199219e-02, -1.68457031e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02]],\n",
       "\n",
       "         [[-1.00000000e+00, -6.13281250e-01,  7.85156250e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [-1.55273438e-01,  2.08740234e-02, -3.49609375e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [-6.13403320e-03,  3.59375000e-01, -2.02148438e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          ...,\n",
       "          [-2.07031250e-01, -1.66015625e-01,  1.06933594e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [-2.57812500e-01,  4.08203125e-01, -2.81250000e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [-1.66015625e-01,  2.41210938e-01, -2.49023438e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.29687500e-01, -4.37500000e-01,  3.69262695e-03,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [-4.10156250e-01, -2.67578125e-01, -1.64062500e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [-3.37890625e-01, -1.82617188e-01, -1.10351562e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          ...,\n",
       "          [-4.31640625e-01, -4.10156250e-01, -2.44140625e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [-3.73046875e-01, -2.11914062e-01, -1.75781250e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [-3.14453125e-01, -1.85546875e-01, -2.46093750e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01]],\n",
       "\n",
       "         [[ 4.33593750e-01,  6.75781250e-01, -5.27343750e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [ 1.43554688e-01, -1.40625000e+00, -9.29687500e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [-1.36718750e-02, -1.62500000e+00,  1.01562500e+00,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          ...,\n",
       "          [-4.02343750e-01, -5.11718750e-01, -8.63281250e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [-3.20312500e-01, -6.60156250e-01, -9.53125000e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [-8.78906250e-01, -1.05468750e+00, -2.07031250e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01]],\n",
       "\n",
       "         [[ 6.15234375e-02,  1.87683105e-03, -6.44531250e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [ 3.73535156e-02,  5.02929688e-02, -6.54296875e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [-6.34765625e-02,  1.53320312e-01, -2.90527344e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          ...,\n",
       "          [ 5.15136719e-02,  6.34765625e-02,  5.68847656e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [ 5.51757812e-02,  4.24804688e-02,  1.27563477e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [ 7.66601562e-02,  2.85644531e-02, -4.83398438e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.12304688e-02,  7.27539062e-02,  3.44238281e-02,  ...,\n",
       "            9.52148438e-02,  7.86132812e-02, -4.46777344e-02],\n",
       "          [ 3.51562500e-02,  8.05664062e-03,  1.00585938e-01,  ...,\n",
       "            6.49414062e-02,  1.22070312e-01,  1.22558594e-01],\n",
       "          [-3.68652344e-02, -5.90820312e-02, -3.32031250e-02,  ...,\n",
       "            1.07421875e-01,  1.16699219e-01,  8.54492188e-02],\n",
       "          ...,\n",
       "          [ 8.59375000e-02,  1.30859375e-01,  2.35351562e-01,  ...,\n",
       "            9.08203125e-02,  1.44531250e-01, -1.77001953e-02],\n",
       "          [ 7.22656250e-02,  1.98242188e-01,  1.25976562e-01,  ...,\n",
       "            9.61914062e-02,  5.85937500e-02,  6.49414062e-02],\n",
       "          [ 1.32812500e-01,  2.08007812e-01,  1.58203125e-01,  ...,\n",
       "            5.39550781e-02,  6.93359375e-02,  2.18505859e-02]],\n",
       "\n",
       "         [[ 4.56542969e-02, -2.18505859e-02,  6.78710938e-02,  ...,\n",
       "           -1.60156250e-01, -2.89306641e-02,  1.46484375e-01],\n",
       "          [-3.79943848e-03, -7.17773438e-02, -5.98144531e-02,  ...,\n",
       "           -8.20312500e-02, -9.88769531e-03, -5.20019531e-02],\n",
       "          [ 4.51660156e-02,  3.00598145e-03,  2.33459473e-03,  ...,\n",
       "           -1.39648438e-01,  4.93164062e-02, -2.53906250e-02],\n",
       "          ...,\n",
       "          [-1.72119141e-02, -5.68847656e-02, -8.30078125e-02,  ...,\n",
       "           -9.39941406e-03,  2.72216797e-02, -7.91015625e-02],\n",
       "          [-2.20947266e-02,  6.64062500e-02,  1.38671875e-01,  ...,\n",
       "           -4.00390625e-02,  1.94091797e-02, -4.85229492e-03],\n",
       "          [-1.45507812e-01,  1.90429688e-02, -8.34960938e-02,  ...,\n",
       "            1.35742188e-01, -4.15039062e-02, -3.78417969e-02]],\n",
       "\n",
       "         [[-3.14453125e-01, -3.96484375e-01,  5.54687500e-01,  ...,\n",
       "            4.06250000e-01,  1.44531250e-01,  5.44433594e-02],\n",
       "          [-3.12500000e-01,  3.51562500e-02,  1.06445312e-01,  ...,\n",
       "            1.03149414e-02,  4.41406250e-01,  1.66992188e-01],\n",
       "          [-2.03125000e-01, -1.71875000e-01, -1.98242188e-01,  ...,\n",
       "           -1.25000000e-01,  2.96875000e-01,  1.18164062e-01],\n",
       "          ...,\n",
       "          [ 7.08007812e-02,  9.37500000e-02,  1.53320312e-01,  ...,\n",
       "            3.92578125e-01, -9.08203125e-02,  7.51953125e-02],\n",
       "          [-5.39062500e-01,  3.06640625e-01, -1.01562500e-01,  ...,\n",
       "            1.73828125e-01,  5.31250000e-01,  1.64062500e-01],\n",
       "          [ 5.11718750e-01,  6.17187500e-01,  3.65234375e-01,  ...,\n",
       "            4.25781250e-01,  2.29492188e-01,  1.06933594e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.59765625e-01,  1.56250000e-02,  9.42382812e-02,  ...,\n",
       "            3.65234375e-01,  2.32421875e-01,  1.04492188e-01],\n",
       "          [-1.40625000e-01, -1.56250000e-01, -9.76562500e-02,  ...,\n",
       "            3.22265625e-01,  3.76953125e-01,  2.13867188e-01],\n",
       "          [-3.12500000e-01, -2.87109375e-01, -2.43164062e-01,  ...,\n",
       "            3.30078125e-01,  4.27734375e-01,  3.04687500e-01],\n",
       "          ...,\n",
       "          [ 2.31445312e-01,  4.19921875e-01,  3.57421875e-01,  ...,\n",
       "            1.80664062e-01,  1.15234375e-01,  2.15820312e-01],\n",
       "          [ 2.65625000e-01,  3.78906250e-01,  4.06250000e-01,  ...,\n",
       "            1.47460938e-01,  2.08984375e-01,  2.33398438e-01],\n",
       "          [ 4.57031250e-01,  3.35937500e-01,  3.30078125e-01,  ...,\n",
       "            1.15722656e-01,  6.93359375e-02, -3.41796875e-02]],\n",
       "\n",
       "         [[-5.39062500e-01, -8.71093750e-01,  3.67187500e-01,  ...,\n",
       "            7.17773438e-02,  1.52343750e+00,  3.28125000e+00],\n",
       "          [ 1.41906738e-03, -2.21679688e-01,  1.04492188e-01,  ...,\n",
       "           -6.56250000e-01,  5.93750000e-01,  7.22656250e-01],\n",
       "          [-1.12304688e-01,  1.61132812e-01,  7.51953125e-02,  ...,\n",
       "           -8.90625000e-01,  1.21093750e+00,  1.64843750e+00],\n",
       "          ...,\n",
       "          [ 4.23828125e-01,  7.32421875e-02,  1.30859375e-01,  ...,\n",
       "            2.38281250e-01, -1.01562500e-01,  3.67187500e-01],\n",
       "          [-7.07031250e-01,  5.11718750e-01,  1.00781250e+00,  ...,\n",
       "            8.44726562e-02,  6.01562500e-01,  3.11279297e-03],\n",
       "          [ 6.44531250e-01,  2.46093750e-01,  8.78906250e-01,  ...,\n",
       "            9.29687500e-01, -3.65234375e-01,  4.60937500e-01]],\n",
       "\n",
       "         [[-1.17797852e-02, -6.34765625e-03, -3.49121094e-02,  ...,\n",
       "           -1.06811523e-02, -5.12695312e-02,  8.34960938e-02],\n",
       "          [-1.46484375e-02, -6.05468750e-02,  2.00195312e-02,  ...,\n",
       "           -4.98046875e-02, -3.78417969e-02, -4.29687500e-02],\n",
       "          [-5.10253906e-02, -7.81250000e-02, -1.30615234e-02,  ...,\n",
       "           -1.05468750e-01, -6.73828125e-02, -5.46875000e-02],\n",
       "          ...,\n",
       "          [-6.78710938e-02, -5.37109375e-02, -5.02929688e-02,  ...,\n",
       "           -3.73535156e-02, -2.20947266e-02, -1.06933594e-01],\n",
       "          [-7.12890625e-02, -2.51464844e-02, -1.08032227e-02,  ...,\n",
       "            7.66601562e-02, -1.95312500e-02,  4.88281250e-02],\n",
       "          [-8.49609375e-02, -2.73437500e-02, -4.02832031e-02,  ...,\n",
       "           -2.55126953e-02,  3.03955078e-02, -5.34667969e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.32031250e-02,  7.20214844e-03, -3.75976562e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [ 4.15039062e-02,  1.96533203e-02, -5.63964844e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [ 8.34960938e-02, -3.78417969e-02,  2.18505859e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          ...,\n",
       "          [ 6.49414062e-02,  7.66601562e-02,  9.96093750e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [ 1.12792969e-01,  1.08398438e-01,  1.18652344e-01,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01],\n",
       "          [ 5.20019531e-02,  6.73828125e-02,  3.97949219e-02,  ...,\n",
       "            1.84570312e-01,  1.84570312e-01,  1.84570312e-01]],\n",
       "\n",
       "         [[ 5.44433594e-02,  4.32128906e-02,  6.49414062e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 2.27050781e-02,  3.19480896e-05,  6.93359375e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 2.03125000e-01, -8.88824463e-04,  4.10156250e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          ...,\n",
       "          [ 5.34667969e-02,  7.08007812e-02, -9.57031250e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [ 5.85937500e-02,  1.77001953e-02,  5.41992188e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02],\n",
       "          [-8.17871094e-03,  1.31225586e-02,  3.71093750e-02,  ...,\n",
       "           -6.25000000e-02, -6.25000000e-02, -6.25000000e-02]],\n",
       "\n",
       "         [[ 3.47656250e-01,  1.28906250e-01, -3.10546875e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [ 2.19726562e-01,  1.42578125e-01, -4.57031250e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [ 1.01562500e+00, -3.22265625e-01, -3.59375000e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          ...,\n",
       "          [ 2.81250000e-01,  1.93359375e-01,  3.71093750e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [ 6.49414062e-02,  2.63671875e-02,  3.26171875e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01],\n",
       "          [ 6.59179688e-02, -1.44531250e-01,  1.97265625e-01,  ...,\n",
       "            2.92968750e-01,  2.92968750e-01,  2.92968750e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.49023438e-01, -2.51953125e-01, -2.98828125e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [ 7.75146484e-03, -1.93359375e-01, -3.80859375e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [ 1.04980469e-01, -3.24218750e-01, -2.71484375e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          ...,\n",
       "          [ 1.76757812e-01,  1.23535156e-01,  1.25976562e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [ 1.14257812e-01,  2.09960938e-01,  2.92968750e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01],\n",
       "          [ 1.11816406e-01,  5.56640625e-02,  1.66015625e-01,  ...,\n",
       "            5.19531250e-01,  5.19531250e-01,  5.19531250e-01]],\n",
       "\n",
       "         [[-1.28906250e+00, -2.08007812e-01, -5.70312500e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [ 1.16406250e+00, -3.69140625e-01,  3.35937500e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [ 2.29687500e+00,  7.89062500e-01, -4.82421875e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          ...,\n",
       "          [-4.66308594e-02,  4.57031250e-01, -4.84375000e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [-1.57226562e-01,  1.23046875e-01,  2.59765625e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01],\n",
       "          [ 3.08593750e-01, -3.69140625e-01,  1.96289062e-01,  ...,\n",
       "            5.00000000e-01,  5.00000000e-01,  5.00000000e-01]],\n",
       "\n",
       "         [[ 2.64892578e-02, -4.63867188e-03,  1.40380859e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [-1.06048584e-03, -1.19018555e-02,  2.78320312e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [-2.67333984e-02, -2.63671875e-02,  7.76367188e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          ...,\n",
       "          [-1.86767578e-02,  6.49414062e-02, -5.81054688e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [-7.87353516e-03, -3.12500000e-02,  2.62451172e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02],\n",
       "          [ 1.41601562e-02,  2.23388672e-02, -2.72216797e-02,  ...,\n",
       "           -8.05664062e-02, -8.05664062e-02, -8.05664062e-02]]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[1.5507771, 1.477785 , 1.4047928, 1.3172023, 1.2442101, 1.2296118,\n",
       "        1.2442101, 1.2442101, 1.273407 , 1.273407 ],\n",
       "       [1.5215802, 1.4631865, 1.3901944, 1.3172023, 1.2442101, 1.2296118,\n",
       "        1.2442101, 1.2442101, 1.273407 , 1.273407 ],\n",
       "       [1.5069818, 1.4339896, 1.3609976, 1.2880055, 1.2296118, 1.2296118,\n",
       "        1.2442101, 1.2442101, 1.273407 , 1.273407 ],\n",
       "       [1.477785 , 1.3901944, 1.3172023, 1.2442101, 1.2150133, 1.2296118,\n",
       "        1.2442101, 1.2442101, 1.273407 , 1.273407 ],\n",
       "       [1.4047928, 1.3463992, 1.3026038, 1.2442101, 1.2150133, 1.2296118,\n",
       "        1.2442101, 1.2442101, 1.273407 , 1.273407 ],\n",
       "       [1.3318007, 1.3026038, 1.2880055, 1.273407 , 1.273407 , 1.2588086,\n",
       "        1.2588086, 1.2588086, 1.2880055, 1.2880055],\n",
       "       [1.2880055, 1.2880055, 1.2880055, 1.2880055, 1.2880055, 1.273407 ,\n",
       "        1.273407 , 1.273407 , 1.2880055, 1.2880055],\n",
       "       [1.2880055, 1.2880055, 1.2880055, 1.2880055, 1.2880055, 1.273407 ,\n",
       "        1.273407 , 1.273407 , 1.2880055, 1.2880055],\n",
       "       [1.2296118, 1.2588086, 1.2880055, 1.3026038, 1.3026038, 1.2880055,\n",
       "        1.273407 , 1.273407 , 1.2880055, 1.2880055],\n",
       "       [1.1858164, 1.2150133, 1.2588086, 1.273407 , 1.273407 , 1.273407 ,\n",
       "        1.273407 , 1.273407 , 1.2880055, 1.2880055]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_jax.pixel_values[0,0,0,0,:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_modules at 0x7ef9b8d478b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_torch.named_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43moutput_torch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlast_hidden_state\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[:, :, :, \u001b[38;5;241m0\u001b[39m, i:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m10\u001b[39m)]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "output_torch['last_hidden_state'][:, :, :, 0, i:(i+10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[5.65625, -5.25, -5.375, 5.8125, 4.90625, -4.21875, 1.55469,\n",
       "          -3.15625, 0.984375, 3.90625],\n",
       "         [6.34375, -3.03125, -6.4375, 6.03125, 7.8125, -3.59375,\n",
       "          2.48438, -7.15625, -0.753906, 4.1875],\n",
       "         [14, -4.09375, -7.625, 2.76562, 11.8125, -10.5, 0.722656,\n",
       "          -6.59375, 0.808594, 5.59375],\n",
       "         [4.21875, 3.5, -2.51562, -3.89062, -1.25, 0.566406, 4.65625,\n",
       "          2.09375, 8.8125, 4.5625]]]], dtype=bfloat16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_jax1['last_hidden_state'][:, :, :, 0, i:(i+10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[5.65625, -5.25, -5.375, 5.8125, 4.90625, -4.21875, 1.55469,\n",
       "          -3.15625, 0.984375, 3.90625],\n",
       "         [6.34375, -3.03125, -6.4375, 6.03125, 7.8125, -3.59375,\n",
       "          2.48438, -7.15625, -0.753906, 4.1875],\n",
       "         [14, -4.09375, -7.625, 2.76562, 11.8125, -10.5, 0.722656,\n",
       "          -6.59375, 0.808594, 5.59375],\n",
       "         [4.21875, 3.5, -2.51562, -3.89062, -1.25, 0.566406, 4.65625,\n",
       "          2.09375, 8.8125, 4.5625]]]], dtype=bfloat16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_jax2['last_hidden_state'][:, :, :, 0, i:(i+10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import tree_util\n",
    "\n",
    "def print_detailed_info(name, param):\n",
    "    print(f\"{name}   {param.shape}; {param.dtype}\")\n",
    "\n",
    "tree_util.tree_map_with_path(\n",
    "    lambda path, x: print_detailed_info(\"\".join(str(p) for p in path), x),\n",
    "    model.params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_torch.named_parameters():\n",
    "    print(f\"[\\\"{name}\\\"]   {param.shape}; {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_torch.state_dict()[\"transformer.layers.31.self_attn.q_proj.weight\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params['vision_model']['transformer']['layers.31']['self_attn']['q_proj']['kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_jax['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_jax['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
